<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="tvdboom">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Examples - ATOM</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Examples";
    var mkdocs_page_input_path = "examples.md";
    var mkdocs_page_url = "/ATOM/examples/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ATOM</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../getting_started/">Getting started</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../api/">API</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Examples</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#binary-classification">Binary classification</a></li>
    

    <li class="toctree-l2"><a href="#multiclass-classification">Multiclass classification</a></li>
    

    <li class="toctree-l2"><a href="#regression">Regression</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../dependencies/">Dependencies</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../license/">License</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ATOM</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Examples</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="binary-classification">Binary classification</h1>
<hr />
<p>Download the Australian weather dataset from <a href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package">https://www.kaggle.com/jsphyg/weather-dataset-rattle-package</a>.
 This dataset tries to predict whether or not it will rain tomorrow by
 training a classification model on target <code>RainTomorrow</code>.</p>
<p><strong>Load the data</strong></p>
<pre><code class="python"># Import packages
import numpy as np
import pandas as pd
from sklearn.metrics import fbeta_score
from atom import ATOMClassifier

# Load the Australian weather dataset
X = pd.read_csv('../weatherAUS.csv')
X = X.drop(['RISK_MM', 'Date'], axis=1)  # Drop unrelated features
</code></pre>

<p><strong>Run the pipeline</strong></p>
<pre><code class="python"># Call ATOM using only a percentage of the complete dataset (for explanatory purposes)
atom = ATOMClassifier(X, y=&quot;RainTomorrow&quot;, percentage=5, log='auto', n_jobs=2, verbose=3)
</code></pre>

<pre><code>&lt;&lt;=============== ATOM ===============&gt;&gt;
Parallel processing with 2 cores.
Initial data cleaning...
 --&gt; Dropping 45 duplicate rows.
Algorithm task: binary classification.

Dataset stats ===================&gt;
Shape: (7107, 22)
Missing values: 15680
Categorical columns: 5
Scaled: False
----------------------------------
Size of training set: 4974
Size of test set: 2133
----------------------------------
Class balance: No:Yes &lt;==&gt; 3.4:1.0
Instances in RainTomorrow per class:
|        |    total |    train_set |    test_set |
|:-------|---------:|-------------:|------------:|
| 0: No  |     5502 |         3854 |        1648 |
| 1: Yes |     1605 |         1120 |         485 |
</code></pre>
<pre><code class="python"># If we change a column during the pre-processing,
# we need to call the update method to update all data attributes

atom.X['MaxTemp'] = np.log(atom.X['MaxTemp'])

# MaxTemp has now been changed for atom.X, but not in atom.X_train, atom.dataset, etc...
# To do so, we use the update method...
atom.update('X')

assert atom.X['MaxTemp'].equals(atom.dataset['MaxTemp'])
</code></pre>

<pre><code class="python"># Impute missing values
atom.impute(strat_num='knn', strat_cat='remove', max_frac_rows=0.8)
</code></pre>

<pre><code>Imputing missing values...
 --&gt; Removing 741 rows for containing too many missing values.
 --&gt; Imputing 3 missing values using the KNN imputer in feature MinTemp.
 --&gt; Imputing 4 missing values using the KNN imputer in feature MaxTemp.
 --&gt; Imputing 43 missing values using the KNN imputer in feature Rainfall.
 --&gt; Imputing 2315 missing values using the KNN imputer in feature Evaporation.
 --&gt; Imputing 2661 missing values using the KNN imputer in feature Sunshine.
 --&gt; Removing 222 rows due to missing values in feature WindGustDir.
 --&gt; Imputing 221 missing values using the KNN imputer in feature WindGustSpeed.
 --&gt; Removing 327 rows due to missing values in feature WindDir9am.
 --&gt; Removing 24 rows due to missing values in feature WindDir3pm.
 --&gt; Imputing 6 missing values using the KNN imputer in feature WindSpeed9am.
 --&gt; Imputing 2 missing values using the KNN imputer in feature WindSpeed3pm.
 --&gt; Imputing 25 missing values using the KNN imputer in feature Humidity9am.
 --&gt; Imputing 55 missing values using the KNN imputer in feature Humidity3pm.
 --&gt; Imputing 56 missing values using the KNN imputer in feature Pressure9am.
 --&gt; Imputing 51 missing values using the KNN imputer in feature Pressure3pm.
 --&gt; Imputing 2118 missing values using the KNN imputer in feature Cloud9am.
 --&gt; Imputing 2253 missing values using the KNN imputer in feature Cloud3pm.
 --&gt; Imputing 5 missing values using the KNN imputer in feature Temp9am.
 --&gt; Imputing 32 missing values using the KNN imputer in feature Temp3pm.
 --&gt; Removing 43 rows due to missing values in feature RainToday.
</code></pre>
<pre><code class="python"># Encode the categorical features
atom.encode(max_onehot=10, frac_to_other=0.04)
</code></pre>

<pre><code>Encoding categorical features...
 --&gt; Target-encoding feature Location.  Contains 1 unique categories.
 --&gt; Target-encoding feature WindGustDir.  Contains 16 unique categories.
 --&gt; Target-encoding feature WindDir9am.  Contains 16 unique categories.
 --&gt; Target-encoding feature WindDir3pm.  Contains 16 unique categories.
 --&gt; Label-encoding feature RainToday. Contains 2 unique categories.
</code></pre>
<pre><code class="python"># Perform undersampling of the majority class to balance the dataset
atom.balance(undersample=0.8)
</code></pre>

<pre><code>Performing undersampling...
 --&gt; Removing 249 rows from class No.
</code></pre>
<pre><code class="python"># Remove outliers from the training set
atom.outliers(max_sigma=5)
</code></pre>

<pre><code>Handling outliers...
 --&gt; Dropping 18 rows due to outliers.
</code></pre>
<pre><code class="python"># Select only the best 10 features
atom.feature_selection(strategy=&quot;univariate&quot;, max_features=15, max_correlation=0.8)

# See which features were removed due to collinearity
atom.collinear
</code></pre>

<pre><code>Performing feature selection...
 --&gt; Feature Location was removed due to low variance: 0.00.
 --&gt; Feature Pressure3pm was removed due to collinearity with another feature.
 --&gt; Feature Temp9am was removed due to collinearity with another feature.
 --&gt; Feature Temp3pm was removed due to collinearity with another feature.
 --&gt; Feature MinTemp was removed after the univariate test (score: 9.36  p-value: 0.00).
 --&gt; Feature Evaporation was removed after the univariate test (score: 27.00  p-value: 0.00).
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drop_feature</th>
      <th>correlated_feature</th>
      <th>correlation_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Pressure3pm</td>
      <td>Pressure9am</td>
      <td>0.95413</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Temp9am</td>
      <td>MinTemp, MaxTemp</td>
      <td>0.9201, 0.89643</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Temp3pm</td>
      <td>MaxTemp, Temp9am</td>
      <td>0.9587, 0.87527</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Change the verbosity of ATOM to not print too much details while fitting
atom.verbose = 2

# Let's use a custom metric
def f2_score(y_true, y_pred):
    return fbeta_score(y_true, y_pred, beta=2)

# Let's compare the performance of various gradient boosting algorithms
atom.pipeline(['gbm', 'lgb', 'catb'],
              metric=f2_score,
              max_iter=5,
              init_points=5,
              cv=1,
              bagging=5)
</code></pre>

<pre><code>Running pipeline =================&gt;
Models in pipeline: GBM, LGB, CatB
Metric: f2_score


Running BO for Gradient Boosting Machine...
Final results for Gradient Boosting Machine:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'n_estimators': 414, 'learning_rate': 1.0, 'subsample': 0.5, 'max_depth': 2, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 12, 'min_samples_leaf': 1, 'ccp_alpha': 0.0}
Best score on the BO: 0.7485
Time elapsed: 37.958s
Fitting -----------------------------------------
Score on the training set: 0.8465
Score on the test set: 0.5824
Time elapsed: 0.796s
Bagging -----------------------------------------
Mean: 0.5587   Std: 0.0105
Time elapsed: 3.562s
-------------------------------------------------
Total time: 42.316s


Running BO for LightGBM...
Final results for LightGBM:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'n_estimators': 345, 'learning_rate': 0.6, 'max_depth': 3, 'num_leaves': 24, 'min_child_weight': 9, 'min_child_samples': 17, 'subsample': 0.5, 'colsample_bytree': 0.5, 'reg_alpha': 0.0, 'reg_lambda': 0.1}
Best score on the BO: 0.7583
Time elapsed: 3.261s
Fitting -----------------------------------------
Score on the training set: 0.9937
Score on the test set: 0.6182
Time elapsed: 0.187s
Bagging -----------------------------------------
Mean: 0.6169   Std: 0.0197
Time elapsed: 0.370s
-------------------------------------------------
Total time: 3.819s


Running BO for CatBoost...
Final results for CatBoost:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'n_estimators': 499, 'learning_rate': 0.09, 'max_depth': 9, 'subsample': 0.7, 'colsample_bylevel': 0.8, 'reg_lambda': 100.0}
Best score on the BO: 0.7714
Time elapsed: 14.712s
Fitting -----------------------------------------
Score on the training set: 0.9390
Score on the test set: 0.6365
Time elapsed: 3.890s
Bagging -----------------------------------------
Mean: 0.6374   Std: 0.0083
Time elapsed: 19.245s
-------------------------------------------------
Total time: 37.847s


Final results ================&gt;&gt;
Duration: 1m:23s
Metric: f2_score
--------------------------------
Gradient Boosting Machine --&gt; 0.559 ± 0.010 ~
LightGBM                  --&gt; 0.617 ± 0.020 ~
CatBoost                  --&gt; 0.637 ± 0.008 !! ~
</code></pre>
<p><strong>Analyze the results</strong></p>
<pre><code class="python"># Let's have a look at the best model
print('And the winner is...', atom.winner.longname)

print('Score on the training set: ', atom.winner.score_train)
print('Score on the test set: ', atom.winner.score_test)
</code></pre>

<pre><code>And the winner is... CatBoost
Score on the training set:  0.9390495867768595
Score on the test set:  0.6364787840405319
</code></pre>
<pre><code class="python"># Make some plots to analyze the results
atom.winner.plot_confusion_matrix(normalize=True, figsize=(7, 7), filename='confusion_matrix.png')
atom.winner.plot_probabilities()

# Change plots aesthetics
ATOMClassifier.set_style('whitegrid')
ATOMClassifier.set_title_fontsize(24)

atom.plot_ROC(models=('LGB', 'CatB'), title=&quot;ROC for the LightGBM vs CatBoost model&quot;)
atom.plot_PRC(title=&quot;PRC comparison of the models&quot;)
atom.catb.plot_threshold(metric=['f1', 'accuracy', 'average_precision'], steps=50, filename='thresholds.png')
</code></pre>

<p><img alt="confusion_matrix" src="../img/confusion_matrix.png" /></p>
<p><img alt="probabilities" src="../img/probabilities.png" /></p>
<p><img alt="ROC" src="../img/ROC.png" /></p>
<p><img alt="PRC" src="../img/PRC.png" /></p>
<p><img alt="threshold" src="../img/threshold.png" /></p>
<p><br><br></p>
<h1 id="multiclass-classification">Multiclass classification</h1>
<hr />
<p>Import the wine dataset from <a href="https://scikit-learn.org/stable/datasets/index.html#wine-dataset">sklearn.datasets</a>.
 This is a small and easy to train dataset which goal is to classify wines
 into three groups (which cultivator it's from) using features based on the results
 of chemical analysis.</p>
<p><strong>Load the data</strong></p>
<pre><code class="python"># Import packages
import numpy as np
import pandas as pd
from sklearn.datasets load_wine
from atom.atom import ATOMClassifier

# Load the dataset's features and targets
dataset = load_wine()

# Convert to pd.DtaFrame to get the names of the features
data = np.c_[dataset.data, dataset.target]
columns = np.append(dataset.feature_names, [&quot;target&quot;])
data = pd.DataFrame(data, columns=columns)
X = data.drop('target', axis=1)
y = data['target']
</code></pre>

<p><strong>Run the pipeline</strong></p>
<pre><code class="python"># Call ATOMclass for ML task exploration
atom = ATOMClassifier(X, y, n_jobs=-1, verbose=3)

# Fit the pipeline with the selected models
atom.pipeline(models=['LDA','RF', 'lSVM'],
              metric='f1_macro',
              max_iter=4,
              init_points=3,
              cv=3,
              bagging=10)
</code></pre>

<pre><code>&lt;&lt;=============== ATOM ===============&gt;&gt;
Parallel processing with 4 cores.
Initial data cleaning...
Algorithm task: multiclass classification. Number of classes: 3.

Dataset stats ===================&gt;
Shape: (178, 14)
Scaled: False
----------------------------------
Size of training set: 124
Size of test set: 54
----------------------------------
Instances in target per class:
|    |    total |    train_set |    test_set |
|---:|---------:|-------------:|------------:|
|  0 |       59 |           42 |          17 |
|  1 |       71 |           47 |          24 |
|  2 |       48 |           35 |          13 |


Running pipeline =================&gt;
Models in pipeline: LDA, RF, lSVM
Metric: f1_macro


Running BO for Linear Discriminant Analysis...
Initial point: 1 --------------------------------
Parameters --&gt; {'solver': 'lsqr', 'shrinkage': 0.9}
Evaluation --&gt; f1_macro: 0.6787
Time elapsed: 0.815s   Total time: 0.816s
Initial point: 2 --------------------------------
Parameters --&gt; {'solver': 'lsqr', 'shrinkage': 0.8}
Evaluation --&gt; f1_macro: 0.6865
Time elapsed: 0.505s   Total time: 1.320s
Initial point: 3 --------------------------------
Parameters --&gt; {'solver': 'eigen', 'shrinkage': 0.7}
Evaluation --&gt; f1_macro: 0.6667
Time elapsed: 0.021s   Total time: 1.341s
Iteration: 1 ------------------------------------
Parameters --&gt; {'solver': 'svd'}
Evaluation --&gt; f1_macro: 0.9753
Time elapsed: 0.020s   Total time: 1.560s
Iteration: 2 ------------------------------------
Parameters --&gt; {'solver': 'svd'}
Evaluation --&gt; f1_macro: 0.9753
Time elapsed: 0.026s   Total time: 1.796s

Final results for Linear Discriminant Analysis:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'solver': 'svd'}
Best score on the BO: 0.9753
Time elapsed: 1.936s
Fitting -----------------------------------------
Score on the training set: 1.0000
Score on the test set: 0.9617
Time elapsed: 0.079s
Bagging -----------------------------------------
Mean: 0.9788   Std: 0.0159
Time elapsed: 0.035s
-------------------------------------------------
Total time: 2.050s


Running BO for Random Forest...
Initial point: 1 --------------------------------
Parameters --&gt; {'n_estimators': 460, 'max_depth': 5, 'max_features': 0.9, 'criterion': 'entropy', 'min_samples_split': 10, 'min_samples_leaf': 20, 'ccp_alpha': 0.03, 'bootstrap': True, 'max_samples': 0.7}
Evaluation --&gt; f1_macro: 0.8673
Time elapsed: 0.835s   Total time: 0.836s
Initial point: 2 --------------------------------
Parameters --&gt; {'n_estimators': 210, 'max_depth': 6, 'max_features': 0.5, 'criterion': 'entropy', 'min_samples_split': 11, 'min_samples_leaf': 14, 'ccp_alpha': 0.025, 'bootstrap': False}
Evaluation --&gt; f1_macro: 0.9357
Time elapsed: 0.449s   Total time: 1.286s
Initial point: 3 --------------------------------
Parameters --&gt; {'n_estimators': 155, 'max_depth': 7, 'max_features': 0.7, 'criterion': 'entropy', 'min_samples_split': 18, 'min_samples_leaf': 13, 'ccp_alpha': 0.02, 'bootstrap': False}
Evaluation --&gt; f1_macro: 0.8638
Time elapsed: 0.405s   Total time: 1.691s
Iteration: 1 ------------------------------------
Parameters --&gt; {'n_estimators': 460, 'max_depth': 6, 'max_features': 0.9, 'criterion': 'entropy', 'min_samples_split': 10, 'min_samples_leaf': 20, 'ccp_alpha': 0.035, 'bootstrap': True, 'max_samples': 0.7}
Evaluation --&gt; f1_macro: 0.9073
Time elapsed: 0.827s   Total time: 2.801s
Iteration: 2 ------------------------------------
Parameters --&gt; {'n_estimators': 20, 'max_depth': 3, 'max_features': 0.7, 'criterion': 'gini', 'min_samples_split': 3, 'min_samples_leaf': 18, 'ccp_alpha': 0.015, 'bootstrap': False}
Evaluation --&gt; f1_macro: 0.8953
Time elapsed: 0.234s   Total time: 3.362s
Iteration: 3 ------------------------------------
Parameters --&gt; {'n_estimators': 20, 'max_depth': 8, 'max_features': 0.6, 'criterion': 'entropy', 'min_samples_split': 14, 'min_samples_leaf': 3, 'ccp_alpha': 0.03, 'bootstrap': True, 'max_samples': 0.6}
Evaluation --&gt; f1_macro: 0.9512
Time elapsed: 0.231s   Total time: 3.822s
Iteration: 4 ------------------------------------
Parameters --&gt; {'n_estimators': 20, 'max_depth': 9, 'max_features': 1.0, 'criterion': 'entropy', 'min_samples_split': 20, 'min_samples_leaf': 7, 'ccp_alpha': 0.02, 'bootstrap': False}
Evaluation --&gt; f1_macro: 0.8560
Time elapsed: 0.235s   Total time: 4.563s

Final results for Random Forest:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'n_estimators': 20, 'max_depth': 8, 'max_features': 0.6, 'criterion': 'entropy', 'min_samples_split': 14, 'min_samples_leaf': 3, 'ccp_alpha': 0.03, 'bootstrap': True, 'max_samples': 0.6}
Best score on the BO: 0.9512
Time elapsed: 4.790s
Fitting -----------------------------------------
Score on the training set: 1.0000
Score on the test set: 0.9448
Time elapsed: 5.671s
Bagging -----------------------------------------
Mean: 0.9240   Std: 0.0274
Time elapsed: 2.345s
-------------------------------------------------
Total time: 12.806s


Running BO for Linear SVM...
Initial point: 1 --------------------------------
Parameters --&gt; {'C': 0.01, 'loss': 'squared_hinge', 'dual': True, 'penalty': 'l2'}
Evaluation --&gt; f1_macro: 0.9833
Time elapsed: 0.031s   Total time: 0.031s
Initial point: 2 --------------------------------
Parameters --&gt; {'C': 0.001, 'loss': 'hinge', 'dual': True, 'penalty': 'l2'}
Evaluation --&gt; f1_macro: 0.9290
Time elapsed: 0.016s   Total time: 0.047s
Initial point: 3 --------------------------------
Parameters --&gt; {'C': 0.001, 'loss': 'squared_hinge', 'dual': True, 'penalty': 'l2'}
Evaluation --&gt; f1_macro: 0.9601
Time elapsed: 0.031s   Total time: 0.078s
Iteration: 1 ------------------------------------
Parameters --&gt; {'C': 10, 'loss': 'squared_hinge', 'dual': False, 'penalty': 'l1'}
Evaluation --&gt; f1_macro: 0.9842
Time elapsed: 0.028s   Total time: 0.359s
Iteration: 2 ------------------------------------
Parameters --&gt; {'C': 100, 'loss': 'squared_hinge', 'dual': False, 'penalty': 'l1'}
Evaluation --&gt; f1_macro: 0.9842
Time elapsed: 0.025s   Total time: 0.730s
Iteration: 3 ------------------------------------
Parameters --&gt; {'C': 100, 'loss': 'squared_hinge', 'dual': False, 'penalty': 'l1'}
Evaluation --&gt; f1_macro: 0.9842
Time elapsed: 0.016s   Total time: 1.059s

Final results for Linear SVM:         
Bayesian Optimization ---------------------------
Best hyperparameters: {'C': 10.0, 'loss': 'squared_hinge', 'dual': False, 'penalty': 'l1'}
Best score on the BO: 0.9842
Time elapsed: 1.230s
Fitting -----------------------------------------
Score on the training set: 1.0000
Score on the test set: 0.9617
Time elapsed: 0.047s
Bagging -----------------------------------------
Mean: 0.9498   Std: 0.0117
Time elapsed: 0.101s
-------------------------------------------------
Total time: 1.379s


Final results ================&gt;&gt;
Duration: 16.238s
Metric: f1_macro
--------------------------------
Linear Discriminant Analysis --&gt; 0.979 ± 0.016 !!
Random Forest                --&gt; 0.924 ± 0.027
Linear SVM                   --&gt; 0.950 ± 0.012
</code></pre>
<p><strong>Analyze the results</strong></p>
<pre><code class="python">atom.scores
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>total_time</th>
      <th>score_train</th>
      <th>score_test</th>
      <th>fit_time</th>
      <th>bagging_mean</th>
      <th>bagging_std</th>
      <th>bagging_time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LDA</td>
      <td>2.050s</td>
      <td>1.0</td>
      <td>0.961698</td>
      <td>0.079s</td>
      <td>0.978848</td>
      <td>0.015898</td>
      <td>0.035s</td>
    </tr>
    <tr>
      <th>1</th>
      <td>RF</td>
      <td>12.806s</td>
      <td>1.0</td>
      <td>0.944813</td>
      <td>5.671s</td>
      <td>0.923975</td>
      <td>0.027393</td>
      <td>2.345s</td>
    </tr>
    <tr>
      <th>2</th>
      <td>lSVM</td>
      <td>1.379s</td>
      <td>1.0</td>
      <td>0.961698</td>
      <td>0.047s</td>
      <td>0.949818</td>
      <td>0.011717</td>
      <td>0.101s</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python"># Show the results for a different metric
atom.results('precision_macro')
</code></pre>

<pre><code>Final results ================&gt;&gt;
Metric: precision_macro
--------------------------------
Linear Discriminant Analysis --&gt; 0.956 !!
Random Forest                --&gt; 0.941
Linear SVM                   --&gt; 0.956 !!
</code></pre>
<pre><code class="python">atom.plot_bagging()
</code></pre>

<p><img alt="bagging_results" src="../img/bagging_results.png" /></p>
<p><strong>Let's have a closer look at the Random Forest</strong></p>
<pre><code class="python"># Get the results on some other metrics
print('Jaccard score:', atom.rf.jaccard_weighted)
print('Recall score:', atom.rf.recall_macro)
</code></pre>

<pre><code>Jaccard score: 0.8960493827160495
Recall score: 0.9526143790849674
</code></pre>
<pre><code class="python"># Plot the feature importance and compare it to the permutation importance of the LDA
atom.rf.plot_feature_importance(show=10)
atom.lda.plot_permutation_importance(show=10)
</code></pre>

<p><img alt="feature_importance" src="../img/feature_importance.png" /></p>
<p><img alt="permutation_importance" src="../img/permutation_importance.png" /></p>
<pre><code class="python"># Save the random forest class for production
atom.RF.save('Random_Forest_class')
</code></pre>

<pre><code>Random Forest model subclass saved successfully!
</code></pre>
<p><br><br></p>
<h1 id="regression">Regression</h1>
<hr />
<p>Download the abalone dataset from <a href="https://archive.ics.uci.edu/ml/datasets/Abalone">https://archive.ics.uci.edu/ml/datasets/Abalone</a>.
 The goal of this dataset is to predict the age of abalone shells from physical measurements.</p>
<p><strong>Load the data</strong></p>
<pre><code class="python"># Import packages
import pandas as pd
from atom import ATOMRegressor

# Load the abalone dataset
X = pd.read_csv('../abalone.csv')
atom = ATOMRegressor(X, y=&quot;Rings&quot;, percentage=10, warnings=False, verbose=1, random_state=42)

# Encode categorical features
atom.encode()

# Apply PCA for dimensionality reduction
atom.feature_selection(strategy=&quot;pca&quot;, max_features=6)
atom.plot_PCA(figsize=(8, 6), filename='atom_PCA_plot')
</code></pre>

<pre><code>&lt;&lt;=============== ATOM ===============&gt;&gt;
Algorithm task: regression.
</code></pre>
<p><img alt="PCA" src="../img/PCA.png" /></p>
<p><strong>Run the pipeline</strong></p>
<pre><code class="python"># Let's compare tree-based models using a successive halving approach
atom.pipeline(['tree', 'bag', 'et', 'rf', 'gbm', 'lgb'],
              successive_halving=True,
              metric='neg_mean_squared_error',
              max_iter=5,
              init_points=5,
              cv=1,
              bagging=5)
</code></pre>

<pre><code>Running pipeline =================&gt;
Metric: neg_mean_squared_error


&lt;&lt;=============== Iteration 0 ==============&gt;&gt;
Models in pipeline: Tree, Bag, ET, RF, GBM, LGB


Processing: 100%|████████████████████████████████| 6/6 [00:25&lt;00:00,  4.18s/it]




Final results ================&gt;&gt;
Duration: 25.079s
Metric: neg_mean_squared_error
--------------------------------
Decision Tree             --&gt; -9.479 ± 0.667 !! ~
Bagging Regressor         --&gt; -11.409 ± 2.167 ~
Extra-Trees               --&gt; -11.788 ± 1.270 ~
Random Forest             --&gt; -11.441 ± 1.059 ~
Gradient Boosting Machine --&gt; -11.044 ± 2.575 ~
LightGBM                  --&gt; -12.929 ± 3.211 ~


&lt;&lt;=============== Iteration 1 ==============&gt;&gt;
Models in pipeline: Tree, Bag, GBM


Processing: 100%|████████████████████████████████| 3/3 [00:12&lt;00:00,  4.03s/it]




Final results ================&gt;&gt;
Duration: 37.229s
Metric: neg_mean_squared_error
--------------------------------
Decision Tree             --&gt; -11.110 ± 5.487 ~
Bagging Regressor         --&gt; -6.780 ± 1.605 !! ~
Gradient Boosting Machine --&gt; -8.079 ± 0.545 ~


&lt;&lt;=============== Iteration 2 ==============&gt;&gt;
Model in pipeline: Bag


Processing: 100%|████████████████████████████████| 1/1 [00:10&lt;00:00, 10.36s/it]



Final results ================&gt;&gt;
Duration: 47.619s
Metric: neg_mean_squared_error
--------------------------------
Bagging Regressor --&gt; -4.925 ± 0.403 ~
</code></pre>
<p><strong>Analyze the results</strong></p>
<pre><code class="python"># Plot successive halving results
atom.plot_successive_halving()
</code></pre>

<p><img alt="successive_halving" src="../img/successive_halving.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../dependencies/" class="btn btn-neutral float-right" title="Dependencies">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../api/" class="btn btn-neutral" title="API"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>© Copyright 2020, by tvdboom.</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/tvdboom/ATOM" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../api/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../dependencies/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
