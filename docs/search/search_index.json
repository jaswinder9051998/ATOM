{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Automated Tool for Optimized Modelling There is no magic formula in data science that can tell us which type of machine learning algorithm will perform best for a specific use-case. Best practices tell us to start with a simple model (e.g. linear regression) and build up to more complicated models (e.g. logistic regression -> random forest -> multilayer perceptron) if you are not satisfied with the results. Unfortunately, different models require different data cleaning steps, tuning a new set of hyperparameters, etc. Refactoring the code for all these steps can be very time consuming. This result in many data scientists just using the model best known to them and fine-tuning this particular model without ever trying other ones. This can result in poor performance (because the model is just not the right one for the task) or in poor time management (because you could have achieved a similar performance with a simpler/faster model). ATOM is here to help us solve these issues. With just a few lines of code, you can perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset. ATOM should be able to provide quick insights on which algorithms perform best for the task at hand and provide an indication of the feasibility of the ML solution. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you to determine the right model, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. So, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or TPOT ? Well, ATOM does AutoML in the sense that it helps you find the best model for a specific task, but contrary to the aforementioned packages, it does not actively search for the best model. It just runs all of them and let you pick the one that you think suites the best. AutoML packages are often black boxes to which you provide data, and magically, a good model comes out. Although it works great, they often produce complicated pipelines with low explainability, hard to sell to the business. In this, ATOM excels. Every step of the pipeline is accounted for, and using the provided plotting methods, its easy to demonstrate why a model is better/worse than the other. Note A data scientist with domain knowledge can outperform ATOM if he applies usecase-specific feature engineering or data cleaning steps! Possible steps taken by ATOM's pipeline: Data Cleaning Handle missing values Encode categorical features Balance the dataset Remove outliers Perform feature selection Remove features with too high collinearity Remove features with too low variance Select best features according to a chosen strategy Train and validate models Select hyperparameters using a Bayesian Optimization approach Train and test the models on the provided data Perform bagging to assess the robustness of the models Analyze the results","title":"Home"},{"location":"#automated-tool-for-optimized-modelling","text":"There is no magic formula in data science that can tell us which type of machine learning algorithm will perform best for a specific use-case. Best practices tell us to start with a simple model (e.g. linear regression) and build up to more complicated models (e.g. logistic regression -> random forest -> multilayer perceptron) if you are not satisfied with the results. Unfortunately, different models require different data cleaning steps, tuning a new set of hyperparameters, etc. Refactoring the code for all these steps can be very time consuming. This result in many data scientists just using the model best known to them and fine-tuning this particular model without ever trying other ones. This can result in poor performance (because the model is just not the right one for the task) or in poor time management (because you could have achieved a similar performance with a simpler/faster model). ATOM is here to help us solve these issues. With just a few lines of code, you can perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset. ATOM should be able to provide quick insights on which algorithms perform best for the task at hand and provide an indication of the feasibility of the ML solution. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you to determine the right model, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. So, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or TPOT ? Well, ATOM does AutoML in the sense that it helps you find the best model for a specific task, but contrary to the aforementioned packages, it does not actively search for the best model. It just runs all of them and let you pick the one that you think suites the best. AutoML packages are often black boxes to which you provide data, and magically, a good model comes out. Although it works great, they often produce complicated pipelines with low explainability, hard to sell to the business. In this, ATOM excels. Every step of the pipeline is accounted for, and using the provided plotting methods, its easy to demonstrate why a model is better/worse than the other. Note A data scientist with domain knowledge can outperform ATOM if he applies usecase-specific feature engineering or data cleaning steps! Possible steps taken by ATOM's pipeline: Data Cleaning Handle missing values Encode categorical features Balance the dataset Remove outliers Perform feature selection Remove features with too high collinearity Remove features with too low variance Select best features according to a chosen strategy Train and validate models Select hyperparameters using a Bayesian Optimization approach Train and test the models on the provided data Perform bagging to assess the robustness of the models Analyze the results","title":"Automated Tool for Optimized Modelling"},{"location":"dependencies/","text":"Python As of the moment, ATOM supports Python 3.6 , 3.7 and 3.8 . Packages ATOM is built on top of several existing Python libraries. The required packages are necessary for it's correct functioning. Additionnaly, you can install some optional packages needed for specific methods or to use machine learning models not provided by sklearn. Required numpy (>=1.17.2) scipy (>=1.4.1) pandas (>=1.0.3) tqdm (>=4.35.0) joblib (>=0.16.0) typeguard (>=2.7.1) tabulate (>=0.8.6) scikit-learn (>=0.23.1) scikit-optimize (>=0.7.4) pandas-profiling (>=2.3.0) category-encoders (>=2.1.0) imbalanced-learn (>=0.5.0) featuretools (>=0.17.0) gplearn (>=0.4.1) matplotlib (>=3.3.0) seaborn (>=0.9.0) Optional xgboost (>=0.90) lightgbm (>=2.3.0) catboost (>=0.19.1)","title":"Dependencies"},{"location":"dependencies/#python","text":"As of the moment, ATOM supports Python 3.6 , 3.7 and 3.8 .","title":"Python"},{"location":"dependencies/#packages","text":"ATOM is built on top of several existing Python libraries. The required packages are necessary for it's correct functioning. Additionnaly, you can install some optional packages needed for specific methods or to use machine learning models not provided by sklearn. Required numpy (>=1.17.2) scipy (>=1.4.1) pandas (>=1.0.3) tqdm (>=4.35.0) joblib (>=0.16.0) typeguard (>=2.7.1) tabulate (>=0.8.6) scikit-learn (>=0.23.1) scikit-optimize (>=0.7.4) pandas-profiling (>=2.3.0) category-encoders (>=2.1.0) imbalanced-learn (>=0.5.0) featuretools (>=0.17.0) gplearn (>=0.4.1) matplotlib (>=3.3.0) seaborn (>=0.9.0) Optional xgboost (>=0.90) lightgbm (>=2.3.0) catboost (>=0.19.1)","title":"Packages"},{"location":"getting_started/","text":"Installation Note Since atom was already taken, download the package under the name atom-ml ! Intall ATOM's newest release easily via pip : $ pip install -U atom-ml or via conda : $ conda install -c conda-forge atom-ml Usage Call the ATOMClassifier or ATOMRegressor class and provide the data you want to use: from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier X, y = load_breast_cancer(return_X_y) atom = ATOMClassifier(X, y, logger='auto', n_jobs=2, verbose=2) ATOM has multiple data cleaning methods to help you prepare the data for modelling: atom.impute(strat_num='knn', strat_cat='most_frequent', min_frac_rows=0.1) atom.encode(max_onehot=10, frac_to_other=0.05) atom.feature_selection(strategy='PCA', n_features=12) Run the pipeline with the models you want to compare: atom.run(models=['LR', 'LDA', 'XGB', 'lSVM'], metric='f1', n_calls=25, n_random_starts=10, bagging=4) Make plots to analyze the results: atom.plot_bagging(figsize=(9, 6), filename='bagging_results.png') atom.LDA.plot_confusion_matrix(normalize=True, filename='cm.png')","title":"Getting started"},{"location":"getting_started/#installation","text":"Note Since atom was already taken, download the package under the name atom-ml ! Intall ATOM's newest release easily via pip : $ pip install -U atom-ml or via conda : $ conda install -c conda-forge atom-ml","title":"Installation"},{"location":"getting_started/#usage","text":"Call the ATOMClassifier or ATOMRegressor class and provide the data you want to use: from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier X, y = load_breast_cancer(return_X_y) atom = ATOMClassifier(X, y, logger='auto', n_jobs=2, verbose=2) ATOM has multiple data cleaning methods to help you prepare the data for modelling: atom.impute(strat_num='knn', strat_cat='most_frequent', min_frac_rows=0.1) atom.encode(max_onehot=10, frac_to_other=0.05) atom.feature_selection(strategy='PCA', n_features=12) Run the pipeline with the models you want to compare: atom.run(models=['LR', 'LDA', 'XGB', 'lSVM'], metric='f1', n_calls=25, n_random_starts=10, bagging=4) Make plots to analyze the results: atom.plot_bagging(figsize=(9, 6), filename='bagging_results.png') atom.LDA.plot_confusion_matrix(normalize=True, filename='cm.png')","title":"Usage"},{"location":"license/","text":"MIT License Copyright (c) 2020 tvdboom Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2020 tvdboom Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"user_guide/","text":"Introduction There is no magic formula in data science that can tell us which type of machine learning algorithm will perform best for a specific use-case. Best practices tell us to start with a simple model (e.g. linear regression) and build up to more complicated models (e.g. logistic regression -> random forest -> multilayer perceptron) if you are not satisfied with the results. Unfortunately, different models require different data cleaning steps, tuning a new set of hyperparameters, etc. Refactoring the code for all these steps can be very time consuming. This result in many data scientists just using the model best known to them and fine-tuning this particular model without ever trying other ones. This can result in poor performance (because the model is just not the right one for the task) or in poor time management (because you could have achieved a similar performance with a simpler/faster model). ATOM is here to help us solve these issues. With just a few lines of code, you can perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset. ATOM should be able to provide quick insights on which algorithms perform best for the task at hand and provide an indication of the feasibility of the ML solution. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you to determine the right model, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. So, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or TPOT ? Well, ATOM does AutoML in the sense that it helps you find the best model for a specific task, but contrary to the aforementioned packages, it does not actively search for the best model. It just runs all of them and let you pick the one that you think suites the best. AutoML packages are often black boxes to which you provide data, and magically, a good model comes out. Although it works great, they often produce complicated pipelines with low explainability, hard to sell to the business. In this, ATOM excels. Every step of the pipeline is accounted for, and using the provided plotting methods, its easy to demonstrate why a model is better/worse than the other. First steps You can quickly install atom via pip or conda , see the intallation guide . The easiest way to use ATOM is through ATOMClassifier or ATOMRegressor . These classes are wrappers for all other data cleaning, training and plotting methods. Contrary to sklearn's API, the ATOM instance contains the data you want to manipulate. Calling a method will apply all transformations on the data it contains. First, create an ATOM instance and provide the data you want to use. atom = ATOMClassifier(X, y) Apply data cleaning methods through the class. For example, calling the impute will impute all missing values in the dataset. atom.impute(strat_num='median', strat_cat='most_frequent', min_frac_rows=0.1) Fit the models you want to compare to the data. atom.run(['GNB', 'ET', 'MLP], metric='average_precision', n_calls=25, n_random_starts=10) Analyze the results: atom.ET.feature_importances(show=10, filename='feature_importance_plot') atom.plot_prc(title='Precision-recall curve comparison plot') Data cleaning ATOM provides four classes that can help you perform standard data cleaning steps: Standard data cleaning steps Scaling the features Handling missing values Encoding categorical features Handling outliers in the training set Balancing the data Feature engineering \"Applied machine learning\" is basically feature engineering. ~ Andrew Ng. Feature engineering is the process of creating new features from the existing ones, in order to capture relationships with the target column that the first set of features didn't had on their own. This process is very important to improve the performance of machine learning algorithms. Although feature engineering works best when the data scientist applies use-case specific transformations, there are ways to do this in an automated manner, without prior domain knowledge. One of the problems of creating new features without human expert intervention, is that many of the newly created features can be useless, i.e. they do not help the algorithm to make better predictions. Even worse, having useless features can drop your performance. To avoid this, we perform feature selection, a process in which we select the relevant features in the dataset. Click here for an example. Generating new features The FeatureGenerator class creates new non-linear features based on the original feature set. It can be accessed from an ATOM instance through the feature_generation method. You can choose between two strategies: Deep Feature Synthesis and Genetic Feature Generation. Deep Feature Synthesis Deep feature synthesis (DFS) applies the selected operators on the features in the dataset. For example, if the operator is 'log', it will create the new feature LOG(old_feature) and if the operator is 'mul', it will create the new feature old_feature_1 x old_feature_2 . DFS can create many new features. The operators can be chosen through the operators parameter. Available options are: add: Sum two features together. sub: Subtract two features from each other. mul: Multiply two features with each other. div: Divide two features with each other. srqt: Take the square root of a feature. log: Take the logarithm of a feature. sin: Calculate the sine of a feature. cos: Calculate the cosine of a feature. tan: Calculate the tangent of a feature. ATOM's implementation of DFS uses the featuretools package. Tip DFS can create many new features and not all of them will be useful. Use the FeatureSelector class to reduce the number of features! Warning When using DFS with n_jobs>1, make sure to protect your code with a if __name__ == \"__main__\". Featuretools uses dask, which uses python multiprocessing for parallelization. The spawn method on multiprocessing starts a new python process, which requires it to import the __main__ module before it can do its task. Genetic Feature Generation Genetic feature generation (GFG) uses genetic programming , a branch of evolutionary programming, to determine which features are successful and create new ones based on those. Where DFS' method can be seen as some kind of \"brute force\" for feature engineering, GFG tries to improve its features with every generation of the algorithm. GFG uses the same operators as DFS, but instead of only applying the transformations once, it evolves them further, creating complicated non-linear combinations of features with many transformations. The new features are given the name Feature N for the N-th feature. You can access the genetic feature's fitness and description (how they are calculated) through the genetic_features attribute. ATOM uses the SymbolicTransformer class from the gplearn package for the genetic algorithm. Read more about this implementation here . Selecting useful features The FeatureSelector class provides tooling to select the relevant features from a dataset. It can be accessed from an ATOM instance through the feature_selection method. The following strategies are implemented: univariate, PCA, SFM, RFE and RFECV. Tip Use the plot_feature_importance method to examine how much a specific feature contributes to the fianl predictions. If the model doesn't have a feature_importances_ attribute, use plot_permutation_importance instead. Univariate Univariate feature selection works by selecting the best features based on univariate statistical F-test. The test is provided via the solver parameter. It takes any function taking two arrays (X, y), and returning arrays (scores, p-values). Read more in the sklearn documentation . Principal Components Analysis Applying PCA will reduce the dimensionality of the dataset by maximizing the variance of each dimension. The new features will be called Component 0, Component 1, etc... The dataset will be scaled before applying the transformation (if it wasn't already). Selection from model SFM uses a supervised machine learning model with feature_importances_ or coef_ attributes to select the best features in a dataset based on importance weights. The model is provided through the solver parameter and can be already fitted. ATOM allows you to input one of the pre-defined models, e.g. solver='RF' . If you called the FeatureSelector class without using the ATOM wrapper, don't forget to indicate the task adding _class or _reg after the name, e.g. RF_class to use a random forest classifier. Recursive feature elimination Select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Note that, since RFE needs to fit the model again every iteration, this method can be fairly slow. RFECV applies the same algorithm as RFE but uses a cross-validated metric (under the scoring parameter, see RFECV ) to assess every step's performance. Also, where RFE returns the number of features selected by n_features , RFECV returns the number of features that achieved the optimal score on the specified metric. Note that this is not always equal to the amount specified by n_features . Removing features with low variance Variance is the expectation of the squared deviation of a random variable from its mean. Features with low variance have many values repeated, which means the model will not learn a lot from them. FeatureSelector removes all features where the same value is repeated in at least max_frac_repeated fraction of the rows. The default option is to remove a feature if all values in it are the same. Removing features with multi-collinearity Two features that are highly correlated are redundant. Two will not contribute more than only one of them. FeatureSelector will remove one of two features that have a Pearson correlation coefficient larger than max_correlation . The default option is to remove one of 2 equal columns. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute. Model fitting and evaluation Multi-metric runs Hyperparameter tuning Bagging Early stopping Model subclasses Handle new data streams It is possible, that after running the whole ATOM pipeline, you would like to apply the same data transformations and maybe make predictions on a new dataset. This is possible using ATOM's prediction methods . Plotting After fitting the models to the data, it's time to analyze the results. ATOM provides many plotting methods to compare the model performances. Click here for a list of the available plots. The plot descriptions and examples can be find in the API section. ATOM uses the matplotlib and seaborn packages for plotting. The plot methods can be called from the ATOM instance directly, e.g. atom.plot_roc() , or from one of the model subclasses, e.g. atom.LGB.plot_roc() . If called from ATOM, it will make the plot for all models in the pipeline. This can be useful to compare the results of multiple models. If called from the model subclass, it will only make the plot for that model. Use this option if you want information just for a specific model or to make the plot a bit less crowded. Apart from the plot-specific parameters they may have, all plots have four parameters in common: The title parameter allows you to add a custom title to the plot. The figsize parameter allows you to adjust the plot's size. The filename parameter is used to save the plot as a .png file. The display parameter determines whether the plot is rendered. You can customize the following plot aesthetics using the plotting properties : Seaborn style Color palette Title fontsize Label and legend fontsize Tick fontsize","title":"User guide"},{"location":"user_guide/#introduction","text":"There is no magic formula in data science that can tell us which type of machine learning algorithm will perform best for a specific use-case. Best practices tell us to start with a simple model (e.g. linear regression) and build up to more complicated models (e.g. logistic regression -> random forest -> multilayer perceptron) if you are not satisfied with the results. Unfortunately, different models require different data cleaning steps, tuning a new set of hyperparameters, etc. Refactoring the code for all these steps can be very time consuming. This result in many data scientists just using the model best known to them and fine-tuning this particular model without ever trying other ones. This can result in poor performance (because the model is just not the right one for the task) or in poor time management (because you could have achieved a similar performance with a simpler/faster model). ATOM is here to help us solve these issues. With just a few lines of code, you can perform basic data cleaning steps, select relevant features and compare the performance of multiple models on a given dataset. ATOM should be able to provide quick insights on which algorithms perform best for the task at hand and provide an indication of the feasibility of the ML solution. It is important to realize that ATOM is not here to replace all the work a data scientist has to do before getting his model into production. ATOM doesn't spit out production-ready models just by tuning some parameters in its API. After helping you to determine the right model, you will most probably need to fine-tune it using use-case specific features and data cleaning steps in order to achieve maximum performance. So, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or TPOT ? Well, ATOM does AutoML in the sense that it helps you find the best model for a specific task, but contrary to the aforementioned packages, it does not actively search for the best model. It just runs all of them and let you pick the one that you think suites the best. AutoML packages are often black boxes to which you provide data, and magically, a good model comes out. Although it works great, they often produce complicated pipelines with low explainability, hard to sell to the business. In this, ATOM excels. Every step of the pipeline is accounted for, and using the provided plotting methods, its easy to demonstrate why a model is better/worse than the other.","title":"Introduction"},{"location":"user_guide/#first-steps","text":"You can quickly install atom via pip or conda , see the intallation guide . The easiest way to use ATOM is through ATOMClassifier or ATOMRegressor . These classes are wrappers for all other data cleaning, training and plotting methods. Contrary to sklearn's API, the ATOM instance contains the data you want to manipulate. Calling a method will apply all transformations on the data it contains. First, create an ATOM instance and provide the data you want to use. atom = ATOMClassifier(X, y) Apply data cleaning methods through the class. For example, calling the impute will impute all missing values in the dataset. atom.impute(strat_num='median', strat_cat='most_frequent', min_frac_rows=0.1) Fit the models you want to compare to the data. atom.run(['GNB', 'ET', 'MLP], metric='average_precision', n_calls=25, n_random_starts=10) Analyze the results: atom.ET.feature_importances(show=10, filename='feature_importance_plot') atom.plot_prc(title='Precision-recall curve comparison plot')","title":"First steps"},{"location":"user_guide/#data-cleaning","text":"ATOM provides four classes that can help you perform standard data cleaning steps:","title":"Data cleaning"},{"location":"user_guide/#standard-data-cleaning-steps","text":"","title":"Standard data cleaning steps"},{"location":"user_guide/#scaling-the-features","text":"","title":"Scaling the features"},{"location":"user_guide/#handling-missing-values","text":"","title":"Handling missing values"},{"location":"user_guide/#encoding-categorical-features","text":"","title":"Encoding categorical features"},{"location":"user_guide/#handling-outliers-in-the-training-set","text":"","title":"Handling outliers in the training set"},{"location":"user_guide/#balancing-the-data","text":"","title":"Balancing the data"},{"location":"user_guide/#feature-engineering","text":"\"Applied machine learning\" is basically feature engineering. ~ Andrew Ng. Feature engineering is the process of creating new features from the existing ones, in order to capture relationships with the target column that the first set of features didn't had on their own. This process is very important to improve the performance of machine learning algorithms. Although feature engineering works best when the data scientist applies use-case specific transformations, there are ways to do this in an automated manner, without prior domain knowledge. One of the problems of creating new features without human expert intervention, is that many of the newly created features can be useless, i.e. they do not help the algorithm to make better predictions. Even worse, having useless features can drop your performance. To avoid this, we perform feature selection, a process in which we select the relevant features in the dataset. Click here for an example.","title":"Feature engineering"},{"location":"user_guide/#generating-new-features","text":"The FeatureGenerator class creates new non-linear features based on the original feature set. It can be accessed from an ATOM instance through the feature_generation method. You can choose between two strategies: Deep Feature Synthesis and Genetic Feature Generation. Deep Feature Synthesis Deep feature synthesis (DFS) applies the selected operators on the features in the dataset. For example, if the operator is 'log', it will create the new feature LOG(old_feature) and if the operator is 'mul', it will create the new feature old_feature_1 x old_feature_2 . DFS can create many new features. The operators can be chosen through the operators parameter. Available options are: add: Sum two features together. sub: Subtract two features from each other. mul: Multiply two features with each other. div: Divide two features with each other. srqt: Take the square root of a feature. log: Take the logarithm of a feature. sin: Calculate the sine of a feature. cos: Calculate the cosine of a feature. tan: Calculate the tangent of a feature. ATOM's implementation of DFS uses the featuretools package. Tip DFS can create many new features and not all of them will be useful. Use the FeatureSelector class to reduce the number of features! Warning When using DFS with n_jobs>1, make sure to protect your code with a if __name__ == \"__main__\". Featuretools uses dask, which uses python multiprocessing for parallelization. The spawn method on multiprocessing starts a new python process, which requires it to import the __main__ module before it can do its task. Genetic Feature Generation Genetic feature generation (GFG) uses genetic programming , a branch of evolutionary programming, to determine which features are successful and create new ones based on those. Where DFS' method can be seen as some kind of \"brute force\" for feature engineering, GFG tries to improve its features with every generation of the algorithm. GFG uses the same operators as DFS, but instead of only applying the transformations once, it evolves them further, creating complicated non-linear combinations of features with many transformations. The new features are given the name Feature N for the N-th feature. You can access the genetic feature's fitness and description (how they are calculated) through the genetic_features attribute. ATOM uses the SymbolicTransformer class from the gplearn package for the genetic algorithm. Read more about this implementation here .","title":"Generating new features"},{"location":"user_guide/#selecting-useful-features","text":"The FeatureSelector class provides tooling to select the relevant features from a dataset. It can be accessed from an ATOM instance through the feature_selection method. The following strategies are implemented: univariate, PCA, SFM, RFE and RFECV. Tip Use the plot_feature_importance method to examine how much a specific feature contributes to the fianl predictions. If the model doesn't have a feature_importances_ attribute, use plot_permutation_importance instead. Univariate Univariate feature selection works by selecting the best features based on univariate statistical F-test. The test is provided via the solver parameter. It takes any function taking two arrays (X, y), and returning arrays (scores, p-values). Read more in the sklearn documentation . Principal Components Analysis Applying PCA will reduce the dimensionality of the dataset by maximizing the variance of each dimension. The new features will be called Component 0, Component 1, etc... The dataset will be scaled before applying the transformation (if it wasn't already). Selection from model SFM uses a supervised machine learning model with feature_importances_ or coef_ attributes to select the best features in a dataset based on importance weights. The model is provided through the solver parameter and can be already fitted. ATOM allows you to input one of the pre-defined models, e.g. solver='RF' . If you called the FeatureSelector class without using the ATOM wrapper, don't forget to indicate the task adding _class or _reg after the name, e.g. RF_class to use a random forest classifier. Recursive feature elimination Select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Note that, since RFE needs to fit the model again every iteration, this method can be fairly slow. RFECV applies the same algorithm as RFE but uses a cross-validated metric (under the scoring parameter, see RFECV ) to assess every step's performance. Also, where RFE returns the number of features selected by n_features , RFECV returns the number of features that achieved the optimal score on the specified metric. Note that this is not always equal to the amount specified by n_features . Removing features with low variance Variance is the expectation of the squared deviation of a random variable from its mean. Features with low variance have many values repeated, which means the model will not learn a lot from them. FeatureSelector removes all features where the same value is repeated in at least max_frac_repeated fraction of the rows. The default option is to remove a feature if all values in it are the same. Removing features with multi-collinearity Two features that are highly correlated are redundant. Two will not contribute more than only one of them. FeatureSelector will remove one of two features that have a Pearson correlation coefficient larger than max_correlation . The default option is to remove one of 2 equal columns. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute.","title":"Selecting useful features"},{"location":"user_guide/#model-fitting-and-evaluation","text":"","title":"Model fitting and evaluation"},{"location":"user_guide/#multi-metric-runs","text":"","title":"Multi-metric runs"},{"location":"user_guide/#hyperparameter-tuning","text":"","title":"Hyperparameter tuning"},{"location":"user_guide/#bagging","text":"","title":"Bagging"},{"location":"user_guide/#early-stopping","text":"","title":"Early stopping"},{"location":"user_guide/#model-subclasses","text":"","title":"Model subclasses"},{"location":"user_guide/#handle-new-data-streams","text":"It is possible, that after running the whole ATOM pipeline, you would like to apply the same data transformations and maybe make predictions on a new dataset. This is possible using ATOM's prediction methods .","title":"Handle new data streams"},{"location":"user_guide/#plotting","text":"After fitting the models to the data, it's time to analyze the results. ATOM provides many plotting methods to compare the model performances. Click here for a list of the available plots. The plot descriptions and examples can be find in the API section. ATOM uses the matplotlib and seaborn packages for plotting. The plot methods can be called from the ATOM instance directly, e.g. atom.plot_roc() , or from one of the model subclasses, e.g. atom.LGB.plot_roc() . If called from ATOM, it will make the plot for all models in the pipeline. This can be useful to compare the results of multiple models. If called from the model subclass, it will only make the plot for that model. Use this option if you want information just for a specific model or to make the plot a bit less crowded. Apart from the plot-specific parameters they may have, all plots have four parameters in common: The title parameter allows you to add a custom title to the plot. The figsize parameter allows you to adjust the plot's size. The filename parameter is used to save the plot as a .png file. The display parameter determines whether the plot is rendered. You can customize the following plot aesthetics using the plotting properties : Seaborn style Color palette Title fontsize Label and legend fontsize Tick fontsize","title":"Plotting"},{"location":"API/ATOM/atomclassifier/","text":"ATOMClassifier class atom.api. ATOMClassifier (X, y=-1, n_rows=1, test_size=0.2, logger=None, n_jobs=1, warnings=True, verbose=0, random_state=None) [source] ATOMClassifier is the ATOM wrapper for classification tasks. These can include binary and multiclass. Use this class to easily apply all data transformations and model management of the ATOM package on a given dataset. Note that contrary to scikit-learn's API, the ATOMClassifier object already contains the dataset on which we want to perform the analysis. Calling a method will automatically apply it on the dataset it contains. The class initializer always calls StandardCleaner with default parameters. The following data types can't (yet) be handled properly by ATOMClassifier and are therefore removed: 'datetime64', 'datetime64[ns]', 'timedelta[ns]'. Parameters: X: dict, sequence, np.array or pd.DataFrame Dataset containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=-1) If int: Position of the target column in X. The default value selects the last column. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) n_rows: int or float, optional (default=1) if < =1: Fraction of the data to use. if >1: Number of rows to use. test_size: float, optional (default=0.2) Split fraction for the training and test set. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, optional (default=True) If True: Default warning action (equal to 'default' when string). If False: Suppress all warnings (equal to 'ignore' when string). If str: One of the possible actions in python's warnings environment. Note that changing this parameter will affect the PYTHONWARNINGS environment. Note that ATOM can't manage warnings that go directly from C++ code to the stdout/stderr. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' for default name. If class: python Logger object. Note that warnings will not be saved to the logger in any case. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random. Properties Data properties The dataset within ATOM's pipeline can be accessed at any time through multiple properties, e.g. calling atom.train will return the training set. The data can also be changed through these properties, e.g. atom.test = atom.test.drop(0) will drop the first row from the test set. This will also update the other data properties. Data properties: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. Plotting properties The plot aesthetics can be customized using the properties described hereunder, e.g. atom.style = 'white' . Plotting properties: style: str Seaborn plotting style. See the documentation . palette: str Seaborn color palette. See the documentation . title_fontsize: int Fontsize for plot titles. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for ticks. Attributes Attributes: mapping: dict Dictionary of the target values mapped to their respective encoded integer. profile: ProfileReport Profile created by pandas-profiling after calling the report method. genetic_features: pd.DataFrame Dataframe of the non-linear features created by the feature_generation method. Columns include: name: Name of the feature (automatically created). description: Operators used to create this feature. fitness: Fitness score. collinear: pd.DataFrame Dataframe of the collinear features removed by the feature_selection method. Columns include: drop_feature: name of the feature dropped by the method. correlated feature: Name of the correlated feature(s). correlation_value: Pearson correlation coefficient(s) of the feature pairs. errors: dict Dictionary of the encountered exceptions (if any) after calling any of the training methods. winner: model subclass Model subclass that performed best on the test set. If multi-metric, only the first metric is checked. results: pd.DataFrame Dataframe of the training results with model's acronym as index. For successive_halving and train_sizing , an extra index level is added to indicate the different runs. Columns can include: name: Name of the model. score_bo: Best score achieved during the BO. time_bo: Time spent on the BO. score_train: Score on the training set. score_test: Score on the test set. time_fit: Time spent fitting and evaluating. mean_bagging: Mean score of the bagging's results. std_bagging: Standard deviation score of the bagging's results. time_bagging: Time spent on the bagging algorithm. time: Total time spent on the whole run. Utility methods The ATOM class contains a variety of methods to help you handle the data and inspect the pipeline. stats Print out a list of basic statistics on the dataset. log Save information to the logger and print to stdout. report Get an extensive profile analysis of the data. calibrate Calibrate the winning model. scoring Print the scoring of the models for a specific metric. save Save the ATOMClassifier instance to a pickle file. function ATOMClassifier. stats () [source] Print out a list of basic information on the dataset. function ATOMClassifier. log (msg, level=0) [source] Save information to the ATOM logger and print it to stdout. Parameters: msg: str Message to save to the logger and print to stdout. level: int, optional (default=0) Minimum verbosity level in order to print the message. function ATOMClassifier. report (df='dataset', n_rows=None, filename=None) [source] Get an extensive profile analysis of the data. The report is rendered in HTML5 and CSS3 and saved to the profile attribute. Note that this method can be slow for n_rows > 10k. Parameters: df: str, optional (default='dataset') Name of the data class property to get the profile from. n_rows: int or None, optional (default=None) Number of (randomly picked) rows to process. None for all rows. filename: str or None, optional (default=None) Name of the file when saved (as .html). None to not save anything. function ATOMClassifier. scoring (metric=None) [source] Print the scoring of the models for a specific metric. If a model shows a XXX , it means the metric failed for that specific model. This can happen if either the metric is unavailable for classification tasks or if the model does not have a predict_proba method while the metric requires it. Parameters: metric: str or None, optional (default=None) String of one of sklearn's predefined metrics (see documentation ). If None, the metric used to run the pipeline is selected. function ATOMClassifier. calibrate (**kwargs) [source] Applies probability calibration on the winning model. The calibration is done with the CalibratedClassifierCV class from sklearn. The model will be trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the model attribute. Parameters: **kwargs Additional keyword arguments for the CalibratedClassifierCV instance. Using cv='prefit' will use the trained model and fit the calibrator on the test set. Note that doing this will result in data leakage in the test set. Use this only if you have another, independent set for testing. function ATOMClassifier. save (filename=None, save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as property, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str or None, optional (default=None) Name to save the file with. If None or 'auto', use default name (ATOMClassifier). save_data: bool, optional (default=True) Whether to save the data as an attribute of the instance. If False, remember to update the data immediately after loading the pickle using the dataset's @setter. Data cleaning Usually, before throwing your data in a model, you need to apply some data cleaning steps. ATOM provides four data cleaning methods to handle missing values, categorical columns, outliers and unbalanced datasets. Calling on one of them will automatically apply the method on the dataset in the pipeline. Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. scale Scale all the features to mean=1 and std=0. impute Handle missing values in the dataset. encode Encode categorical columns. outliers Remove outliers from the training set. balance Balance the number of rows per category. function ATOMClassifier. scale () [source] Scale the feature set to mean=1 and std=0. This method calls the Cleaner class under the hood and fits it only on the training set to avoid data leakage. function ATOMClassifier. impute (strat_num='drop', strat_cat='drop', min_frac_rows=0.5, min_frac_cols=0.5, missing=None) [source] Handle missing values according to the selected strategy. Also removes rows and columns with too many missing values. The imputer is fitted only on the training set to avoid data leakage. See Imputer for a description of the parameters. function ATOMClassifier. encode (max_onehot=10, encode_type='LeaveOneOut', frac_to_other=None) [source] Perform encoding of categorical features. The encoding type depends on the number of unique values in the column: If n_unique=2, use label-encoding. If 2 < n_unique <= max_onehot, use one-hot-encoding. If n_unique > max_onehot, use encode_type . Also replaces classes with low occurrences with the value 'other' in order to prevent too high cardinality. Categorical features are defined as all columns whose dtype.kind not in 'ifu'. The encoder is fitted only on the training set to avoid data leakage. See Encoder for a description of the parameters. function ATOMClassifier. outliers (strategy='drop', max_sigma=3, include_target=False) [source] Remove or replace outliers in the training set. Outliers are defined as values that lie further than max_sigma * standard_deviation away from the mean of the column. See Outliers for a description of the parameters. function ATOMClassifier. balance (oversample=None, undersample=None, n_neighbors=5) [source] Balance the number of instances per target category in the training set. Using oversample and undersample at the same time or not using any will raise an exception. Only the training set is balanced in order to maintain the original distribution of target categories in the test set. See Balancer for a description of the parameters. Feature selection To further pre-process the data you can create new non-linear features using a genetic algorithm or, if your dataset is too large, remove features using one of the provided strategies. feature_generation Use a genetic algorithm to create new combinations of existing features. feature_selection Remove features according to the selected strategy. function ATOMClassifier. feature_generation (strategy='dfs', n_features=None, generations=20, population=500, operators=None) [source] Use Deep feature Synthesis or a genetic algorithm to create new combinations of existing features to capture the non-linear relations between the original features. See FeatureGenerator for a description of the parameters. function ATOMClassifier. feature_selection (strategy=None, solver=None, n_features=None, max_frac_repeated=1., max_correlation=1., **kwargs) [source] Remove features according to the selected strategy. Ties between features with equal scores will be broken in an unspecified way. Also removes features with too low variance and finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified limit (in terms of absolute value), it removes one of the two. See FeatureSelector for a description of the parameters. Note When strategy='univariate' and solver=None, f_classif will be used as default solver. When strategy is one of 'SFM', 'RFE' or 'RFECV' and the solver is one of ATOM's models, the algorithm will automatically select the classifier (no need to add _class to the solver). When strategy is one of 'SFM', 'RFE' or 'RFECV' and solver=None, ATOM will use the winning model (if it exists) as solver. When strategy='RFECV', ATOM will use the metric in the pipeline (if it exists) as the scoring parameter (only if not specified manually). Training The training methods are where the models are fitted to the data and their performance is evaluated according to the selected metric. ATOMClassifier contains three methods to call the training classes from the ATOM package. All relevant attributes and methods from the training classes are attached to ATOMClassifier for convenience. These include the errors, winner and results attributes, the model subclasses, and the prediction and plotting methods. run Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function ATOMClassifier. run (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_params={}, bagging=None) [source] Calls a TrainerClassifier instance. Using this class through ATOMClassifier allows subsequent runs with different models without losing previous information (only the model subclasses are overwritten if the same model is rerun). function ATOMClassifier. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_params={}, bagging=None) [source] Calls a SuccessiveHalvingClassifier instance. function ATOMClassifier. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspace(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_params={}, bagging=None) [source] Calls a TrainSizingClassifier instance. Model subclasses After running any of the training methods, a class for every selected model is created and attached to the main ATOMClassifier instance as an attribute. We call these classes model subclasses. They can be accessed using the models' acronyms, e.g. atom.LGB for the LightGBM model subclass. Lowercase calls are also allowed for this attribute, e.g. atom.lgb . The model subclasses contain a variety of methods and attributes to help you understand how every specific model performed. Attributes You can see the data used to train and test every specific model using the same data properties as the ATOMClassifier has. These can differ from each other if the model needs scaled features and the data wasn't already scaled. Note that the data can not be updated from the model subclasses (i.e. the data properties have no @setter ). A list of the remaining available attributes can be found hereunder: Attributes: bo: pd.DataFrame Dataframe containing the information of every step taken by the BO. Columns include: 'params': Parameters used in the model. 'model': Model used for this iteration (fitted on last cross-validation). 'score': Score of the chosen metric. List of scores for multi-metric. 'time_iteration': Time spent on this iteration. 'time': Total ime spent since the start of the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. model: class Model instance with the best combination of hyperparameters fitted on the complete training set. predict_train: np.ndarray Predictions of the model on the training set. predict_test: np.ndarray Predictions of the model on the test set. predict_proba_train: np.ndarray Predict probabilities of the model on the training set. Only for models with a `predict_proba` method. predict_proba_test: np.ndarray Predict probabilities of the model on the test set. Only for models with a `predict_proba` method. predict_log_proba_train: np.ndarray Predict log probabilities of the model on the training set. Only for models with a `predict_proba` method. predict_log_proba_test: np.ndarray Predict log probabilities of the model on the test set. Only for models with a `predict_proba` method. decision_function_train: np.ndarray Decision function scores on the training set. Only for models with a `decision_function` method. decision_function_test: np.ndarray Decision function scores on the test set. Only for models with a `decision_function` method. time_bo: str Time it took to run the bayesian optimization algorithm. score_bo: float Best score of the model on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric on the test set. score_train: float Metric score of the model on the training set. score_test: float Metric score of the model on the test set. evals: dict Dictionary of the metric calculated during training. The metric is provided by the model's package and is different for every model and every task. Only for models that allow in-training evaluation (XGB, LGB, CatB). Available keys: 'metric': Name of the used metric. 'train': List of scores calculated on the training set. 'test': List of scores calculated on the test set. score_bagging: list Array of the bagging's results. mean_bagging: float Mean of the bagging's results. std_bagging: float Standard deviation of the bagging's results. permutations: dict Dictionary of the permutation's results (if `plot_permutation_importance` was used). Methods The majority of the plots can be called directly from the subclasses. For example, to plot the ROC for the LightGBM model we could type atom.lgb.plot_roc() . A list of the remaining methods can be found hereunder: calibrate Calibrate the model. scoring Get the scoring of a specific metric on the test set. save_model Save the model to a pickle file. function BaseModel. calibrate (**kwargs) [source] Applies probability calibration on the winning model. The calibration is done with the CalibratedClassifierCV class from sklearn. The model will be trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the model attribute. Parameters: **kwargs Additional keyword arguments for the CalibratedClassifierCV instance. Using cv='prefit' will use the trained model and fit the calibrator on the test set. Note that doing this will result in data leakage in the test set. Use this only if you have another, independent set for testing. function BaseModel. scoring (metric=None) [source] Get the scoring of a specific metric on the test set. Parameters: metric: str or None, optional (default=None) Name of the metric to calculate. If None, return mean_bagging if exists, else score_test. If string, choose from any of sklearn's SCORERS or one of the following custom metrics: 'cm' or 'confusion_matrix' for a (2x2)-array of the confusion matrix. 'tn' for true negatives. 'fp' for false positives. 'fn' for false negatives. 'lift' for the lift metric. 'fpr' for the false positive rate. 'tpr' for true positive rate. 'sup' for the support metric. function BaseModel. save_model (filename=None) [source] Save the model (fitted to the complete training set) to a pickle file. Parameters: filename: str or None, optional (default=None) Name of the file to save. If None or 'auto', use default name (<name>_model). Prediction methods Like the majority of estimators in scikit-learn, you can use a fitted instance of ATOMClassifier to make predictions onto new data, e.g. atom.predict(X) . The following methods will apply all selected data pre-processing steps on the provided data first, and use the winning model from the pipeline (under attribute winner ) to make the predictions. If you want to use a different model, you can call the method from the model subclass, e.g. atom.LGB.predict(X) . transform Transform new data through all the pre-processing data steps. predict Make predictions on new data. predict_proba Make probability predictions on new data. predict_log_proba Make logarithmic probability predictions on new data. decision_function Return the decision function of a model on new data. score Return the score of a model on new data. function ATOMClassifier. transform (X, standard_cleaner=True, scale=True, impute=True, encode=True, outliers=False, balance=False, feature_generation=True, feature_selection=True, verbose=None) [source] Transform new data through all the pre-processing steps. The outliers and balancer steps are not included in the default steps since they should only be applied on the training set. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). standard_cleaner: bool, optional (default=True) Whether to apply the standard cleaning step in the transformer. scale: bool, optional (default=True) Whether to apply the scaler step in the transformer. impute: bool, optional (default=True) Whether to apply the imputer step in the transformer. encode: bool, optional (default=True) Whether to apply the encoder step in the transformer. outliers: bool, optional (default=False) Whether to apply the outliers step in the transformer. balance: bool, optional (default=False) Whether to apply the balance step in the transformer. feature_generation: bool, optional (default=True) Whether to apply the feature_generation step in the transformer. feature_selection: bool, optional (default=True) Whether to apply the feature_selection step in the transformer. verbose: int, optional (default=None) Verbosity level of the output. If None, it uses ATOM's verbosity. function ATOMClassifier. predict (X, **kwargs) [source] Transform the data and make predictions using the winning model in the pipeline. The model has to have a predict method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. predict_proba (X, **kwargs) [source] Transform the data and make probability predictions using the winning model in the pipeline. The model has to have a predict_proba method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. predict_log_proba (X, **kwargs) [source] Transform the data and make logarithmic probability predictions using the winning model in the pipeline. The model has to have a predict_log_proba method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. decision_function (X, **kwargs) [source] Transform the data and run the decision function of the winning model in the pipeline. The model has to have a decision_function method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. score (X, y, **kwargs) [source] Transform the data and run the scoring method of the winning model in the pipeline. The model has to have a score method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, dict, sequence, np.array or pd.Series If int: Index of the column of X which is selected as target. If str: Name of the target column in X. Else: Data target column with shape=(n_samples,). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. Plots Just like the prediction methods, the plots from the training class can be called directly from the instance, e.g. atom.plot_prc() , or from the model subclasses, e.g. atom.LDA.plot_prc() . The plots aesthetics can be customized using the plotting properties . Available plots are: plot_correlation Plot the correlation matrix of the dataset. plot_pca Plot the explained variance ratio vs the number of components. plot_components Plot the explained variance ratio per component. plot_rfecv Plot the scores obtained by the estimator on the RFECV. plot_successive_halving Plot the models' scores per iteration of the successive halving. plot_learning_curve Plot the model's learning curve: score vs training samples. plot_bagging Plot a boxplot of the bagging's results. plot_bo Plot the bayesian optimization scoring. plot_evals Plot evaluation curves for the train and test set. plot_roc Plot the Receiver Operating Characteristics curve. plot_prc Plot the precision-recall curve. plot_permutation_importance Plot the feature permutation importance of models. plot_feature_importance Plot the feature permutation importance of models. plot_confusion_matrix Plot a model's confusion matrix. plot_threshold Plot performance metric(s) against threshold values. plot_probabilities Plot the probabilities of the different classes of belonging to the target class. plot_calibration Plot the calibration curve for a binary classifier. plot_gains Plot the cumulative gains curve. plot_lift Plot the lift curve. Example from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier X, y = load_breast_cancer(return_X_y=True) # Initialize class atom = ATOMClassifier(X, y, logger='auto', n_jobs=2, verbose=2) # Apply data cleaning methods atom.impute(strat_num='knn', strat_cat='most_frequent', min_frac_rows=0.1) atom.encode(max_onehot=10, frac_to_other=0.05) atom.balance(oversample=0.7) # Fit the models to the data atom.run(models=['QDA', 'CatB'], metric='precision', n_calls=25, n_random_starts=10, bo_params={'cv': 1}, bagging=4) # Analyze the results print(f\"The winning model is: {atom.winner.name}\") print(atom.results) # Make some plots atom.palette = 'Blues' atom.plot_roc(figsize=(9, 6), filename='roc.png') atom.CatB.plot_feature_importance(filename='catboost_feature_importance.png') # Run an extra model atom.run(models='LR', metric='precision', n_calls=25, n_random_starts=10, bo_params={'cv': 1}, bagging=4) # Get the predictions for the best model on new data predictions = atom.predict(X_new)","title":"ATOMClassifier"},{"location":"API/ATOM/atomclassifier/#atomclassifier","text":"class atom.api. ATOMClassifier (X, y=-1, n_rows=1, test_size=0.2, logger=None, n_jobs=1, warnings=True, verbose=0, random_state=None) [source] ATOMClassifier is the ATOM wrapper for classification tasks. These can include binary and multiclass. Use this class to easily apply all data transformations and model management of the ATOM package on a given dataset. Note that contrary to scikit-learn's API, the ATOMClassifier object already contains the dataset on which we want to perform the analysis. Calling a method will automatically apply it on the dataset it contains. The class initializer always calls StandardCleaner with default parameters. The following data types can't (yet) be handled properly by ATOMClassifier and are therefore removed: 'datetime64', 'datetime64[ns]', 'timedelta[ns]'. Parameters: X: dict, sequence, np.array or pd.DataFrame Dataset containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=-1) If int: Position of the target column in X. The default value selects the last column. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) n_rows: int or float, optional (default=1) if < =1: Fraction of the data to use. if >1: Number of rows to use. test_size: float, optional (default=0.2) Split fraction for the training and test set. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. warnings: bool or str, optional (default=True) If True: Default warning action (equal to 'default' when string). If False: Suppress all warnings (equal to 'ignore' when string). If str: One of the possible actions in python's warnings environment. Note that changing this parameter will affect the PYTHONWARNINGS environment. Note that ATOM can't manage warnings that go directly from C++ code to the stdout/stderr. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' for default name. If class: python Logger object. Note that warnings will not be saved to the logger in any case. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random.","title":"ATOMClassifier"},{"location":"API/ATOM/atomclassifier/#properties","text":"","title":"Properties"},{"location":"API/ATOM/atomclassifier/#data-properties","text":"The dataset within ATOM's pipeline can be accessed at any time through multiple properties, e.g. calling atom.train will return the training set. The data can also be changed through these properties, e.g. atom.test = atom.test.drop(0) will drop the first row from the test set. This will also update the other data properties. Data properties: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target.","title":"Data properties"},{"location":"API/ATOM/atomclassifier/#plotting-properties","text":"The plot aesthetics can be customized using the properties described hereunder, e.g. atom.style = 'white' . Plotting properties: style: str Seaborn plotting style. See the documentation . palette: str Seaborn color palette. See the documentation . title_fontsize: int Fontsize for plot titles. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for ticks.","title":"Plotting properties"},{"location":"API/ATOM/atomclassifier/#attributes","text":"Attributes: mapping: dict Dictionary of the target values mapped to their respective encoded integer. profile: ProfileReport Profile created by pandas-profiling after calling the report method. genetic_features: pd.DataFrame Dataframe of the non-linear features created by the feature_generation method. Columns include: name: Name of the feature (automatically created). description: Operators used to create this feature. fitness: Fitness score. collinear: pd.DataFrame Dataframe of the collinear features removed by the feature_selection method. Columns include: drop_feature: name of the feature dropped by the method. correlated feature: Name of the correlated feature(s). correlation_value: Pearson correlation coefficient(s) of the feature pairs. errors: dict Dictionary of the encountered exceptions (if any) after calling any of the training methods. winner: model subclass Model subclass that performed best on the test set. If multi-metric, only the first metric is checked. results: pd.DataFrame Dataframe of the training results with model's acronym as index. For successive_halving and train_sizing , an extra index level is added to indicate the different runs. Columns can include: name: Name of the model. score_bo: Best score achieved during the BO. time_bo: Time spent on the BO. score_train: Score on the training set. score_test: Score on the test set. time_fit: Time spent fitting and evaluating. mean_bagging: Mean score of the bagging's results. std_bagging: Standard deviation score of the bagging's results. time_bagging: Time spent on the bagging algorithm. time: Total time spent on the whole run.","title":"Attributes"},{"location":"API/ATOM/atomclassifier/#utility-methods","text":"The ATOM class contains a variety of methods to help you handle the data and inspect the pipeline. stats Print out a list of basic statistics on the dataset. log Save information to the logger and print to stdout. report Get an extensive profile analysis of the data. calibrate Calibrate the winning model. scoring Print the scoring of the models for a specific metric. save Save the ATOMClassifier instance to a pickle file. function ATOMClassifier. stats () [source] Print out a list of basic information on the dataset. function ATOMClassifier. log (msg, level=0) [source] Save information to the ATOM logger and print it to stdout. Parameters: msg: str Message to save to the logger and print to stdout. level: int, optional (default=0) Minimum verbosity level in order to print the message. function ATOMClassifier. report (df='dataset', n_rows=None, filename=None) [source] Get an extensive profile analysis of the data. The report is rendered in HTML5 and CSS3 and saved to the profile attribute. Note that this method can be slow for n_rows > 10k. Parameters: df: str, optional (default='dataset') Name of the data class property to get the profile from. n_rows: int or None, optional (default=None) Number of (randomly picked) rows to process. None for all rows. filename: str or None, optional (default=None) Name of the file when saved (as .html). None to not save anything. function ATOMClassifier. scoring (metric=None) [source] Print the scoring of the models for a specific metric. If a model shows a XXX , it means the metric failed for that specific model. This can happen if either the metric is unavailable for classification tasks or if the model does not have a predict_proba method while the metric requires it. Parameters: metric: str or None, optional (default=None) String of one of sklearn's predefined metrics (see documentation ). If None, the metric used to run the pipeline is selected. function ATOMClassifier. calibrate (**kwargs) [source] Applies probability calibration on the winning model. The calibration is done with the CalibratedClassifierCV class from sklearn. The model will be trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the model attribute. Parameters: **kwargs Additional keyword arguments for the CalibratedClassifierCV instance. Using cv='prefit' will use the trained model and fit the calibrator on the test set. Note that doing this will result in data leakage in the test set. Use this only if you have another, independent set for testing. function ATOMClassifier. save (filename=None, save_data=True) [source] Save the instance to a pickle file. Remember that the class contains the complete dataset as property, so the file can become large for big datasets! To avoid this, use save_data=False . Parameters: filename: str or None, optional (default=None) Name to save the file with. If None or 'auto', use default name (ATOMClassifier). save_data: bool, optional (default=True) Whether to save the data as an attribute of the instance. If False, remember to update the data immediately after loading the pickle using the dataset's @setter.","title":"Utility methods"},{"location":"API/ATOM/atomclassifier/#data-cleaning","text":"Usually, before throwing your data in a model, you need to apply some data cleaning steps. ATOM provides four data cleaning methods to handle missing values, categorical columns, outliers and unbalanced datasets. Calling on one of them will automatically apply the method on the dataset in the pipeline. Tip Use the report method to examine the data and help you determine suitable parameters for the data cleaning methods. scale Scale all the features to mean=1 and std=0. impute Handle missing values in the dataset. encode Encode categorical columns. outliers Remove outliers from the training set. balance Balance the number of rows per category. function ATOMClassifier. scale () [source] Scale the feature set to mean=1 and std=0. This method calls the Cleaner class under the hood and fits it only on the training set to avoid data leakage. function ATOMClassifier. impute (strat_num='drop', strat_cat='drop', min_frac_rows=0.5, min_frac_cols=0.5, missing=None) [source] Handle missing values according to the selected strategy. Also removes rows and columns with too many missing values. The imputer is fitted only on the training set to avoid data leakage. See Imputer for a description of the parameters. function ATOMClassifier. encode (max_onehot=10, encode_type='LeaveOneOut', frac_to_other=None) [source] Perform encoding of categorical features. The encoding type depends on the number of unique values in the column: If n_unique=2, use label-encoding. If 2 < n_unique <= max_onehot, use one-hot-encoding. If n_unique > max_onehot, use encode_type . Also replaces classes with low occurrences with the value 'other' in order to prevent too high cardinality. Categorical features are defined as all columns whose dtype.kind not in 'ifu'. The encoder is fitted only on the training set to avoid data leakage. See Encoder for a description of the parameters. function ATOMClassifier. outliers (strategy='drop', max_sigma=3, include_target=False) [source] Remove or replace outliers in the training set. Outliers are defined as values that lie further than max_sigma * standard_deviation away from the mean of the column. See Outliers for a description of the parameters. function ATOMClassifier. balance (oversample=None, undersample=None, n_neighbors=5) [source] Balance the number of instances per target category in the training set. Using oversample and undersample at the same time or not using any will raise an exception. Only the training set is balanced in order to maintain the original distribution of target categories in the test set. See Balancer for a description of the parameters.","title":"Data cleaning"},{"location":"API/ATOM/atomclassifier/#feature-selection","text":"To further pre-process the data you can create new non-linear features using a genetic algorithm or, if your dataset is too large, remove features using one of the provided strategies. feature_generation Use a genetic algorithm to create new combinations of existing features. feature_selection Remove features according to the selected strategy. function ATOMClassifier. feature_generation (strategy='dfs', n_features=None, generations=20, population=500, operators=None) [source] Use Deep feature Synthesis or a genetic algorithm to create new combinations of existing features to capture the non-linear relations between the original features. See FeatureGenerator for a description of the parameters. function ATOMClassifier. feature_selection (strategy=None, solver=None, n_features=None, max_frac_repeated=1., max_correlation=1., **kwargs) [source] Remove features according to the selected strategy. Ties between features with equal scores will be broken in an unspecified way. Also removes features with too low variance and finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified limit (in terms of absolute value), it removes one of the two. See FeatureSelector for a description of the parameters. Note When strategy='univariate' and solver=None, f_classif will be used as default solver. When strategy is one of 'SFM', 'RFE' or 'RFECV' and the solver is one of ATOM's models, the algorithm will automatically select the classifier (no need to add _class to the solver). When strategy is one of 'SFM', 'RFE' or 'RFECV' and solver=None, ATOM will use the winning model (if it exists) as solver. When strategy='RFECV', ATOM will use the metric in the pipeline (if it exists) as the scoring parameter (only if not specified manually).","title":"Feature selection"},{"location":"API/ATOM/atomclassifier/#training","text":"The training methods are where the models are fitted to the data and their performance is evaluated according to the selected metric. ATOMClassifier contains three methods to call the training classes from the ATOM package. All relevant attributes and methods from the training classes are attached to ATOMClassifier for convenience. These include the errors, winner and results attributes, the model subclasses, and the prediction and plotting methods. run Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function ATOMClassifier. run (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_params={}, bagging=None) [source] Calls a TrainerClassifier instance. Using this class through ATOMClassifier allows subsequent runs with different models without losing previous information (only the model subclasses are overwritten if the same model is rerun). function ATOMClassifier. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_params={}, bagging=None) [source] Calls a SuccessiveHalvingClassifier instance. function ATOMClassifier. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspace(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_params={}, bagging=None) [source] Calls a TrainSizingClassifier instance.","title":"Training"},{"location":"API/ATOM/atomclassifier/#model-subclasses","text":"After running any of the training methods, a class for every selected model is created and attached to the main ATOMClassifier instance as an attribute. We call these classes model subclasses. They can be accessed using the models' acronyms, e.g. atom.LGB for the LightGBM model subclass. Lowercase calls are also allowed for this attribute, e.g. atom.lgb . The model subclasses contain a variety of methods and attributes to help you understand how every specific model performed.","title":"Model subclasses"},{"location":"API/ATOM/atomclassifier/#attributes_1","text":"You can see the data used to train and test every specific model using the same data properties as the ATOMClassifier has. These can differ from each other if the model needs scaled features and the data wasn't already scaled. Note that the data can not be updated from the model subclasses (i.e. the data properties have no @setter ). A list of the remaining available attributes can be found hereunder: Attributes: bo: pd.DataFrame Dataframe containing the information of every step taken by the BO. Columns include: 'params': Parameters used in the model. 'model': Model used for this iteration (fitted on last cross-validation). 'score': Score of the chosen metric. List of scores for multi-metric. 'time_iteration': Time spent on this iteration. 'time': Total ime spent since the start of the BO. best_params: dict Dictionary of the best combination of hyperparameters found by the BO. model: class Model instance with the best combination of hyperparameters fitted on the complete training set. predict_train: np.ndarray Predictions of the model on the training set. predict_test: np.ndarray Predictions of the model on the test set. predict_proba_train: np.ndarray Predict probabilities of the model on the training set. Only for models with a `predict_proba` method. predict_proba_test: np.ndarray Predict probabilities of the model on the test set. Only for models with a `predict_proba` method. predict_log_proba_train: np.ndarray Predict log probabilities of the model on the training set. Only for models with a `predict_proba` method. predict_log_proba_test: np.ndarray Predict log probabilities of the model on the test set. Only for models with a `predict_proba` method. decision_function_train: np.ndarray Decision function scores on the training set. Only for models with a `decision_function` method. decision_function_test: np.ndarray Decision function scores on the test set. Only for models with a `decision_function` method. time_bo: str Time it took to run the bayesian optimization algorithm. score_bo: float Best score of the model on the BO. time_fit: str Time it took to train the model on the complete training set and calculate the metric on the test set. score_train: float Metric score of the model on the training set. score_test: float Metric score of the model on the test set. evals: dict Dictionary of the metric calculated during training. The metric is provided by the model's package and is different for every model and every task. Only for models that allow in-training evaluation (XGB, LGB, CatB). Available keys: 'metric': Name of the used metric. 'train': List of scores calculated on the training set. 'test': List of scores calculated on the test set. score_bagging: list Array of the bagging's results. mean_bagging: float Mean of the bagging's results. std_bagging: float Standard deviation of the bagging's results. permutations: dict Dictionary of the permutation's results (if `plot_permutation_importance` was used).","title":"Attributes"},{"location":"API/ATOM/atomclassifier/#methods","text":"The majority of the plots can be called directly from the subclasses. For example, to plot the ROC for the LightGBM model we could type atom.lgb.plot_roc() . A list of the remaining methods can be found hereunder: calibrate Calibrate the model. scoring Get the scoring of a specific metric on the test set. save_model Save the model to a pickle file. function BaseModel. calibrate (**kwargs) [source] Applies probability calibration on the winning model. The calibration is done with the CalibratedClassifierCV class from sklearn. The model will be trained via cross-validation on a subset of the training data, using the rest to fit the calibrator. The new classifier will replace the model attribute. Parameters: **kwargs Additional keyword arguments for the CalibratedClassifierCV instance. Using cv='prefit' will use the trained model and fit the calibrator on the test set. Note that doing this will result in data leakage in the test set. Use this only if you have another, independent set for testing. function BaseModel. scoring (metric=None) [source] Get the scoring of a specific metric on the test set. Parameters: metric: str or None, optional (default=None) Name of the metric to calculate. If None, return mean_bagging if exists, else score_test. If string, choose from any of sklearn's SCORERS or one of the following custom metrics: 'cm' or 'confusion_matrix' for a (2x2)-array of the confusion matrix. 'tn' for true negatives. 'fp' for false positives. 'fn' for false negatives. 'lift' for the lift metric. 'fpr' for the false positive rate. 'tpr' for true positive rate. 'sup' for the support metric. function BaseModel. save_model (filename=None) [source] Save the model (fitted to the complete training set) to a pickle file. Parameters: filename: str or None, optional (default=None) Name of the file to save. If None or 'auto', use default name (<name>_model).","title":"Methods"},{"location":"API/ATOM/atomclassifier/#prediction-methods","text":"Like the majority of estimators in scikit-learn, you can use a fitted instance of ATOMClassifier to make predictions onto new data, e.g. atom.predict(X) . The following methods will apply all selected data pre-processing steps on the provided data first, and use the winning model from the pipeline (under attribute winner ) to make the predictions. If you want to use a different model, you can call the method from the model subclass, e.g. atom.LGB.predict(X) . transform Transform new data through all the pre-processing data steps. predict Make predictions on new data. predict_proba Make probability predictions on new data. predict_log_proba Make logarithmic probability predictions on new data. decision_function Return the decision function of a model on new data. score Return the score of a model on new data. function ATOMClassifier. transform (X, standard_cleaner=True, scale=True, impute=True, encode=True, outliers=False, balance=False, feature_generation=True, feature_selection=True, verbose=None) [source] Transform new data through all the pre-processing steps. The outliers and balancer steps are not included in the default steps since they should only be applied on the training set. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). standard_cleaner: bool, optional (default=True) Whether to apply the standard cleaning step in the transformer. scale: bool, optional (default=True) Whether to apply the scaler step in the transformer. impute: bool, optional (default=True) Whether to apply the imputer step in the transformer. encode: bool, optional (default=True) Whether to apply the encoder step in the transformer. outliers: bool, optional (default=False) Whether to apply the outliers step in the transformer. balance: bool, optional (default=False) Whether to apply the balance step in the transformer. feature_generation: bool, optional (default=True) Whether to apply the feature_generation step in the transformer. feature_selection: bool, optional (default=True) Whether to apply the feature_selection step in the transformer. verbose: int, optional (default=None) Verbosity level of the output. If None, it uses ATOM's verbosity. function ATOMClassifier. predict (X, **kwargs) [source] Transform the data and make predictions using the winning model in the pipeline. The model has to have a predict method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. predict_proba (X, **kwargs) [source] Transform the data and make probability predictions using the winning model in the pipeline. The model has to have a predict_proba method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. predict_log_proba (X, **kwargs) [source] Transform the data and make logarithmic probability predictions using the winning model in the pipeline. The model has to have a predict_log_proba method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. decision_function (X, **kwargs) [source] Transform the data and run the decision function of the winning model in the pipeline. The model has to have a decision_function method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer. function ATOMClassifier. score (X, y, **kwargs) [source] Transform the data and run the scoring method of the winning model in the pipeline. The model has to have a score method. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, dict, sequence, np.array or pd.Series If int: Index of the column of X which is selected as target. If str: Name of the target column in X. Else: Data target column with shape=(n_samples,). **kwargs Same arguments as the transform method to include/exclude pre-processing steps from the transformer.","title":"Prediction methods"},{"location":"API/ATOM/atomclassifier/#plots","text":"Just like the prediction methods, the plots from the training class can be called directly from the instance, e.g. atom.plot_prc() , or from the model subclasses, e.g. atom.LDA.plot_prc() . The plots aesthetics can be customized using the plotting properties . Available plots are: plot_correlation Plot the correlation matrix of the dataset. plot_pca Plot the explained variance ratio vs the number of components. plot_components Plot the explained variance ratio per component. plot_rfecv Plot the scores obtained by the estimator on the RFECV. plot_successive_halving Plot the models' scores per iteration of the successive halving. plot_learning_curve Plot the model's learning curve: score vs training samples. plot_bagging Plot a boxplot of the bagging's results. plot_bo Plot the bayesian optimization scoring. plot_evals Plot evaluation curves for the train and test set. plot_roc Plot the Receiver Operating Characteristics curve. plot_prc Plot the precision-recall curve. plot_permutation_importance Plot the feature permutation importance of models. plot_feature_importance Plot the feature permutation importance of models. plot_confusion_matrix Plot a model's confusion matrix. plot_threshold Plot performance metric(s) against threshold values. plot_probabilities Plot the probabilities of the different classes of belonging to the target class. plot_calibration Plot the calibration curve for a binary classifier. plot_gains Plot the cumulative gains curve. plot_lift Plot the lift curve.","title":"Plots"},{"location":"API/ATOM/atomclassifier/#example","text":"from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier X, y = load_breast_cancer(return_X_y=True) # Initialize class atom = ATOMClassifier(X, y, logger='auto', n_jobs=2, verbose=2) # Apply data cleaning methods atom.impute(strat_num='knn', strat_cat='most_frequent', min_frac_rows=0.1) atom.encode(max_onehot=10, frac_to_other=0.05) atom.balance(oversample=0.7) # Fit the models to the data atom.run(models=['QDA', 'CatB'], metric='precision', n_calls=25, n_random_starts=10, bo_params={'cv': 1}, bagging=4) # Analyze the results print(f\"The winning model is: {atom.winner.name}\") print(atom.results) # Make some plots atom.palette = 'Blues' atom.plot_roc(figsize=(9, 6), filename='roc.png') atom.CatB.plot_feature_importance(filename='catboost_feature_importance.png') # Run an extra model atom.run(models='LR', metric='precision', n_calls=25, n_random_starts=10, bo_params={'cv': 1}, bagging=4) # Get the predictions for the best model on new data predictions = atom.predict(X_new)","title":"Example"},{"location":"API/ATOM/atomregressor/","text":"class atom. ATOM (X, y=None, n_rows=1, test_size=0.3, log=None, n_jobs=1, warnings=True, verbose=0, random_state=None) [source] Main class of the package. The ATOM class is a parent class of the ATOMClassifier and ATOMRegressor classes. These will inherit all methods and attributes described in this page. Note that contrary to scikit-learn's API, the ATOM object already contains the dataset on which we want to perform the analysis. Calling a method will automatically apply it on the dataset it contains. Warning Don't call the ATOM class directly! Use ATOMClassifier or ATOMRegressor depending on the task at hand. Click here for an example. The class initializer will label-encode the target column if its labels are not ordered integers. It will also apply some standard data cleaning steps unto the dataset. These steps include: Transforming the input data into a pd.DataFrame (if it wasn't one already) that can be accessed through the class' data attributes. Strip categorical features from white spaces. Removing columns with prohibited data types ('datetime64', 'datetime64[ns]', 'timedelta[ns]'). ATOM can't (yet) handle these types. Removing categorical columns with maximal cardinality (the number of unique values is equal to the number of instances. Usually the case for names, IDs, etc...). Removing columns with minimum cardinality (all values are the same). Removing rows with missing values in the target column. Parameters: X: dict, sequence, np.array or pd.DataFrame Dataset containing the features, with shape=(n_samples, n_features). y: string, sequence, np.array or pd.Series, optional (default=None) If None: the last column of X is selected as target column If string: name of the target column in X Else: data target column with shape=(n_samples,) n_rows: int or float, optional (default=1) if < =1: fraction of the data to use. if >1: number of rows to use. test_size: float, optional (default=0.3) Split fraction of the train and test set. log: string or None, optional (default=None) Name of the logging file. 'auto' for default name with date and time. None to not save any log. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If -1, use all available cores If < -1, use available_cores - 1 + n_jobs Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything 1 to print minimum information 2 to print average information 3 to print maximum information warnings: bool, optional (default=True) If False, suppresses all warnings. Note that this will change the PYTHONWARNINGS environment. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random. Data attributes: dataset: pd.DataFrame Complete dataset in the pipeline. train: pd.DataFrame Training set. test: pd.DataFrame Test set. X: pd.DataFrame Feature set. y: pd.Series Target column. X_train: pd.DataFrame Training features. y_train: pd.Series Training target. X_test: pd.DataFrame Test features. y_test: pd.Series Test target. Attributes: mapping: dict Dictionary of the target values mapped to their respective encoded integer. Only for classification tasks. errors: dict Dictionary of the encountered exceptions (if any) while fitting the models. winner: callable Model subclass that performed best on the test set. scores: pd.DataFrame Dataframe (or list of dataframes if the pipeline was called using successive_halving or train_sizing) of the results. Columns can include: model: model's name (acronym). total_time: time spent on this model. score_train: metric score on the training set. score_test: metric score on the test set. fit_time: time spent fitting and predicting. bagging_mean: mean score of the bagging's results. bagging_std: standard deviation score of the bagging's results. bagging_time: time spent on the bagging algorithm.","title":"ATOMRegressor"},{"location":"API/data_cleaning/balancer/","text":"Balancer class atom.data_cleaning. Balancer (oversample=None, undersample=None, n_neighbors=5, n_jobs=1, verbose=0, logger=None, random_state=None) [source] Balance the number of instances per target category. Using oversample and undersample at the same time or not using any will raise an exception. Use only for classification tasks. Dependency: imbalanced-learn . Parameters: oversample: float, string or None, optional (default=None) Oversampling strategy using ADASYN . Choose from: None: Don't oversample. float: Fraction minority/majority (only for binary classification). 'minority': Resample only the minority category. 'not minority': Resample all but the minority category. 'not majority': Resample all but the majority category. 'all': Resample all categories. undersample: float, string or None, optional (default=None) Undersampling strategy using NearMiss . Choose from: None: Don't oversample. float: Fraction minority/majority (only for binary classification). 'majority': Resample only the majority category. 'not minority': Resample all but the minority category. 'not majority': Resample all but the majority category. 'all': Resample all categories. n_neighbors: int, optional (default=5) Number of nearest neighbors used for the ADASYN and NearMiss algorithms. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random. Methods fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Balancer. fit_transform (X, y) [source] Oversample or undersample the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. function Balancer. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Balancer. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Balancer. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Balancer Estimator instance. function Balancer. transform (X, y) [source] Oversample or undersample the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Example from atom.data_cleaning import Balancer Balancer = Balancer(undersample='not majority', n_neigbors=10, verbose=2, random_state=1) X_balanced, y_balanced = Balancer.transform(X, y)","title":"Balancer"},{"location":"API/data_cleaning/balancer/#balancer","text":"class atom.data_cleaning. Balancer (oversample=None, undersample=None, n_neighbors=5, n_jobs=1, verbose=0, logger=None, random_state=None) [source] Balance the number of instances per target category. Using oversample and undersample at the same time or not using any will raise an exception. Use only for classification tasks. Dependency: imbalanced-learn . Parameters: oversample: float, string or None, optional (default=None) Oversampling strategy using ADASYN . Choose from: None: Don't oversample. float: Fraction minority/majority (only for binary classification). 'minority': Resample only the minority category. 'not minority': Resample all but the minority category. 'not majority': Resample all but the majority category. 'all': Resample all categories. undersample: float, string or None, optional (default=None) Undersampling strategy using NearMiss . Choose from: None: Don't oversample. float: Fraction minority/majority (only for binary classification). 'majority': Resample only the majority category. 'not minority': Resample all but the minority category. 'not majority': Resample all but the majority category. 'all': Resample all categories. n_neighbors: int, optional (default=5) Number of nearest neighbors used for the ADASYN and NearMiss algorithms. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random.","title":"Balancer"},{"location":"API/data_cleaning/balancer/#methods","text":"fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Balancer. fit_transform (X, y) [source] Oversample or undersample the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. function Balancer. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Balancer. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Balancer. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Balancer Estimator instance. function Balancer. transform (X, y) [source] Oversample or undersample the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column.","title":"Methods"},{"location":"API/data_cleaning/balancer/#example","text":"from atom.data_cleaning import Balancer Balancer = Balancer(undersample='not majority', n_neigbors=10, verbose=2, random_state=1) X_balanced, y_balanced = Balancer.transform(X, y)","title":"Example"},{"location":"API/data_cleaning/encoder/","text":"Encoder class atom.data_cleaning. Encoder (max_onehot=10, encode_type='Target', frac_to_other=None, verbose=0, logger=None, **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of unique values in the column: If n_unique=2, use label-encoding. If 2 < n_unique <= max_onehot, use one-hot-encoding. If n_unique > max_onehot, use encode_type . Also replaces classes with low occurrences with the value 'other' in order to prevent too high cardinality. Categorical features are defined as all columns whose dtype.kind not in 'ifu'. Parameters: max_onehot: int or None, optional (default=10) Maximum number of unique values in a feature to perform one-hot-encoding. If None, it will never do one-hot-encoding. encode_type: str, optional (default='Target') Type of encoding to use for high cardinality features. Choose from one of the encoders available in the category_encoders package (only HashingEncoder is excluded). frac_to_other: float, optional (default=None) Categories with less rows than n_rows * fraction_to_other are replaced with the string 'other'. If None, skip this step. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. **kwargs Additional keyword arguments passed to the encoder selected by the 'encode_type' parameter. Methods fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Encoder. fit (X, y) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: Encoder Fitted instance of self. function Encoder. fit_transform (X, y) [source] Fit the Encoder and return the encoded data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. function Encoder. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Encoder. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Encoder. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Encoder Estimator instance. function Encoder. transform (X, y=None) [source] Encode the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Transformed feature set. Example from atom.data_cleaning import Encoder encoder = Encoder(max_onehot=5, encode_type='LeaveOneOut') X_encoded = encoder.fit_transform(X, y)","title":"Encoder"},{"location":"API/data_cleaning/encoder/#encoder","text":"class atom.data_cleaning. Encoder (max_onehot=10, encode_type='Target', frac_to_other=None, verbose=0, logger=None, **kwargs) [source] Perform encoding of categorical features. The encoding type depends on the number of unique values in the column: If n_unique=2, use label-encoding. If 2 < n_unique <= max_onehot, use one-hot-encoding. If n_unique > max_onehot, use encode_type . Also replaces classes with low occurrences with the value 'other' in order to prevent too high cardinality. Categorical features are defined as all columns whose dtype.kind not in 'ifu'. Parameters: max_onehot: int or None, optional (default=10) Maximum number of unique values in a feature to perform one-hot-encoding. If None, it will never do one-hot-encoding. encode_type: str, optional (default='Target') Type of encoding to use for high cardinality features. Choose from one of the encoders available in the category_encoders package (only HashingEncoder is excluded). frac_to_other: float, optional (default=None) Categories with less rows than n_rows * fraction_to_other are replaced with the string 'other'. If None, skip this step. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. **kwargs Additional keyword arguments passed to the encoder selected by the 'encode_type' parameter.","title":"Encoder"},{"location":"API/data_cleaning/encoder/#methods","text":"fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Encoder. fit (X, y) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: Encoder Fitted instance of self. function Encoder. fit_transform (X, y) [source] Fit the Encoder and return the encoded data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. function Encoder. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Encoder. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Encoder. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Encoder Estimator instance. function Encoder. transform (X, y=None) [source] Encode the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Transformed feature set.","title":"Methods"},{"location":"API/data_cleaning/encoder/#example","text":"from atom.data_cleaning import Encoder encoder = Encoder(max_onehot=5, encode_type='LeaveOneOut') X_encoded = encoder.fit_transform(X, y)","title":"Example"},{"location":"API/data_cleaning/imputer/","text":"Imputer class atom.data_cleaning. Imputer (strat_num='drop', strat_cat='drop', min_frac_rows=0.5, min_frac_cols=0.5, missing=None, verbose=0, logger=None) [source] Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Parameters: strat_num: str, int or float, optional (default='drop') Imputing strategy for numerical columns. Choose from: 'drop': Drop rows containing missing values. 'mean': Impute with mean of column. 'median': Impute with median of column. 'knn': Impute using a K-Nearest Neighbors approach. 'most_frequent': Impute with most frequent value. int or float: Impute with provided numerical value. strat_cat: str, optional (default='drop') Imputing strategy for categorical columns. Choose from: 'drop': Drop rows containing missing values. 'most_frequent': Impute with most frequent value. string: Impute with provided string. min_frac_rows: float, optional (default=0.5) Minimum fraction of non-missing values in a row. If less, the row is removed. min_frac_cols: float, optional (default=0.5) Minimum fraction of non-missing values in a column. If less, the column is removed. missing: int, float or list, optional (default=None) List of values to treat as 'missing'. None to use the default values: [None, np.NaN, np.inf, -np.inf, '', '?', 'NA', 'nan', 'inf']. Note that np.NaN, None, np.inf and -np.inf will always be imputed because of the incompatibility with the models. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. Methods fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Imputer. fit (X, y=None) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: self: Imputer Fitted instance of self. function Imputer. fit_transform (X, y=None) [source] Fit the Imputer and return the imputed data. Note Leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped in the transformation. Use this option only for prediction methods. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If None: y is not used in the transformation. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. function Imputer. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Imputer. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Imputer. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: imputer Estimator instance. function Imputer. transform (X, y=None) [source] Impute the data. Note Leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped in the transformation. Use this option only for prediction methods. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If None: y is not used in the transformation. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. Example from atom.data_cleaning import Imputer imputer = Imputer(strat_num='knn', strat_cat='drop', min_frac_cols=0.8) X_imputed, y = imputer.fit_transform(X, y)","title":"Imputer"},{"location":"API/data_cleaning/imputer/#imputer","text":"class atom.data_cleaning. Imputer (strat_num='drop', strat_cat='drop', min_frac_rows=0.5, min_frac_cols=0.5, missing=None, verbose=0, logger=None) [source] Impute or remove missing values according to the selected strategy. Also removes rows and columns with too many missing values. Parameters: strat_num: str, int or float, optional (default='drop') Imputing strategy for numerical columns. Choose from: 'drop': Drop rows containing missing values. 'mean': Impute with mean of column. 'median': Impute with median of column. 'knn': Impute using a K-Nearest Neighbors approach. 'most_frequent': Impute with most frequent value. int or float: Impute with provided numerical value. strat_cat: str, optional (default='drop') Imputing strategy for categorical columns. Choose from: 'drop': Drop rows containing missing values. 'most_frequent': Impute with most frequent value. string: Impute with provided string. min_frac_rows: float, optional (default=0.5) Minimum fraction of non-missing values in a row. If less, the row is removed. min_frac_cols: float, optional (default=0.5) Minimum fraction of non-missing values in a column. If less, the column is removed. missing: int, float or list, optional (default=None) List of values to treat as 'missing'. None to use the default values: [None, np.NaN, np.inf, -np.inf, '', '?', 'NA', 'nan', 'inf']. Note that np.NaN, None, np.inf and -np.inf will always be imputed because of the incompatibility with the models. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object.","title":"Imputer"},{"location":"API/data_cleaning/imputer/#methods","text":"fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Imputer. fit (X, y=None) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: self: Imputer Fitted instance of self. function Imputer. fit_transform (X, y=None) [source] Fit the Imputer and return the imputed data. Note Leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped in the transformation. Use this option only for prediction methods. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If None: y is not used in the transformation. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. function Imputer. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Imputer. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Imputer. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: imputer Estimator instance. function Imputer. transform (X, y=None) [source] Impute the data. Note Leaving y=None can lead to inconsistencies in data length between X and y if rows are dropped in the transformation. Use this option only for prediction methods. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If None: y is not used in the transformation. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/imputer/#example","text":"from atom.data_cleaning import Imputer imputer = Imputer(strat_num='knn', strat_cat='drop', min_frac_cols=0.8) X_imputed, y = imputer.fit_transform(X, y)","title":"Example"},{"location":"API/data_cleaning/outliers/","text":"Outliers class atom.data_cleaning. Outliers (srategy='drop', max_sigma=3, include_target=False, verbose=0, logger=None, **kwargs) [source] Remove or replace outliers in the data. Outliers are defined as values that lie further than max_sigma * standard_deviation away from the mean of the column. Parameters: strategy: int, float or str, optional (default='drop') Strategy to apply on the outliers. Choose from: 'drop': Drop any row with outliers. 'min_max': Replace the outlier with the min or max of the column. Any numerical value with which to replace the outliers. max_sigma: int or float, optional (default=3) Maximum allowed standard deviations from the mean of the column. If more, it is considered an outlier. include_target: bool, optional (default=False) Whether to include the target column in the transformation. This can be useful for regression tasks. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. Methods fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Outliers. fit_transform (X, y=None) [source] Apply the outlier strategy on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided. function Outliers. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Outliers. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Outliers. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Outliers Estimator instance. function Outliers. transform (X, y=None) [source] Apply the outlier strategy on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided. Example from atom.data_cleaning import Outliers Outliers = Outliers(strategy='min_max', max_sigma=2, include_target=True) X_transformed, y_transformed = Outliers.transform(X, y)","title":"Outliers"},{"location":"API/data_cleaning/outliers/#outliers","text":"class atom.data_cleaning. Outliers (srategy='drop', max_sigma=3, include_target=False, verbose=0, logger=None, **kwargs) [source] Remove or replace outliers in the data. Outliers are defined as values that lie further than max_sigma * standard_deviation away from the mean of the column. Parameters: strategy: int, float or str, optional (default='drop') Strategy to apply on the outliers. Choose from: 'drop': Drop any row with outliers. 'min_max': Replace the outlier with the min or max of the column. Any numerical value with which to replace the outliers. max_sigma: int or float, optional (default=3) Maximum allowed standard deviations from the mean of the column. If more, it is considered an outlier. include_target: bool, optional (default=False) Whether to include the target column in the transformation. This can be useful for regression tasks. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object.","title":"Outliers"},{"location":"API/data_cleaning/outliers/#methods","text":"fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Outliers. fit_transform (X, y=None) [source] Apply the outlier strategy on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided. function Outliers. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Outliers. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Outliers. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Outliers Estimator instance. function Outliers. transform (X, y=None) [source] Apply the outlier strategy on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. X: pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/outliers/#example","text":"from atom.data_cleaning import Outliers Outliers = Outliers(strategy='min_max', max_sigma=2, include_target=True) X_transformed, y_transformed = Outliers.transform(X, y)","title":"Example"},{"location":"API/data_cleaning/scaler/","text":"Scaler class atom.data_cleaning. Scaler (verbose=0, logger=None) [source] Scales data to mean=0 and std=1. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). Parameters: verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. Methods fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Scaler. fit (X, y=None) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: self: Scaler Fitted instance of self. function Scaler. fit_transform (X, y=None) [source] Fit the Scaler and return the scaled data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. function Scaler. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Scaler. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Scaler. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Scaler Estimator instance. function Scaler. transform (X, y=None) [source] Scale the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. Example from atom.data_cleaning import Scaler X_scaled = Scaler().fit_transform(X)","title":"Scaler"},{"location":"API/data_cleaning/scaler/#scaler","text":"class atom.data_cleaning. Scaler (verbose=0, logger=None) [source] Scales data to mean=0 and std=1. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). Parameters: verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object.","title":"Scaler"},{"location":"API/data_cleaning/scaler/#methods","text":"fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function Scaler. fit (X, y=None) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: self: Scaler Fitted instance of self. function Scaler. fit_transform (X, y=None) [source] Fit the Scaler and return the scaled data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set. function Scaler. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function Scaler. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function Scaler. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: Scaler Estimator instance. function Scaler. transform (X, y=None) [source] Scale the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Scaled feature set.","title":"Methods"},{"location":"API/data_cleaning/scaler/#example","text":"from atom.data_cleaning import Scaler X_scaled = Scaler().fit_transform(X)","title":"Example"},{"location":"API/data_cleaning/standard_cleaner/","text":"StandardCleaner class atom.data_cleaning. standardcleaner (prohibited_types=[], strip_categorical=True, maximum_cardinality=True, minimum_cardinality=True, missing_target=True, map_target=None, verbose=0, logger=None) [source] Performs standard data cleaning steps on a dataset. These steps can include: Strip categorical features from white spaces. Removing columns with prohibited data types. Removing categorical columns with maximal cardinality. Removing columns with minimum cardinality. Removing rows with missing values in the target column. Label-encode the target column. Parameters: prohibited_types: str or sequence, optional (default=[]) Columns with any of these types will be removed from the dataset. strip_categorical: bool, optional (default=True) Whether to strip the spaces from values in the categorical columns. maximum_cardinality: bool, optional (default=True) Whether to remove categorical columns with maximum cardinality, i.e. the number of unique values is equal to the number of instances. Usually the case for names, IDs, etc... minimum_cardinality: bool, optional (default=True) Whether to remove columns with minimum cardinality, i.e. all values in the column are the same. missing_target: bool, optional (default=True) Whether to remove rows with missing values in the target column. Ignored if y is not provided. map_target: bool or None, optional (default=None) Whether to map the target column to numerical values. Should only be used for classification tasks. If None, infer task from the provided target column and set to True if it is classification. Ignored if y is not provided or if it already consists of ordered integers. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. Attributes Attributes: mapping: dict Dictionary of the target values mapped to their respective encoded integer. Only available if map_target was performed. Methods fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function StandardCleaner. fit_transform (X, y=None) [source] Apply the data cleaning steps on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in the estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. function StandardCleaner. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function StandardCleaner. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function StandardCleaner. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: StandardCleaner Estimator instance. function StandardCleaner. transform (X, y=None) [source] Apply the data cleaning steps on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) If None: y is not used in the estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. Example from atom.data_cleaning import StandardCleaner cleaner = StandardCleaner(prohinited_types=['str'], target_mapping=False) X_cleaned, y_cleaned = cleaner.transform(X, y)","title":"StandardCleaner"},{"location":"API/data_cleaning/standard_cleaner/#standardcleaner","text":"class atom.data_cleaning. standardcleaner (prohibited_types=[], strip_categorical=True, maximum_cardinality=True, minimum_cardinality=True, missing_target=True, map_target=None, verbose=0, logger=None) [source] Performs standard data cleaning steps on a dataset. These steps can include: Strip categorical features from white spaces. Removing columns with prohibited data types. Removing categorical columns with maximal cardinality. Removing columns with minimum cardinality. Removing rows with missing values in the target column. Label-encode the target column. Parameters: prohibited_types: str or sequence, optional (default=[]) Columns with any of these types will be removed from the dataset. strip_categorical: bool, optional (default=True) Whether to strip the spaces from values in the categorical columns. maximum_cardinality: bool, optional (default=True) Whether to remove categorical columns with maximum cardinality, i.e. the number of unique values is equal to the number of instances. Usually the case for names, IDs, etc... minimum_cardinality: bool, optional (default=True) Whether to remove columns with minimum cardinality, i.e. all values in the column are the same. missing_target: bool, optional (default=True) Whether to remove rows with missing values in the target column. Ignored if y is not provided. map_target: bool or None, optional (default=None) Whether to map the target column to numerical values. Should only be used for classification tasks. If None, infer task from the provided target column and set to True if it is classification. Ignored if y is not provided or if it already consists of ordered integers. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object.","title":"StandardCleaner"},{"location":"API/data_cleaning/standard_cleaner/#attributes","text":"Attributes: mapping: dict Dictionary of the target values mapped to their respective encoded integer. Only available if map_target was performed.","title":"Attributes"},{"location":"API/data_cleaning/standard_cleaner/#methods","text":"fit_transform Same as transform. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function StandardCleaner. fit_transform (X, y=None) [source] Apply the data cleaning steps on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in the estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided. function StandardCleaner. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function StandardCleaner. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function StandardCleaner. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: StandardCleaner Estimator instance. function StandardCleaner. transform (X, y=None) [source] Apply the data cleaning steps on the data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series, optional (default=None) If None: y is not used in the estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. y: pd.Series Transformed target column. Only returned if provided.","title":"Methods"},{"location":"API/data_cleaning/standard_cleaner/#example","text":"from atom.data_cleaning import StandardCleaner cleaner = StandardCleaner(prohinited_types=['str'], target_mapping=False) X_cleaned, y_cleaned = cleaner.transform(X, y)","title":"Example"},{"location":"API/feature_engineering/feature_generator/","text":"FeatureGenerator class atom.feature_selection. FeatureGenerator (strategy='DFS', n_features=None, generations=20, population=500, operators=None, n_jobs=1, verbose=0, logger=None, random_state=None) [source] Use Deep feature Synthesis or a genetic algorithm to create new combinations of existing features to capture the non-linear relations between the original features. See the user guide for more information. Parameters: strategy: str, optional (default='DFS') Strategy to crate new features. Choose from: 'DFS' to use Deep Feature Synthesis. 'GFG' or 'genetic' to use Genetic Feature Generation. n_features: int or None, optional (default=None) Number of newly generated features to add to the dataset (if strategy='genetic', no more than 1% of the population). If None, select all created. generations: int, optional (default=20) Number of generations to evolve. Only if strategy='genetic'. population: int, optional (default=500) Number of programs in each generation. Only if strategy='genetic'. operators: str, sequence or None, optional (default=None) Name of the operators to be used on the features (for both strategies). None to use all. Valid options are: 'add', 'sub', 'mul', 'div', 'sqrt', 'log', 'sin', 'cos', 'tan'. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random. Attributes Attributes: symbolic_transformer: class Class used to calculate the genetic features, from SymbolicTransformer . Only if strategy='genetic'. genetic_features: pd.DataFrame Dataframe of the newly created non-linear features. Only if strategy='genetic'. Columns include: name: Name of the feature (automatically created). description: Operators used to create this feature. fitness: Fitness score. Methods fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function FeatureGenerator. fit (X, y) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: FeatureGenerator Fitted instance of self. function FeatureGenerator. fit_transform (X, y) [source] Fit the FeatureGenerator and return the transformed data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Feature set with the newly generated features. function FeatureGenerator. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function FeatureGenerator. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function FeatureGenerator. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: FeatureGenerator Estimator instance. function FeatureGenerator. transform (X, y=None) [source] Create new features in the dataset. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Feature set with the newly generated features. Example from atom.feature_engineering import FeatureGenerator feature_generator = FeatureGenerator(n_features=3, generations=30, n_jobs=3, verbose=2) X_transformed = FeatureGenerator.fit_transform(X, y) print(feature_generator.genetic_features)","title":"FetaureGenerator"},{"location":"API/feature_engineering/feature_generator/#featuregenerator","text":"class atom.feature_selection. FeatureGenerator (strategy='DFS', n_features=None, generations=20, population=500, operators=None, n_jobs=1, verbose=0, logger=None, random_state=None) [source] Use Deep feature Synthesis or a genetic algorithm to create new combinations of existing features to capture the non-linear relations between the original features. See the user guide for more information. Parameters: strategy: str, optional (default='DFS') Strategy to crate new features. Choose from: 'DFS' to use Deep Feature Synthesis. 'GFG' or 'genetic' to use Genetic Feature Generation. n_features: int or None, optional (default=None) Number of newly generated features to add to the dataset (if strategy='genetic', no more than 1% of the population). If None, select all created. generations: int, optional (default=20) Number of generations to evolve. Only if strategy='genetic'. population: int, optional (default=500) Number of programs in each generation. Only if strategy='genetic'. operators: str, sequence or None, optional (default=None) Name of the operators to be used on the features (for both strategies). None to use all. Valid options are: 'add', 'sub', 'mul', 'div', 'sqrt', 'log', 'sin', 'cos', 'tan'. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If <-1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random.","title":"FeatureGenerator"},{"location":"API/feature_engineering/feature_generator/#attributes","text":"Attributes: symbolic_transformer: class Class used to calculate the genetic features, from SymbolicTransformer . Only if strategy='genetic'. genetic_features: pd.DataFrame Dataframe of the newly created non-linear features. Only if strategy='genetic'. Columns include: name: Name of the feature (automatically created). description: Operators used to create this feature. fitness: Fitness score.","title":"Attributes"},{"location":"API/feature_engineering/feature_generator/#methods","text":"fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. function FeatureGenerator. fit (X, y) [source] Fit the class. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array or pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: FeatureGenerator Fitted instance of self. function FeatureGenerator. fit_transform (X, y) [source] Fit the FeatureGenerator and return the transformed data. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Feature set with the newly generated features. function FeatureGenerator. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function FeatureGenerator. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function FeatureGenerator. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: FeatureGenerator Estimator instance. function FeatureGenerator. transform (X, y=None) [source] Create new features in the dataset. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Feature set with the newly generated features.","title":"Methods"},{"location":"API/feature_engineering/feature_generator/#example","text":"from atom.feature_engineering import FeatureGenerator feature_generator = FeatureGenerator(n_features=3, generations=30, n_jobs=3, verbose=2) X_transformed = FeatureGenerator.fit_transform(X, y) print(feature_generator.genetic_features)","title":"Example"},{"location":"API/feature_engineering/feature_selector/","text":"FeatureSelector class atom.feature_selection. FeatureSelector (strategy=None, solver=None, n_features=None, max_frac_repeated=1., max_correlation=1., n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Remove features according to the selected strategy. Ties between features with equal scores will be broken in an unspecified way. Also removes features with too low variance and finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified limit (in terms of absolute value), it removes one of the two. See the user guide for more information. Parameters: strategy: string or None, optional (default=None) Feature selection strategy to use. Choose from: None: Do not perform any feature selection algorithm. 'univariate': Select best features according to a univariate F-test. 'PCA': Perform principal component analysis. 'SFM': Select best features according to a model. 'RFE': Perform recursive feature elimination. 'RFECV': Perform RFE with cross-validated selection. solver: string, callable or None, optional (default=None) Solver or model to use for the feature selection strategy. See the sklearn documentation for an extended description of the choices. Select None for the default option per strategy (not applicable for SFM, RFE and RFECV). for 'univariate', choose from: 'f_classif' 'f_regression' 'mutual_info_classif' 'mutual_info_regression' 'chi2' Any function taking two arrays (X, y), and returning arrays (scores, p-values). See the sklearn documentation . for 'PCA', choose from: 'auto' (default) 'full' 'arpack' 'randomized' for 'SFM', 'RFE' and 'RFECV: Choose a supervised learning model. The estimator must have either feature_importances_ or coef_ attribute after fitting. You can use a model from the ATOM package (add _class or _reg after the model name to specify a classification or regression task respectively, e.g. solver='LGB_reg'). No default option. Note that the RFE and RFECV strategies don't work when the solver is a CatBoost model due to incompatibility of the APIs. n_features: int, float or None, optional (default=None) Number of features to select. Choose from: if None: Select all features. if < 1: Fraction of the total features to select. if >= 1: Number of features to select. If strategy='SFM' and the threshold parameter is not specified, the threshold will be set to -np.inf in order to make this parameter the number of features to select. If strategy='RFECV', it's the minimum number of features to select. max_frac_repeated: float or None, optional (default=1.) Remove features with the same value in at least this fraction of the total rows. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. None to skip this step. max_correlation: float or None, optional (default=1.) Minimum value of the Pearson correlation coefficient to identify correlated features. A value of 1 removes on of 2 equal columns. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute. None to skip this step. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random. Properties The plot aesthetics can be customized using the properties described hereunder, e.g. FeatureSelector.tick_fontsize = 12 . Note that the properties are bound to the class, not the instance. Plotting properties: style: str Seaborn plotting style. See the documentation . palette: str Seaborn color palette. See the documentation . title_fontsize: int Fontsize for plot titles. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for ticks. Attributes Attributes: collinear: pd.DataFrame Dataframe of the removed collinear features. Columns include: drop_feature: name of the feature dropped by the method. correlated feature: Name of the correlated feature(s). correlation_value: Pearson correlation coefficient(s) of the feature pairs. univariate: class SelectKBest instance used to fit the estimator. Only if strategy='univariate'. scaler: class Scaler instance used to scale the data. Only if strategy='PCA' and data was not scaled. pca: class PCA instance used to fit the estimator. Only if strategy='PCA'. sfm: class SelectFromModel instance used to fit the estimator. Only if strategy='SFM'. rfe: class RFE instance used to fit the estimator. Only if strategy='RFE'. rfecv: class RFECV instance used to fit the estimator. Only if strategy='RFECV'. Methods fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. plot_pca Plot the explained variance ratio vs the number of components. plot_components Plot the explained variance ratio per component. plot_rfecv Plot the scores obtained by the estimator on the RFECV. function FeatureSelector. fit (X, y=None) [source] Fit the class. Note that the univariate, sfm (when model is not fitted), rfe and rfecv strategies all need a target column. Leaving it None will raise an exception. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: FeatureSelector Fitted instance of self. function FeatureSelector. fit_transform (X, y) [source] Fit the FeatureSelector and return the transformed feature set. Note that the univariate, sfm (when model is not fitted), rfe and rfecv strategies need a target column. Leaving it None will raise an exception. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. function FeatureSelector. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function FeatureSelector. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function FeatureSelector. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: FeatureSelector Estimator instance. function FeatureSelector. transform (X, y=None) [source] Transform the feature set. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Transformed feature set. function atom.ATOM. plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. See plot_pca for a description of the parameters. function atom.ATOM. plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per components. See plot_components for a description of the parameters. function atom.ATOM. plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the scores obtained by the estimator fitted on every subset of the data. See plot_rfecv for a description of the parameters. Example from atom.feature_engineering import FeatureSelector feature_selector = FeatureSelector(stratgey='pca', n_features=12, whiten=True, max_correlation=0.96) X_transformed = FeatureSelector.fit_transform(X, y) feature_selector.plot_pca(filename='pca', figsize=(8, 5))","title":"FetaureSelector"},{"location":"API/feature_engineering/feature_selector/#featureselector","text":"class atom.feature_selection. FeatureSelector (strategy=None, solver=None, n_features=None, max_frac_repeated=1., max_correlation=1., n_jobs=1, verbose=0, logger=None, random_state=None, **kwargs) [source] Remove features according to the selected strategy. Ties between features with equal scores will be broken in an unspecified way. Also removes features with too low variance and finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified limit (in terms of absolute value), it removes one of the two. See the user guide for more information. Parameters: strategy: string or None, optional (default=None) Feature selection strategy to use. Choose from: None: Do not perform any feature selection algorithm. 'univariate': Select best features according to a univariate F-test. 'PCA': Perform principal component analysis. 'SFM': Select best features according to a model. 'RFE': Perform recursive feature elimination. 'RFECV': Perform RFE with cross-validated selection. solver: string, callable or None, optional (default=None) Solver or model to use for the feature selection strategy. See the sklearn documentation for an extended description of the choices. Select None for the default option per strategy (not applicable for SFM, RFE and RFECV). for 'univariate', choose from: 'f_classif' 'f_regression' 'mutual_info_classif' 'mutual_info_regression' 'chi2' Any function taking two arrays (X, y), and returning arrays (scores, p-values). See the sklearn documentation . for 'PCA', choose from: 'auto' (default) 'full' 'arpack' 'randomized' for 'SFM', 'RFE' and 'RFECV: Choose a supervised learning model. The estimator must have either feature_importances_ or coef_ attribute after fitting. You can use a model from the ATOM package (add _class or _reg after the model name to specify a classification or regression task respectively, e.g. solver='LGB_reg'). No default option. Note that the RFE and RFECV strategies don't work when the solver is a CatBoost model due to incompatibility of the APIs. n_features: int, float or None, optional (default=None) Number of features to select. Choose from: if None: Select all features. if < 1: Fraction of the total features to select. if >= 1: Number of features to select. If strategy='SFM' and the threshold parameter is not specified, the threshold will be set to -np.inf in order to make this parameter the number of features to select. If strategy='RFECV', it's the minimum number of features to select. max_frac_repeated: float or None, optional (default=1.) Remove features with the same value in at least this fraction of the total rows. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. None to skip this step. max_correlation: float or None, optional (default=1.) Minimum value of the Pearson correlation coefficient to identify correlated features. A value of 1 removes on of 2 equal columns. A dataframe of the removed features and their correlation values can be accessed through the collinear attribute. None to skip this step. n_jobs: int, optional (default=1) Number of cores to use for parallel processing. If >0: Number of cores to use. If -1: Use all available cores. If < -1: Use available_cores - 1 + n_jobs. Beware that using multiple processes on the same machine may cause memory issues for large datasets. verbose: int, optional (default=0) Verbosity level of the class. Possible values are: 0 to not print anything. 1 to print basic information. 2 to print detailed information. logger: bool, str, class or None, optional (default=None) If None: Doesn't save a logging file. If bool: True for logging file with default name, False for no logger. If str: Name of the logging file. 'auto' to create an automatic name. If class: python Logger object. random_state: int or None, optional (default=None) Seed used by the random number generator. If None, the random number generator is the RandomState instance used by np.random.","title":"FeatureSelector"},{"location":"API/feature_engineering/feature_selector/#properties","text":"The plot aesthetics can be customized using the properties described hereunder, e.g. FeatureSelector.tick_fontsize = 12 . Note that the properties are bound to the class, not the instance. Plotting properties: style: str Seaborn plotting style. See the documentation . palette: str Seaborn color palette. See the documentation . title_fontsize: int Fontsize for plot titles. label_fontsize: int Fontsize for labels and legends. tick_fontsize: int Fontsize for ticks.","title":"Properties"},{"location":"API/feature_engineering/feature_selector/#attributes","text":"Attributes: collinear: pd.DataFrame Dataframe of the removed collinear features. Columns include: drop_feature: name of the feature dropped by the method. correlated feature: Name of the correlated feature(s). correlation_value: Pearson correlation coefficient(s) of the feature pairs. univariate: class SelectKBest instance used to fit the estimator. Only if strategy='univariate'. scaler: class Scaler instance used to scale the data. Only if strategy='PCA' and data was not scaled. pca: class PCA instance used to fit the estimator. Only if strategy='PCA'. sfm: class SelectFromModel instance used to fit the estimator. Only if strategy='SFM'. rfe: class RFE instance used to fit the estimator. Only if strategy='RFE'. rfecv: class RFECV instance used to fit the estimator. Only if strategy='RFECV'.","title":"Attributes"},{"location":"API/feature_engineering/feature_selector/#methods","text":"fit Fit the class. fit_transform Fit the class and return the transformed data. get_params Get parameters for this estimator. save Save the instance to a pickle file. set_params Set the parameters of this estimator. transform Transform the data. plot_pca Plot the explained variance ratio vs the number of components. plot_components Plot the explained variance ratio per component. plot_rfecv Plot the scores obtained by the estimator on the RFECV. function FeatureSelector. fit (X, y=None) [source] Fit the class. Note that the univariate, sfm (when model is not fitted), rfe and rfecv strategies all need a target column. Leaving it None will raise an exception. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: self: FeatureSelector Fitted instance of self. function FeatureSelector. fit_transform (X, y) [source] Fit the FeatureSelector and return the transformed feature set. Note that the univariate, sfm (when model is not fitted), rfe and rfecv strategies need a target column. Leaving it None will raise an exception. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) If None: y is not used in this estimator. If int: Position of the target column in X. If string: Name of the target column in X Else: Data target column with shape=(n_samples,) Returns: X: pd.DataFrame Transformed feature set. function FeatureSelector. get_params (deep=True) [source] Get parameters for this estimator. Parameters: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: params: dict Dictionary of the parameter names mapped to their values. function FeatureSelector. save (filename=None) [source] Save the instance to a pickle file. Parameters: filename: str or None, optional (default=None) Name to save the file with. None to save with default name. function FeatureSelector. set_params (**params) [source] Set the parameters of this estimator. Parameters: **params: dict Estimator parameters. Returns: self: FeatureSelector Estimator instance. function FeatureSelector. transform (X, y=None) [source] Transform the feature set. Parameters: X: dict, sequence, np.array or pd.DataFrame Data containing the features, with shape=(n_samples, n_features). y: int, str, sequence, np.array, pd.Series or None, optional (default=None) Does nothing. Implemented for continuity of the API. Returns: X: pd.DataFrame Transformed feature set. function atom.ATOM. plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. See plot_pca for a description of the parameters. function atom.ATOM. plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per components. See plot_components for a description of the parameters. function atom.ATOM. plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the scores obtained by the estimator fitted on every subset of the data. See plot_rfecv for a description of the parameters.","title":"Methods"},{"location":"API/feature_engineering/feature_selector/#example","text":"from atom.feature_engineering import FeatureSelector feature_selector = FeatureSelector(stratgey='pca', n_features=12, whiten=True, max_correlation=0.96) X_transformed = FeatureSelector.fit_transform(X, y) feature_selector.plot_pca(filename='pca', figsize=(8, 5))","title":"Example"},{"location":"API/plots/plot_bagging/","text":"plot_bagging function atom.plots. plot_bagging (models=None, metric=0, title=None, figsize=None, filename=None, display=True) [source] Plot a boxplot of the bagging's results. Only available for models fitted using bagging. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline that used bagging are selected. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size the to number of models. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'Tree', 'LGB', 'MLP'], metric='accuracy', bagging=5) atom.plot_bagging()","title":"plot_bagging"},{"location":"API/plots/plot_bagging/#plot_bagging","text":"function atom.plots. plot_bagging (models=None, metric=0, title=None, figsize=None, filename=None, display=True) [source] Plot a boxplot of the bagging's results. Only available for models fitted using bagging. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline that used bagging are selected. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size the to number of models. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_bagging"},{"location":"API/plots/plot_bagging/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'Tree', 'LGB', 'MLP'], metric='accuracy', bagging=5) atom.plot_bagging()","title":"Example"},{"location":"API/plots/plot_bo/","text":"plot_bo function atom.plots. plot_bo (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the bayesian optimization scoring. Only for models that ran the hyperparameter optimization. This is the same plot as the one produced by bo_params{'plot_bo': True} while running the BO. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline that used bayesian optimization are selected. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size the to number of models. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LDA', 'LGB'], metric='f1', n_calls=24, n_random_starts=10) atom.plot_bo()","title":"plot_bo"},{"location":"API/plots/plot_bo/#plot_bo","text":"function atom.plots. plot_bo (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the bayesian optimization scoring. Only for models that ran the hyperparameter optimization. This is the same plot as the one produced by bo_params{'plot_bo': True} while running the BO. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline that used bayesian optimization are selected. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size the to number of models. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_bo"},{"location":"API/plots/plot_bo/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LDA', 'LGB'], metric='f1', n_calls=24, n_random_starts=10) atom.plot_bo()","title":"Example"},{"location":"API/plots/plot_calibration/","text":"plot_calibration function atom.plots. plot_calibration (models=None, n_bins=10, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot the calibration curve for a binary classifier. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class. This figure shows two plots: the calibration curve, where the x-axis represents the average predicted probability in each bin and the y-axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin); and a distribution of all predicted probabilities of the classifier. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. n_bins: int, optional (default=10) Number of bins for the calibration calculation and the histogram. Minimum of 5 required. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 10)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X) atom.run(['GNB', 'LR', 'LGB'], metric='average_precision') atom.plot_calibration()","title":"plot_calibration"},{"location":"API/plots/plot_calibration/#plot_calibration","text":"function atom.plots. plot_calibration (models=None, n_bins=10, title=None, figsize=(10, 10), filename=None, display=True) [source] Plot the calibration curve for a binary classifier. Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class. This figure shows two plots: the calibration curve, where the x-axis represents the average predicted probability in each bin and the y-axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin); and a distribution of all predicted probabilities of the classifier. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. n_bins: int, optional (default=10) Number of bins for the calibration calculation and the histogram. Minimum of 5 required. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 10)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_calibration"},{"location":"API/plots/plot_calibration/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X) atom.run(['GNB', 'LR', 'LGB'], metric='average_precision') atom.plot_calibration()","title":"Example"},{"location":"API/plots/plot_components/","text":"plot_components function atom.plots. plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per components. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied PCA on the dataset. Can't be called from the model subclasses. Parameters: show: int or None, optional (default=None) Number of components to show. If None, the number of components in the data are plotted. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.feature_selection(strategy='PCA', n_features=11) atom.plot_components()","title":"plot_components"},{"location":"API/plots/plot_components/#plot_components","text":"function atom.plots. plot_components (show=None, title=None, figsize=None, filename=None, display=True) [source] Plot the explained variance ratio per components. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied PCA on the dataset. Can't be called from the model subclasses. Parameters: show: int or None, optional (default=None) Number of components to show. If None, the number of components in the data are plotted. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_components"},{"location":"API/plots/plot_components/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.feature_selection(strategy='PCA', n_features=11) atom.plot_components()","title":"Example"},{"location":"API/plots/plot_confusion_matrix/","text":"plot_confusion_matrix function atom.plots. plot_confusion_matrix (models=None, normalize=False, title=None, figsize=None, filename=None, display=True) [source] For 1 model: plot the confusion matrix in a heatmap. For multiple models: compare TP, FP, FN and TN in a barplot (not implemented for multiclass classification). Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. normalize: bool, optional (default=False) Whether to normalize the matrix. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, filename: str or None, optional (default=None) Name of the file (to save). If None, adapts size to plot type. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['Tree', 'Bag']) atom.Tree.plot_confusion_matrix(normalize=True) atom.plot_confusion_matrix()","title":"plot_confusion_matrix"},{"location":"API/plots/plot_confusion_matrix/#plot_confusion_matrix","text":"function atom.plots. plot_confusion_matrix (models=None, normalize=False, title=None, figsize=None, filename=None, display=True) [source] For 1 model: plot the confusion matrix in a heatmap. For multiple models: compare TP, FP, FN and TN in a barplot (not implemented for multiclass classification). Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. normalize: bool, optional (default=False) Whether to normalize the matrix. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, filename: str or None, optional (default=None) Name of the file (to save). If None, adapts size to plot type. display: bool, optional (default=True) Whether to render the plot.","title":"plot_confusion_matrix"},{"location":"API/plots/plot_confusion_matrix/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['Tree', 'Bag']) atom.Tree.plot_confusion_matrix(normalize=True) atom.plot_confusion_matrix()","title":"Example"},{"location":"API/plots/plot_correlation/","text":"plot_correlation function atom.plots. plot_correlation (title=None, figsize=(10, 10), filename=None, display=True) [source] Correlation matrix plot of the dataset. Ignores non-numeric columns. Can only be called from an ATOMClassifier / ATOMRegressor instance. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 10)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.plot_correlation()","title":"plot_correlation"},{"location":"API/plots/plot_correlation/#plot_correlation","text":"function atom.plots. plot_correlation (title=None, figsize=(10, 10), filename=None, display=True) [source] Correlation matrix plot of the dataset. Ignores non-numeric columns. Can only be called from an ATOMClassifier / ATOMRegressor instance. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 10)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_correlation"},{"location":"API/plots/plot_correlation/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.plot_correlation()","title":"Example"},{"location":"API/plots/plot_evals/","text":"plot_evals function atom.plots. plot_evals (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot evaluation curves for the train and test set. Only for models that allow in-training evaluation (XGB, LGB, CatB). Only allows plotting of one model at a time. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. Note that this will raise an exception if there are multiple models in the pipeline. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom.run('XGB') atom.XGB.plot_evals()","title":"plot_evals"},{"location":"API/plots/plot_evals/#plot_evals","text":"function atom.plots. plot_evals (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot evaluation curves for the train and test set. Only for models that allow in-training evaluation (XGB, LGB, CatB). Only allows plotting of one model at a time. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. Note that this will raise an exception if there are multiple models in the pipeline. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_evals"},{"location":"API/plots/plot_evals/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom.run('XGB') atom.XGB.plot_evals()","title":"Example"},{"location":"API/plots/plot_feature_importance/","text":"plot_feature_importance function atom.plots. plot_feature_importance (models=None, show=None, title=None, figsize=None, filename=None, display=True) [source] Plot a tree-based model's feature importance. The importances are normalized in order to be able to compare them between models. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. show: int, optional (default=None) Number of best features to show in the plot. None to show for all. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF'], metric='recall_weighted') atom.RF.plot_feature_importance(show=10, filename='random_forest_importance.png')","title":"plot_feature_importance"},{"location":"API/plots/plot_feature_importance/#plot_feature_importance","text":"function atom.plots. plot_feature_importance (models=None, show=None, title=None, figsize=None, filename=None, display=True) [source] Plot a tree-based model's feature importance. The importances are normalized in order to be able to compare them between models. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. show: int, optional (default=None) Number of best features to show in the plot. None to show for all. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_feature_importance"},{"location":"API/plots/plot_feature_importance/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF'], metric='recall_weighted') atom.RF.plot_feature_importance(show=10, filename='random_forest_importance.png')","title":"Example"},{"location":"API/plots/plot_gains/","text":"plot_gains function atom.plots. plot_gains (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the cumulative gains curve. Only for binary classification. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['GNB', 'RF', 'LGB'], metric='roc_auc') atom.plot_gains(filename='cumulative_gains_curve.png')","title":"plot_gains"},{"location":"API/plots/plot_gains/#plot_gains","text":"function atom.plots. plot_gains (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the cumulative gains curve. Only for binary classification. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_gains"},{"location":"API/plots/plot_gains/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['GNB', 'RF', 'LGB'], metric='roc_auc') atom.plot_gains(filename='cumulative_gains_curve.png')","title":"Example"},{"location":"API/plots/plot_learning_curve/","text":"plot_learning_curve function atom.plots. plot_learning_curve (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the model's learning curve: score vs number of training samples. Only available if the models were fitted with a TrainSizingClassifier / TrainSizingRegressor instance. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. If you call the plot from a model subclass, the parameter will automatically be filled with the model's name. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.train_sizing(['GNB', 'LDA'], metric='accuracy', bagging=5) atom.plot_learning_curve()","title":"plot_learning_curve"},{"location":"API/plots/plot_learning_curve/#plot_learning_curve","text":"function atom.plots. plot_learning_curve (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the model's learning curve: score vs number of training samples. Only available if the models were fitted with a TrainSizingClassifier / TrainSizingRegressor instance. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. If you call the plot from a model subclass, the parameter will automatically be filled with the model's name. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_learning_curve"},{"location":"API/plots/plot_learning_curve/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.train_sizing(['GNB', 'LDA'], metric='accuracy', bagging=5) atom.plot_learning_curve()","title":"Example"},{"location":"API/plots/plot_lift/","text":"plot_lift function atom.plots. plot_lift (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the lift curve. Only for binary classification. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['GNB', 'RF', 'LGB'], metric='roc_auc') atom.plot_lift(filename='lift_curve.png')","title":"plot_lift"},{"location":"API/plots/plot_lift/#plot_lift","text":"function atom.plots. plot_lift (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the lift curve. Only for binary classification. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_lift"},{"location":"API/plots/plot_lift/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['GNB', 'RF', 'LGB'], metric='roc_auc') atom.plot_lift(filename='lift_curve.png')","title":"Example"},{"location":"API/plots/plot_pca/","text":"plot_pca function atom.plots. plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied PCA on the dataset. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.feature_selection(strategy='PCA', n_features=11) atom.plot_pca()","title":"plot_pca"},{"location":"API/plots/plot_pca/#plot_pca","text":"function atom.plots. plot_pca (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the explained variance ratio vs the number of components. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied PCA on the dataset. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_pca"},{"location":"API/plots/plot_pca/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.feature_selection(strategy='PCA', n_features=11) atom.plot_pca()","title":"Example"},{"location":"API/plots/plot_permutation_importance/","text":"plot_permutation_importance function atom.plots. plot_permutation_importance (models=None, show=None, n_repeats=10, title=None, figsize=None, filename=None, display=True) [source] Plot the feature permutation importance of models. Calculating all permutations can be time consuming, especially if n_repeats is high. They are stored under the attribute permutations . This means that if a plot is repeated for the same model with the same n_repeats , it will be considerably faster. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. show: int, optional (default=None) Number of best features to show in the plot. None to show all. n_repeats: int, optional (default=10) Number of times to permute each feature. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'LDA'], metric='average_precision') atom.LDA.plot_permutation_importance(show=10, n_repeats=7)","title":"plot_permutation_importance"},{"location":"API/plots/plot_permutation_importance/#plot_permutation_importance","text":"function atom.plots. plot_permutation_importance (models=None, show=None, n_repeats=10, title=None, figsize=None, filename=None, display=True) [source] Plot the feature permutation importance of models. Calculating all permutations can be time consuming, especially if n_repeats is high. They are stored under the attribute permutations . This means that if a plot is repeated for the same model with the same n_repeats , it will be considerably faster. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. show: int, optional (default=None) Number of best features to show in the plot. None to show all. n_repeats: int, optional (default=10) Number of times to permute each feature. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=None) Figure's size, format as (x, y). If None, adapts size to show parameter. filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_permutation_importance"},{"location":"API/plots/plot_permutation_importance/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'LDA'], metric='average_precision') atom.LDA.plot_permutation_importance(show=10, n_repeats=7)","title":"Example"},{"location":"API/plots/plot_prc/","text":"plot_prc function atom.plots. plot_prc (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the precision-recall curve. The legend shows the average precision (AP) score. Only for binary classification tasks. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF', 'LGB'], metric='average_precision') atom.plot_prc()","title":"plot_prc"},{"location":"API/plots/plot_prc/#plot_prc","text":"function atom.plots. plot_prc (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the precision-recall curve. The legend shows the average precision (AP) score. Only for binary classification tasks. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_prc"},{"location":"API/plots/plot_prc/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF', 'LGB'], metric='average_precision') atom.plot_prc()","title":"Example"},{"location":"API/plots/plot_probabilities/","text":"plot_probabilities function atom.plots. plot_probabilities (models=None, target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot performance metric(s) against multiple threshold values. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. target: int or str, optional (default=1) Probability of being that category (as index or name). title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y='RainTomorrow') atom.run('rf') atom.plot_probabilities(target='Yes', filenmae='probabilities_category_yes')","title":"plot_probabilities"},{"location":"API/plots/plot_probabilities/#plot_probabilities","text":"function atom.plots. plot_probabilities (models=None, target=1, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot performance metric(s) against multiple threshold values. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. target: int or str, optional (default=1) Probability of being that category (as index or name). title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_probabilities"},{"location":"API/plots/plot_probabilities/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y='RainTomorrow') atom.run('rf') atom.plot_probabilities(target='Yes', filenmae='probabilities_category_yes')","title":"Example"},{"location":"API/plots/plot_rfecv/","text":"plot_rfecv function atom.plots. plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the scores obtained by the estimator fitted on every subset of the data. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied RFECV on the dataset. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom.feature_selection(strategy='RFECV', solver='LGB_reg', n_features=23) atom.plot_rfecv()","title":"plot_rfecv"},{"location":"API/plots/plot_rfecv/#plot_rfecv","text":"function atom.plots. plot_rfecv (title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the scores obtained by the estimator fitted on every subset of the data. Can only be called from an ATOMClassifier / ATOMRegressor or FeatureSelector instance that applied RFECV on the dataset. Can't be called from the model subclasses. Parameters: title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_rfecv"},{"location":"API/plots/plot_rfecv/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom.feature_selection(strategy='RFECV', solver='LGB_reg', n_features=23) atom.plot_rfecv()","title":"Example"},{"location":"API/plots/plot_roc/","text":"plot_roc function atom.plots. plot_roc (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Receiver Operating Characteristics curve. The legend shows the Area Under the ROC Curve (AUC) score. Only for binary classification tasks. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF', 'LGB'], metric='roc_auc') atom.plot_roc(filename='roc_curve.png')","title":"plot_roc"},{"location":"API/plots/plot_roc/#plot_roc","text":"function atom.plots. plot_roc (models=None, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot the Receiver Operating Characteristics curve. The legend shows the Area Under the ROC Curve (AUC) score. Only for binary classification tasks. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_roc"},{"location":"API/plots/plot_roc/#example","text":"from atom import ATOMClassifier atom = ATOMClassifier(X, y) atom.run(['LR', 'RF', 'LGB'], metric='roc_auc') atom.plot_roc(filename='roc_curve.png')","title":"Example"},{"location":"API/plots/plot_successive_halving/","text":"plot_successive_halving function atom.plots. plot_successive_halving (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot of the models' scores per iteration of the successive halving. Only available if the models were fitted with a SuccessiveHalvingClassifier / SuccessiveHalvingRegressor instance. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. If you call the plot from a model subclass, the parameter will automatically be filled with the model's name. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom successive_halving(['tree', 'bag', 'et', 'rf', 'gbm', 'lgb'], metric='neg_mean_squared_error') atom.plot_successive_halving()","title":"plot_successive_halving"},{"location":"API/plots/plot_successive_halving/#plot_successive_halving","text":"function atom.plots. plot_successive_halving (models=None, metric=0, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot of the models' scores per iteration of the successive halving. Only available if the models were fitted with a SuccessiveHalvingClassifier / SuccessiveHalvingRegressor instance. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all the models in the pipeline are selected. If you call the plot from a model subclass, the parameter will automatically be filled with the model's name. metric: int or str, optional (default=0) Index or name of the metric to plot. Only for multi-metric runs. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_successive_halving"},{"location":"API/plots/plot_successive_halving/#example","text":"from atom import ATOMRegressor atom = ATOMRegressor(X, y) atom successive_halving(['tree', 'bag', 'et', 'rf', 'gbm', 'lgb'], metric='neg_mean_squared_error') atom.plot_successive_halving()","title":"Example"},{"location":"API/plots/plot_threshold/","text":"plot_threshold function atom.plots. plot_threshold (models=None, metric=None, steps=100, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot performance metric(s) against multiple threshold values. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. metric: str, callable, sequence or None, optional (default=None) Metric(s) to plot. These can be one of the pre-defined sklearn scorers as string, a metric function or a sklearn scorer object. If None, the metric used to run the pipeline is used. steps: int, optional (default=100) Number of thresholds measured. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot. Example from atom import ATOMClassifier from sklearn.metrics import recall_score atom = ATOMClassifier(X, y) atom.run('KNN') atom.plot_threshold(metric=['accuracy', 'f1', recall_score])","title":"plot_threshold"},{"location":"API/plots/plot_threshold/#plot_threshold","text":"function atom.plots. plot_threshold (models=None, metric=None, steps=100, title=None, figsize=(10, 6), filename=None, display=True) [source] Plot performance metric(s) against multiple threshold values. Parameters: models: str, sequence or None, optional (default=None) Name of the models to plot. If None, all models in the pipeline are selected. metric: str, callable, sequence or None, optional (default=None) Metric(s) to plot. These can be one of the pre-defined sklearn scorers as string, a metric function or a sklearn scorer object. If None, the metric used to run the pipeline is used. steps: int, optional (default=100) Number of thresholds measured. title: str or None, optional (default=None) Plot's title. If None, the default option is used. figsize: tuple, optional (default=(10, 6)) Figure's size, format as (x, y). filename: str or None, optional (default=None) Name of the file (to save). If None, the figure is not saved. display: bool, optional (default=True) Whether to render the plot.","title":"plot_threshold"},{"location":"API/plots/plot_threshold/#example","text":"from atom import ATOMClassifier from sklearn.metrics import recall_score atom = ATOMClassifier(X, y) atom.run('KNN') atom.plot_threshold(metric=['accuracy', 'f1', recall_score])","title":"Example"},{"location":"API/training/successivehalvingclassifier/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"SuccessiveHalvingClassifier"},{"location":"API/training/successivehalvingclassifier/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"API/training/successivehalvingregressor/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"SuccessiveHalvingClassifier"},{"location":"API/training/successivehalvingregressor/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"API/training/trainerclassifier/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"TrainerClassifier"},{"location":"API/training/trainerclassifier/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"API/training/trainerregressor/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"TrainerRegressor"},{"location":"API/training/trainerregressor/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"API/training/trainsizingclassifier/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"TrainSizingClassifier"},{"location":"API/training/trainsizingclassifier/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"API/training/trainsizingregressor/","text":"Pipeline The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"TrainSizingRegressor"},{"location":"API/training/trainsizingregressor/#pipeline","text":"The pipeline method is where the models are fitted to the data and their performance is evaluated according to the selected metric. For every model, the pipeline applies the following steps: The optimal hyperparameters are selected using a Bayesian Optimization (BO) algorithm with gaussian process as kernel. The resulting score of each step of the BO is either computed by cross-validation on the complete training set or by randomly splitting the training set every iteration into a (sub) training set and a validation set. This process can create some data leakage but ensures a maximal use of the provided data. The test set, however, does not contain any leakage and will be used to determine the final score of every model. Note that, if the dataset is relatively small, the best score on the BO can consistently be lower than the final score on the test set (despite the leakage) due to the considerable fewer instances on which it is trained. Once the best hyperparameters are found, the model is trained again, now using the complete training set. After this, predictions are made on the test set. You can choose to evaluate the robustness of each model's applying a bagging algorithm, i.e. the model will be trained multiple times on a bootstrapped training set, returning a distribution of its performance on the test set. A couple of things to take into account: The metric implementation follows sklearn's API . This means that the implementation always tries to maximize the scorer, i.e. loss functions will be made negative. If an exception is encountered while fitting a model, the pipeline will automatically jump to the next model and save the exception in the errors attribute. When showing the final results, a !! indicates the highest score and a ~ indicates that the model is possibly overfitting (training set has a score at least 20% higher than the test set). The winning model subclass will be attached to the winner attribute. There are three methods to call for the pipeline. The pipeline method fits the models directly to the dataset. If you want to compare similar models, you can use the successive_halving method when running the pipeline. This technique fits N models to 1/N of the data. The best half are selected to go to the next iteration where the process is repeated. This continues until only one model remains, which is fitted on the complete dataset. Beware that a model's performance can depend greatly on the amount of data on which it is trained. For this reason we recommend only to use this technique with similar models, e.g. only using tree-based models. The train_sizing method fits the models on subsets of the training data. This can be used to examine the optimum size of the dataset needed for a satisfying performance. pipeline Fit the models to the data in a direct fashion. successive_halving Fit the models to the data in a successive halving fashion. train_sizing Fit the models to the data in a train sizing fashion. function atom.training. pipeline (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, n_calls=10, n_random_points=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclass classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. successive_halving (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, skip_iter=0, n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. skip_iter: int, optional (default=0) Skip last skip_iter iterations of the successive halving. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed. function atom.training. train_sizing (models, metric=None, greater_is_better=True, needs_proba=False, needs_threshold=False, train_sizes=np.linspcae(0.2, 1.0, 5), n_calls=0, n_random_starts=5, bo_kwargs={}, bagging=None) [source] Parameters: models: string or sequence List of models to fit on the data. Use the predefined acronyms to select the models. Possible values are (case insensitive): 'GNB' for Gaussian Naive Bayes Only for classification tasks. No hyperparameter tuning. 'MNB' for Multinomial Naive Bayes Only for classification tasks. 'BNB' for Bernoulli Naive Bayes Only for classification tasks. 'GP' for Gaussian Process classifier / regressor No hyperparameter tuning. 'OLS' for Ordinary Least Squares Only for regression tasks. No hyperparameter tuning. 'Ridge' for Ridge Linear classifier / regressor Only for regression tasks. 'Lasso' for Lasso Linear Regression Only for regression tasks. 'EN' for ElasticNet Linear Regression Only for regression tasks. 'BR' for Bayesian Regression Only for regression tasks. Uses ridge regularization. 'LR' for Logistic Regression Only for classification tasks. 'LDA' for Linear Discriminant Analysis Only for classification tasks. 'QDA' for Quadratic Discriminant Analysis Only for classification tasks. 'KNN' for K-Nearest Neighbors classifier / regressor 'Tree' for a single Decision Tree classifier / regressor 'Bag' for Bagging classifier / regressor Uses a decision tree as base estimator. 'ET' for Extra-Trees classifier / regressor 'RF' for Random Forest classifier / regressor 'AdaB' for AdaBoost classifier / regressor Uses a decision tree as base estimator. 'GBM' for Gradient Boosting Machine classifier / regressor 'XGB' for XGBoost classifier / regressor Only available if package is installed. 'LGB' for LightGBM classifier / regressor Only available if package is installed. 'CatB' for CatBoost classifier / regressor Only available if package is installed. 'lSVM' for Linear Support Vector Machine classifier / regressor Uses a one-vs-rest strategy for multiclass classification tasks. 'kSVM' for Kernel (non-linear) Support Vector Machine classifier / regressor Uses a one-vs-one strategy for multiclass classification tasks. 'PA' for Passive Aggressive classifier / regressor 'SGD' for Stochastic Gradient Descent classifier / regressor 'MLP' for Multilayer Perceptron classifier / regressor Can have between one and three hidden layers. metric: string or callable, optional (default=None) Metric on which the pipeline fits the models. Choose from any of sklearn's predefined scorers , use a score (or loss) function with signature metric(y, y_pred, **kwargs) or use a scorer object. If None, ATOM will try to use any metric it already has in the pipeline. If it hasn't got any, a default metric per task is selected: 'f1' for binary classification 'f1_weighted' for multiclas classification 'r2' for regression greater_is_better: bool, optional (default=True) Whether the metric is a score function or a loss function, i.e. if True, a higher score is better and if False, lower is better. Will be ignored if the metric is a string or a scorer. needs_proba: bool, optional (default=False) Whether the metric function requires probability estimates out of a classifier. If True, make sure that every model in the pipeline has a predict_proba method! Will be ignored if the metric is a string or a scorer. needs_threshold: bool, optional (default=False) Whether the metric function takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. Will be ignored if the metric is a string or a scorer. train_sizes: sequence, optional (default=np.linspace(0.2, 1.0, 5)) Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set. Otherwise it is interpreted as absolute sizes of the training sets. n_calls: int or sequence, optional (default=0) Maximum number of iterations of the BO (including n_random starts ). If 0, skip the BO and fit the model on its default Parameters. If sequence, the n-th value will apply to the n-th model in the pipeline. n_random_starts: int or sequence, optional (default=5) Initial number of random tests of the BO before fitting the surrogate function. If equal to n_calls , the optimizer will technically be performing a random search. If sequence, the n-th value will apply to the n-th model in the pipeline. bo_kwargs: dict, optional (default={}) Dictionary of extra keyword arguments for the BO. These can include: max_time: int Maximum allowed time for the BO (in seconds). delta_x: int or float Maximum distance between two consecutive points. delta_x: int or float Maximum score between two consecutive points. cv: int Number of folds for the cross-validation. If 1, the training set will be randomly split in a subtrain and validation set. callback: callable or list of callables Callbacks for the BO. dimensions: dict or array Custom hyperparameter space for the bayesian optimization. Can be an array (only if there is 1 model in the pipeline) or a dictionary with the model's name as key. plot_bo: bool Whether to plot the BO's progress as it runs. Creates a canvas with two plots: the first plot shows the score of every trial and the second shows the distance between the last consecutive steps. Don't forget to call %matplotlib at the start of the cell if you are using jupyter notebook! Any other parameter for the bayesian optimization function . bagging: int or None, optional (default=None) Number of data sets (bootstrapped from the training set) to use in the bagging algorithm. If None or 0, no bagging is performed.","title":"Pipeline"},{"location":"examples/binary_classification/binary_classification/","text":"Binary classification This example shows how we can use ATOM to perform a variety of data cleaning steps in order to prepare the data for modelling. Then, we compare the prediction performance of an Extra-Trees and a Random Forest. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow . Load the data # Import packages import pandas as pd from sklearn.metrics import fbeta_score from atom import ATOMClassifier # Load data X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 85885 Cairns 21.5 30.4 0.0 6.8 11.1 ENE 31.0 27014 Richmond 16.2 22.0 5.0 NaN NaN ESE 31.0 124422 Walpole 7.3 13.1 11.0 NaN NaN S 43.0 114777 PerthAirport 10.9 24.0 2.0 3.2 10.1 WSW 33.0 139885 Katherine 22.7 35.8 0.0 6.8 NaN E 39.0 Run the pipeline # Call ATOM using only 5% of the complete dataset (for explanatory purposes) atom = ATOMClassifier(X, y='RainTomorrow', n_rows=0.05, n_jobs=8, warnings=False, verbose=2, random_state=1) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 8 cores. Applying data cleaning... Dataset stats ================= >> Shape: (7110, 22) Missing values: 14621 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 5688 Size of test set: 1422 ---------------------------------- Class balance: No:Yes <==> 3.8:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 5615 | 4473 | 1142 | | 1: Yes | 1495 | 1215 | 280 | # We can change the data properties in the pipeline # Note that we can only replace the property with a new df new_train = atom.X new_train.insert(loc=3, column='AvgTemp', value=(atom.X['MaxTemp'] + atom.X['MinTemp'])/2) atom.X = new_train # This will automatically update all other data properties assert 'AvgTemp' in atom.dataset # Impute missing values atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) Fitting Imputer... Imputing missing values... --> Dropping 778 rows for containing less than 80% non-missing values. --> Imputing 5 missing values using the KNN imputer in feature MinTemp. --> Imputing 3 missing values using the KNN imputer in feature MaxTemp. --> Imputing 8 missing values using the KNN imputer in feature AvgTemp. --> Imputing 31 missing values using the KNN imputer in feature Rainfall. --> Imputing 2314 missing values using the KNN imputer in feature Evaporation. --> Imputing 2645 missing values using the KNN imputer in feature Sunshine. --> Imputing 201 missing values with remove in feature WindGustDir. --> Imputing 199 missing values using the KNN imputer in feature WindGustSpeed. --> Imputing 365 missing values with remove in feature WindDir9am. --> Imputing 24 missing values with remove in feature WindDir3pm. --> Imputing 4 missing values using the KNN imputer in feature WindSpeed9am. --> Imputing 3 missing values using the KNN imputer in feature WindSpeed3pm. --> Imputing 23 missing values using the KNN imputer in feature Humidity9am. --> Imputing 55 missing values using the KNN imputer in feature Humidity3pm. --> Imputing 42 missing values using the KNN imputer in feature Pressure9am. --> Imputing 40 missing values using the KNN imputer in feature Pressure3pm. --> Imputing 2112 missing values using the KNN imputer in feature Cloud9am. --> Imputing 2198 missing values using the KNN imputer in feature Cloud3pm. --> Imputing 5 missing values using the KNN imputer in feature Temp9am. --> Imputing 32 missing values using the KNN imputer in feature Temp3pm. --> Imputing 31 missing values with remove in feature RainToday. # Encode the categorical features atom.encode(max_onehot=10, frac_to_other=0.04) Fitting Encoder... Encoding categorical columns... --> Target-encoding feature Location. Contains 1 unique categories. --> Target-encoding feature WindGustDir. Contains 17 unique categories. --> Target-encoding feature WindDir9am. Contains 17 unique categories. --> Target-encoding feature WindDir3pm. Contains 17 unique categories. --> One-hot-encoding feature RainToday. Contains 3 unique categories. # Perform undersampling of the majority class atom.balance(undersample=0.9) atom.stats() # Note the balanced training set Performing undersampling... --> Removing 3145 rows from category: No. Dataset stats ================= >> Shape: (3965, 24) Scaled: False ---------------------------------- Size of training set: 2543 Size of test set: 1422 ---------------------------------- Class balance: No:Yes <==> 1.7:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 2473 | 1338 | 1135 | | 1: Yes | 1492 | 1205 | 287 | # Change the verbosity to avoid printing atom.verbose = 0 # Define a custom metric def f2_score(y_true, y_pred): return fbeta_score(y_true, y_pred, beta=2) # Fit the EXtra-Trees and Random Forest to the data atom.run(models=['et', 'rf'], metric=f2_score, n_calls=5, n_random_starts=2, bo_params={'cv': 1}, bagging=5) Analyze the results # Let's have a look at the final scoring atom.scoring() # The winning model is indicated with a ! and can be accessed through the winner attribute print(f'\\n\\nAnd the winner is the {atom.winner.longname} model!!') print('Score on the training set: ', atom.winner.score_train) print('Score on the test set: ', atom.winner.score_test) Results ===================== >> Extra-Trees --> f2_score: 0.670 \u00b1 0.004 Random Forest --> f2_score: 0.644 \u00b1 0.022 And the winner is the Extra-Trees model!! Score on the training set: 0.748396895038812 Score on the test set: 0.6670709520921769 We can make many plots to check the performance of the models # The probabilties plot shows the distribution of predicted # probabilities for the positive class atom.winner.plot_probabilities() # The threshold plot let us compare how different metrics # perform for different thresholds atom.winner.plot_threshold(metric=['f1', 'accuracy', 'average_precision'], steps=50, filename='thresholds.png') # The ROC and PRC curve are also typical ways of measuring performance atom.plot_roc(title=\"ROC for the LightGBM vs CatBoost model\") atom.plot_prc(title=\"PRC comparison of the models\")","title":"Binary classification"},{"location":"examples/binary_classification/binary_classification/#binary-classification","text":"This example shows how we can use ATOM to perform a variety of data cleaning steps in order to prepare the data for modelling. Then, we compare the prediction performance of an Extra-Trees and a Random Forest. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow .","title":"Binary classification"},{"location":"examples/binary_classification/binary_classification/#load-the-data","text":"# Import packages import pandas as pd from sklearn.metrics import fbeta_score from atom import ATOMClassifier # Load data X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 85885 Cairns 21.5 30.4 0.0 6.8 11.1 ENE 31.0 27014 Richmond 16.2 22.0 5.0 NaN NaN ESE 31.0 124422 Walpole 7.3 13.1 11.0 NaN NaN S 43.0 114777 PerthAirport 10.9 24.0 2.0 3.2 10.1 WSW 33.0 139885 Katherine 22.7 35.8 0.0 6.8 NaN E 39.0","title":"Load the data"},{"location":"examples/binary_classification/binary_classification/#run-the-pipeline","text":"# Call ATOM using only 5% of the complete dataset (for explanatory purposes) atom = ATOMClassifier(X, y='RainTomorrow', n_rows=0.05, n_jobs=8, warnings=False, verbose=2, random_state=1) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 8 cores. Applying data cleaning... Dataset stats ================= >> Shape: (7110, 22) Missing values: 14621 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 5688 Size of test set: 1422 ---------------------------------- Class balance: No:Yes <==> 3.8:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 5615 | 4473 | 1142 | | 1: Yes | 1495 | 1215 | 280 | # We can change the data properties in the pipeline # Note that we can only replace the property with a new df new_train = atom.X new_train.insert(loc=3, column='AvgTemp', value=(atom.X['MaxTemp'] + atom.X['MinTemp'])/2) atom.X = new_train # This will automatically update all other data properties assert 'AvgTemp' in atom.dataset # Impute missing values atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) Fitting Imputer... Imputing missing values... --> Dropping 778 rows for containing less than 80% non-missing values. --> Imputing 5 missing values using the KNN imputer in feature MinTemp. --> Imputing 3 missing values using the KNN imputer in feature MaxTemp. --> Imputing 8 missing values using the KNN imputer in feature AvgTemp. --> Imputing 31 missing values using the KNN imputer in feature Rainfall. --> Imputing 2314 missing values using the KNN imputer in feature Evaporation. --> Imputing 2645 missing values using the KNN imputer in feature Sunshine. --> Imputing 201 missing values with remove in feature WindGustDir. --> Imputing 199 missing values using the KNN imputer in feature WindGustSpeed. --> Imputing 365 missing values with remove in feature WindDir9am. --> Imputing 24 missing values with remove in feature WindDir3pm. --> Imputing 4 missing values using the KNN imputer in feature WindSpeed9am. --> Imputing 3 missing values using the KNN imputer in feature WindSpeed3pm. --> Imputing 23 missing values using the KNN imputer in feature Humidity9am. --> Imputing 55 missing values using the KNN imputer in feature Humidity3pm. --> Imputing 42 missing values using the KNN imputer in feature Pressure9am. --> Imputing 40 missing values using the KNN imputer in feature Pressure3pm. --> Imputing 2112 missing values using the KNN imputer in feature Cloud9am. --> Imputing 2198 missing values using the KNN imputer in feature Cloud3pm. --> Imputing 5 missing values using the KNN imputer in feature Temp9am. --> Imputing 32 missing values using the KNN imputer in feature Temp3pm. --> Imputing 31 missing values with remove in feature RainToday. # Encode the categorical features atom.encode(max_onehot=10, frac_to_other=0.04) Fitting Encoder... Encoding categorical columns... --> Target-encoding feature Location. Contains 1 unique categories. --> Target-encoding feature WindGustDir. Contains 17 unique categories. --> Target-encoding feature WindDir9am. Contains 17 unique categories. --> Target-encoding feature WindDir3pm. Contains 17 unique categories. --> One-hot-encoding feature RainToday. Contains 3 unique categories. # Perform undersampling of the majority class atom.balance(undersample=0.9) atom.stats() # Note the balanced training set Performing undersampling... --> Removing 3145 rows from category: No. Dataset stats ================= >> Shape: (3965, 24) Scaled: False ---------------------------------- Size of training set: 2543 Size of test set: 1422 ---------------------------------- Class balance: No:Yes <==> 1.7:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 2473 | 1338 | 1135 | | 1: Yes | 1492 | 1205 | 287 | # Change the verbosity to avoid printing atom.verbose = 0 # Define a custom metric def f2_score(y_true, y_pred): return fbeta_score(y_true, y_pred, beta=2) # Fit the EXtra-Trees and Random Forest to the data atom.run(models=['et', 'rf'], metric=f2_score, n_calls=5, n_random_starts=2, bo_params={'cv': 1}, bagging=5)","title":"Run the pipeline"},{"location":"examples/binary_classification/binary_classification/#analyze-the-results","text":"# Let's have a look at the final scoring atom.scoring() # The winning model is indicated with a ! and can be accessed through the winner attribute print(f'\\n\\nAnd the winner is the {atom.winner.longname} model!!') print('Score on the training set: ', atom.winner.score_train) print('Score on the test set: ', atom.winner.score_test) Results ===================== >> Extra-Trees --> f2_score: 0.670 \u00b1 0.004 Random Forest --> f2_score: 0.644 \u00b1 0.022 And the winner is the Extra-Trees model!! Score on the training set: 0.748396895038812 Score on the test set: 0.6670709520921769 We can make many plots to check the performance of the models # The probabilties plot shows the distribution of predicted # probabilities for the positive class atom.winner.plot_probabilities() # The threshold plot let us compare how different metrics # perform for different thresholds atom.winner.plot_threshold(metric=['f1', 'accuracy', 'average_precision'], steps=50, filename='thresholds.png') # The ROC and PRC curve are also typical ways of measuring performance atom.plot_roc(title=\"ROC for the LightGBM vs CatBoost model\") atom.plot_prc(title=\"PRC comparison of the models\")","title":"Analyze the results"},{"location":"examples/calibration/calibration/","text":"Calibration This example shows us how to use the calibration method to calibrate a classifier. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow . Load the data # Import packages import pandas as pd from atom import ATOMClassifier # Get the dataset's features and targets X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 99450 MountGambier 11.3 23.1 0.0 7.0 12.8 S 50.0 103753 Woomera 16.0 28.9 0.0 10.4 8.2 ESE 44.0 120513 Perth 14.7 17.8 8.6 2.0 5.3 SW 43.0 47178 Canberra 13.2 18.7 0.0 NaN NaN E 31.0 54139 Ballarat 7.6 19.1 0.0 NaN NaN SE 48.0 Run the pipeline # Initialize the ATOM class atom = ATOMClassifier(X, y='RainTomorrow', n_rows=1e4, verbose=1, warnings='ignore', random_state=1) # Handle missing values and categorical columns in the dataset atom.impute(strat_num='median', strat_cat='most_frequent') atom.encode(5, encode_type='target', frac_to_other=0.05) # Fit a linear SVM to the data atom.run('lsvm') << ================== ATOM ================== >> Algorithm task: binary classification. Applying data cleaning... Dataset stats ================= >> Shape: (10000, 22) Missing values: 20763 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 8000 Size of test set: 2000 Fitting Imputer... Imputing missing values... Fitting Encoder... Encoding categorical columns... Running pipeline ============================= >> Models in pipeline: lSVM Metric: f1 Results for Linear SVM: Fitting ----------------------------------------- Score on the train set --> f1: 0.5654 Score on the test set --> f1: 0.5938 Time elapsed: 0.442s ------------------------------------------------- Total time: 0.451s Final results ========================= >> Duration: 0.453s ------------------------------------------ Linear SVM --> f1: 0.594 Analyze the results # Check our model's calibration atom.plot_calibration() # Let's try to improve it using the calibrate method atom.calibrate(method='isotonic', cv=5) atom.plot_calibration()","title":"Calibration"},{"location":"examples/calibration/calibration/#calibration","text":"This example shows us how to use the calibration method to calibrate a classifier. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow .","title":"Calibration"},{"location":"examples/calibration/calibration/#load-the-data","text":"# Import packages import pandas as pd from atom import ATOMClassifier # Get the dataset's features and targets X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 99450 MountGambier 11.3 23.1 0.0 7.0 12.8 S 50.0 103753 Woomera 16.0 28.9 0.0 10.4 8.2 ESE 44.0 120513 Perth 14.7 17.8 8.6 2.0 5.3 SW 43.0 47178 Canberra 13.2 18.7 0.0 NaN NaN E 31.0 54139 Ballarat 7.6 19.1 0.0 NaN NaN SE 48.0","title":"Load the data"},{"location":"examples/calibration/calibration/#run-the-pipeline","text":"# Initialize the ATOM class atom = ATOMClassifier(X, y='RainTomorrow', n_rows=1e4, verbose=1, warnings='ignore', random_state=1) # Handle missing values and categorical columns in the dataset atom.impute(strat_num='median', strat_cat='most_frequent') atom.encode(5, encode_type='target', frac_to_other=0.05) # Fit a linear SVM to the data atom.run('lsvm') << ================== ATOM ================== >> Algorithm task: binary classification. Applying data cleaning... Dataset stats ================= >> Shape: (10000, 22) Missing values: 20763 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 8000 Size of test set: 2000 Fitting Imputer... Imputing missing values... Fitting Encoder... Encoding categorical columns... Running pipeline ============================= >> Models in pipeline: lSVM Metric: f1 Results for Linear SVM: Fitting ----------------------------------------- Score on the train set --> f1: 0.5654 Score on the test set --> f1: 0.5938 Time elapsed: 0.442s ------------------------------------------------- Total time: 0.451s Final results ========================= >> Duration: 0.453s ------------------------------------------ Linear SVM --> f1: 0.594","title":"Run the pipeline"},{"location":"examples/calibration/calibration/#analyze-the-results","text":"# Check our model's calibration atom.plot_calibration() # Let's try to improve it using the calibrate method atom.calibrate(method='isotonic', cv=5) atom.plot_calibration()","title":"Analyze the results"},{"location":"examples/early_stopping/early_stopping/","text":"Early stopping This example shows how we can use early stopping to reduce the time it takes to run the pipeline. This option is only available for models that allow in-training evaluation (XGBoost, LightGBM and CatBoost). Import the breast cancer dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict whether a patient has breast cancer or not. Load the data # Import packages from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier # Get the dataset's features and targets X, y = load_breast_cancer(return_X_y=True) Run the pipeline # Start ATOM and fit the models using early stopping # An early stopping of 0.1 means that the model will stop if it # didn't improve in the last 10% of it's iterations. atom = ATOMClassifier(X, y, n_jobs=2, verbose=2, warnings=False, random_state=1) atom.run('LGB', metric='ap', n_calls=7, n_random_starts=3, bo_params={'early_stopping': 0.1, 'cv': 1}) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Applying data cleaning... Dataset stats ================= >> Shape: (569, 31) Scaled: False ---------------------------------- Size of training set: 456 Size of test set: 113 ---------------------------------- Class balance: 0:1 <==> 0.6:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 212 | 167 | 45 | | 1 | 357 | 289 | 68 | Running pipeline ============================= >> Models in pipeline: LGB Metric: average_precision Running BO for LightGBM... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 499, 'learning_rate': 0.73, 'max_depth': 2, 'num_leaves': 40, 'min_child_weight': 5, 'min_child_samples': 18, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 100, 'reg_lambda': 100} Early stop at iteration 50 of 499. Evaluation --> average_precision: 0.6304 Best average_precision: 0.6304 Time iteration: 0.025s Total time: 0.038s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 170, 'learning_rate': 0.11, 'max_depth': 5, 'num_leaves': 25, 'min_child_weight': 11, 'min_child_samples': 28, 'subsample': 0.7, 'colsample_bytree': 0.6, 'reg_alpha': 100, 'reg_lambda': 10} Early stop at iteration 18 of 170. Evaluation --> average_precision: 0.6304 Best average_precision: 0.6304 Time iteration: 0.020s Total time: 0.062s Random start 3 ---------------------------------- Parameters --> {'n_estimators': 364, 'learning_rate': 0.4, 'max_depth': 2, 'num_leaves': 30, 'min_child_weight': 17, 'min_child_samples': 27, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 10} Early stop at iteration 45 of 364. Evaluation --> average_precision: 0.9785 Best average_precision: 0.9785 Time iteration: 0.022s Total time: 0.088s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 496, 'learning_rate': 0.5, 'max_depth': 5, 'num_leaves': 37, 'min_child_weight': 15, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 88 of 496. Evaluation --> average_precision: 0.9916 Best average_precision: 0.9916 Time iteration: 0.027s Total time: 0.936s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 500, 'learning_rate': 1.0, 'max_depth': 10, 'num_leaves': 40, 'min_child_weight': 1, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 0.01} Early stop at iteration 57 of 500. Evaluation --> average_precision: 0.9947 Best average_precision: 0.9947 Time iteration: 0.027s Total time: 1.195s Iteration 6 ------------------------------------- Parameters --> {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'num_leaves': 40, 'min_child_weight': 20, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 271 of 500. Evaluation --> average_precision: 0.9980 Best average_precision: 0.9980 Time iteration: 0.048s Total time: 1.478s Iteration 7 ------------------------------------- Parameters --> {'n_estimators': 178, 'learning_rate': 0.28, 'max_depth': 1, 'num_leaves': 21, 'min_child_weight': 12, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 130 of 178. Evaluation --> average_precision: 0.9983 Best average_precision: 0.9983 Time iteration: 0.030s Total time: 1.843s Results for LightGBM: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 178, 'learning_rate': 0.28, 'max_depth': 1, 'num_leaves': 21, 'min_child_weight': 12, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Best evaluation --> average_precision: 0.9983 Time elapsed: 2.103s Fitting ----------------------------------------- Early stop at iteration 174 of 178. Score on the train set --> average_precision: 0.9971 Score on the test set --> average_precision: 0.9847 Time elapsed: 0.040s ------------------------------------------------- Total time: 2.151s Final results ========================= >> Duration: 2.153s ------------------------------------------ LightGBM --> average_precision: 0.985 Analyze the results # For these models, we can plot the evaluation on the train and test set during training # Note that the metric is provided by the model's library, not ATOM! atom.lgb.plot_evals(title=\"LightGBM's evaluation curve\", figsize=(11, 9))","title":"Early stopping"},{"location":"examples/early_stopping/early_stopping/#early-stopping","text":"This example shows how we can use early stopping to reduce the time it takes to run the pipeline. This option is only available for models that allow in-training evaluation (XGBoost, LightGBM and CatBoost). Import the breast cancer dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict whether a patient has breast cancer or not.","title":"Early stopping"},{"location":"examples/early_stopping/early_stopping/#load-the-data","text":"# Import packages from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier # Get the dataset's features and targets X, y = load_breast_cancer(return_X_y=True)","title":"Load the data"},{"location":"examples/early_stopping/early_stopping/#run-the-pipeline","text":"# Start ATOM and fit the models using early stopping # An early stopping of 0.1 means that the model will stop if it # didn't improve in the last 10% of it's iterations. atom = ATOMClassifier(X, y, n_jobs=2, verbose=2, warnings=False, random_state=1) atom.run('LGB', metric='ap', n_calls=7, n_random_starts=3, bo_params={'early_stopping': 0.1, 'cv': 1}) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Applying data cleaning... Dataset stats ================= >> Shape: (569, 31) Scaled: False ---------------------------------- Size of training set: 456 Size of test set: 113 ---------------------------------- Class balance: 0:1 <==> 0.6:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 212 | 167 | 45 | | 1 | 357 | 289 | 68 | Running pipeline ============================= >> Models in pipeline: LGB Metric: average_precision Running BO for LightGBM... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 499, 'learning_rate': 0.73, 'max_depth': 2, 'num_leaves': 40, 'min_child_weight': 5, 'min_child_samples': 18, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 100, 'reg_lambda': 100} Early stop at iteration 50 of 499. Evaluation --> average_precision: 0.6304 Best average_precision: 0.6304 Time iteration: 0.025s Total time: 0.038s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 170, 'learning_rate': 0.11, 'max_depth': 5, 'num_leaves': 25, 'min_child_weight': 11, 'min_child_samples': 28, 'subsample': 0.7, 'colsample_bytree': 0.6, 'reg_alpha': 100, 'reg_lambda': 10} Early stop at iteration 18 of 170. Evaluation --> average_precision: 0.6304 Best average_precision: 0.6304 Time iteration: 0.020s Total time: 0.062s Random start 3 ---------------------------------- Parameters --> {'n_estimators': 364, 'learning_rate': 0.4, 'max_depth': 2, 'num_leaves': 30, 'min_child_weight': 17, 'min_child_samples': 27, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 10} Early stop at iteration 45 of 364. Evaluation --> average_precision: 0.9785 Best average_precision: 0.9785 Time iteration: 0.022s Total time: 0.088s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 496, 'learning_rate': 0.5, 'max_depth': 5, 'num_leaves': 37, 'min_child_weight': 15, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 88 of 496. Evaluation --> average_precision: 0.9916 Best average_precision: 0.9916 Time iteration: 0.027s Total time: 0.936s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 500, 'learning_rate': 1.0, 'max_depth': 10, 'num_leaves': 40, 'min_child_weight': 1, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 0.01} Early stop at iteration 57 of 500. Evaluation --> average_precision: 0.9947 Best average_precision: 0.9947 Time iteration: 0.027s Total time: 1.195s Iteration 6 ------------------------------------- Parameters --> {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'num_leaves': 40, 'min_child_weight': 20, 'min_child_samples': 10, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 271 of 500. Evaluation --> average_precision: 0.9980 Best average_precision: 0.9980 Time iteration: 0.048s Total time: 1.478s Iteration 7 ------------------------------------- Parameters --> {'n_estimators': 178, 'learning_rate': 0.28, 'max_depth': 1, 'num_leaves': 21, 'min_child_weight': 12, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Early stop at iteration 130 of 178. Evaluation --> average_precision: 0.9983 Best average_precision: 0.9983 Time iteration: 0.030s Total time: 1.843s Results for LightGBM: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 178, 'learning_rate': 0.28, 'max_depth': 1, 'num_leaves': 21, 'min_child_weight': 12, 'min_child_samples': 15, 'subsample': 0.9, 'colsample_bytree': 0.5, 'reg_alpha': 0, 'reg_lambda': 100} Best evaluation --> average_precision: 0.9983 Time elapsed: 2.103s Fitting ----------------------------------------- Early stop at iteration 174 of 178. Score on the train set --> average_precision: 0.9971 Score on the test set --> average_precision: 0.9847 Time elapsed: 0.040s ------------------------------------------------- Total time: 2.151s Final results ========================= >> Duration: 2.153s ------------------------------------------ LightGBM --> average_precision: 0.985","title":"Run the pipeline"},{"location":"examples/early_stopping/early_stopping/#analyze-the-results","text":"# For these models, we can plot the evaluation on the train and test set during training # Note that the metric is provided by the model's library, not ATOM! atom.lgb.plot_evals(title=\"LightGBM's evaluation curve\", figsize=(11, 9))","title":"Analyze the results"},{"location":"examples/feature_engineering/feature_engineering/","text":"Feature engineering This example shows how to use automated feature generation to improve your model's performance. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow . Load the data # Import packages import pandas as pd from atom import ATOMClassifier # Load data X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 118244 Perth 20.9 35.5 0.0 11.0 13.1 SW 31.0 17985 NorahHead 19.1 21.0 0.0 NaN NaN S 44.0 96569 Adelaide 7.6 16.1 1.8 NaN NaN SW 35.0 63982 MelbourneAirport 5.8 11.9 3.6 1.8 6.6 N 52.0 97093 MountGambier 6.2 11.7 10.8 1.8 0.8 WNW 31.0 Run the pipeline # Initiate ATOM and apply data cleaning atom = ATOMClassifier(X, n_rows=1e4, test_size=0.2, verbose=0, random_state=1) atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) atom.encode(max_onehot=10, frac_to_other=0.04) # Let's see how a LightGBM model performs without adding additional features atom.run('LGB', metric='auc') atom.scoring() Results ===================== >> LightGBM --> roc_auc: 0.945 # What are the most important fetaures? atom.plot_feature_importance(show=10) Now let's create some new fetaures using Deep Feature Synthesis atom.verbose = 2 # Increase verbosity to see the output # Create 100 new features using DFS atom.feature_generation(strategy='dfs', n_features=100) # Select the best 50 features using RFECV atom.feature_selection(strategy='RFECV', solver='lgb', n_features=30, scoring='auc') Fitting FeatureGenerator... Creating new features... --> 100 new features were added to the dataset. Fitting FeatureSelector... C:\\Users\\Mavs\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) Performing feature selection ... --> Feature Location was removed due to low variance. Value 0.21025 repeated in 100.0% of rows. --> Feature WindDir3pm + WindSpeed9am was removed due to collinearity with another feature. --> Feature Pressure9am + RainToday_Yes was removed due to collinearity with another feature. --> Feature Cloud9am + Humidity9am was removed due to collinearity with another feature. --> Feature MaxTemp + WindGustDir was removed due to collinearity with another feature. --> Feature Location + Pressure3pm was removed due to collinearity with another feature. --> Feature Location + Rainfall was removed due to collinearity with another feature. --> Feature RainToday_Yes + Temp9am was removed due to collinearity with another feature. --> Feature Evaporation + Location was removed due to collinearity with another feature. --> Feature Location + MaxTemp was removed due to collinearity with another feature. --> Feature Location - RainToday_Yes was removed due to collinearity with another feature. --> Feature Cloud3pm - RainToday_Yes was removed due to collinearity with another feature. --> Feature Humidity3pm - WindDir9am was removed due to collinearity with another feature. --> Feature Sunshine - WindDir3pm was removed due to collinearity with another feature. --> Feature WindDir9am - WindSpeed3pm was removed due to collinearity with another feature. --> Feature Humidity9am * Pressure9am was removed due to collinearity with another feature. --> Feature MinTemp * Pressure9am was removed due to collinearity with another feature. --> Feature Location * WindGustDir was removed due to collinearity with another feature. --> Feature MaxTemp * Pressure3pm was removed due to collinearity with another feature. --> Feature Pressure9am * WindSpeed9am was removed due to collinearity with another feature. --> Feature Cloud3pm * Pressure3pm was removed due to collinearity with another feature. --> Feature Temp3pm * WindSpeed3pm was removed due to collinearity with another feature. --> Feature Location * Pressure3pm was removed due to collinearity with another feature. --> Feature Pressure9am * Temp9am was removed due to collinearity with another feature. --> Feature Humidity9am / Pressure3pm was removed due to collinearity with another feature. --> Feature RainToday_No / Location was removed due to collinearity with another feature. --> Feature WindSpeed9am / RainToday_No was removed due to collinearity with another feature. --> Feature Pressure3pm / Location was removed due to collinearity with another feature. --> Feature WindDir9am / Pressure9am was removed due to collinearity with another feature. --> Feature Evaporation / RainToday_Yes was removed due to collinearity with another feature. --> Feature SQRT(WindGustDir) was removed due to collinearity with another feature. --> Feature SQRT(Pressure9am) was removed due to collinearity with another feature. --> Feature WindSpeed9am was removed by the RFECV. --> Feature WindSpeed3pm was removed by the RFECV. --> Feature Cloud3pm was removed by the RFECV. --> Feature RainToday_Yes was removed by the RFECV. --> Feature RainToday_No was removed by the RFECV. --> Feature Rainfall * RainToday_No was removed by the RFECV. --> Feature Location / RainToday_Yes was removed by the RFECV. --> Feature LOG(Rainfall) was removed by the RFECV. # The collineart attribute shows what features # were removed due to collinearity atom.collinear .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drop_feature correlated_feature correlation_value 0 WindDir3pm + WindSpeed9am WindSpeed9am 0.99999 1 Pressure9am + RainToday_Yes Pressure9am 0.99832 2 Cloud9am + Humidity9am Humidity9am 0.99422 3 MaxTemp + WindGustDir MaxTemp 0.99998 4 Location + Pressure3pm Pressure3pm 1.0 5 Location + Rainfall Rainfall 1.0 6 RainToday_Yes + Temp9am Temp9am 0.99794 7 Evaporation + Location Evaporation 1.0 8 Location + MaxTemp MaxTemp, MaxTemp + WindGustDir 1.0, 0.99998 9 Location - RainToday_Yes RainToday_Yes, RainToday_No -1.0, 0.98441 10 Cloud3pm - RainToday_Yes Cloud3pm 0.98149 11 Humidity3pm - WindDir9am Humidity3pm 1.0 12 Sunshine - WindDir3pm Sunshine 0.99987 13 WindDir9am - WindSpeed3pm WindSpeed3pm, RainToday_No - WindSpeed3pm -0.99998, 0.99879 14 Humidity9am * Pressure9am Humidity9am, Cloud9am + Humidity9am 0.99965, 0.99341 15 MinTemp * Pressure9am MinTemp 0.99992 16 Location * WindGustDir WindGustDir 1.0 17 MaxTemp * Pressure3pm MaxTemp, MaxTemp + WindGustDir, Location + Max... 0.9998, 0.99974, 0.9998 18 Pressure9am * WindSpeed9am WindSpeed9am, WindDir3pm + WindSpeed9am, WindD... 0.99991, 0.99989, -0.99991 19 Cloud3pm * Pressure3pm Cloud3pm, Cloud3pm - RainToday_Yes 0.99986, 0.98165 20 Temp3pm * WindSpeed3pm MaxTemp * WindSpeed3pm 0.99029 21 Location * Pressure3pm Pressure3pm, Location + Pressure3pm 1.0, 1.0 22 Pressure9am * Temp9am Temp9am, RainToday_Yes + Temp9am 0.99987, 0.9976 23 Humidity9am / Pressure3pm Humidity9am, Cloud9am + Humidity9am, Humidity9... 0.99965, 0.99422, 0.99863 24 RainToday_No / Location RainToday_Yes, RainToday_No, Location - RainTo... -0.98441, 1.0, 0.98441 25 WindSpeed9am / RainToday_No WindSpeed9am, WindDir3pm + WindSpeed9am, WindD... 1.0, 0.99999, -0.99998, 0.99993 26 Pressure3pm / Location Pressure3pm, Location + Pressure3pm, Location ... 1.0, 1.0, 1.0 27 WindDir9am / Pressure9am WindDir9am, Location - WindDir9am 0.99958, -0.99958 28 Evaporation / RainToday_Yes Evaporation, Evaporation + Location 1.0, 1.0 29 SQRT(WindGustDir) WindGustDir, Location * WindGustDir 0.9979, 0.9979 30 SQRT(Pressure9am) Pressure9am, Pressure9am + RainToday_Yes 1.0, 0.99831 # After applying RFECV, we can plot the score per number of features atom.plot_rfecv() # Since the new features apply divisions, we can have inf values in the dataset atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) Fitting Imputer... Imputing missing values... --> Imputing 4 missing values using the KNN imputer in feature Temp9am / MinTemp. --> Dropping feature WindSpeed3pm / Rainfall for containing 5766 (64%) missing values. --> Imputing 33 missing values using the KNN imputer in feature Evaporation / WindSpeed3pm. --> Imputing 17 missing values using the KNN imputer in feature RainToday_No / Evaporation. --> Imputing 4 missing values using the KNN imputer in feature Cloud9am / MinTemp. --> Imputing 290 missing values using the KNN imputer in feature Pressure9am / Cloud3pm. --> Imputing 33 missing values using the KNN imputer in feature Pressure9am / WindSpeed3pm. --> Imputing 17 missing values using the KNN imputer in feature MinTemp / Evaporation. --> Imputing 4 missing values using the KNN imputer in feature Cloud3pm / MinTemp. --> Imputing 4 missing values using the KNN imputer in feature Rainfall / MinTemp. --> Imputing 17 missing values using the KNN imputer in feature WindSpeed9am / Evaporation. --> Imputing 33 missing values using the KNN imputer in feature WindSpeed9am / WindSpeed3pm. --> Imputing 33 missing values using the KNN imputer in feature Rainfall / WindSpeed3pm. # Let's see how the model performs now atom.run('LGB') Running pipeline ============================= >> Models in pipeline: LGB Metric: roc_auc Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> roc_auc: 0.9955 Score on the test set --> roc_auc: 0.9502 Time elapsed: 1.409s ------------------------------------------------- Total time: 1.428s Final results ========================= >> Duration: 1.430s ------------------------------------------ LightGBM --> roc_auc: 0.950 # Did the feature importance change? atom.plot_feature_importance(show=10) Lets try the same using Genetic Feature Generation atom = ATOMClassifier(X, n_rows=1e4, test_size=0.2, verbose=0, random_state=1) atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) atom.encode(max_onehot=10, frac_to_other=0.04) atom.verbose = 2 # Increase verbosity to see the steps # Create new features using Genetic Programming atom.feature_generation(strategy='genetic', n_features=10, generations=20, population=1000) Fitting FeatureGenerator... | Population Average | Best Individual | ---- ------------------------- ------------------------------------------ ---------- Gen Length Fitness Length Fitness OOB Fitness Time Left 0 3.13 0.135435 3 0.503097 N/A 11.11s 1 3.18 0.348074 7 0.506413 N/A 10.95s 2 3.23 0.437331 5 0.513643 N/A 9.99s 3 3.44 0.457825 5 0.513643 N/A 9.33s 4 4.50 0.449698 9 0.522278 N/A 8.81s 5 5.65 0.45053 13 0.527538 N/A 9.66s 6 7.76 0.472921 17 0.528579 N/A 7.96s 7 9.36 0.49098 15 0.529782 N/A 7.43s 8 9.83 0.488757 15 0.529782 N/A 6.76s 9 10.63 0.488877 15 0.529782 N/A 6.18s 10 11.03 0.485024 13 0.528901 N/A 5.61s 11 11.31 0.48378 11 0.527328 N/A 5.04s 12 10.61 0.482128 11 0.527328 N/A 4.35s 13 10.99 0.486747 11 0.527328 N/A 3.78s 14 10.81 0.487697 11 0.527328 N/A 3.11s 15 11.03 0.487411 11 0.527328 N/A 2.49s 16 10.89 0.490537 11 0.527328 N/A 1.86s 17 10.98 0.484706 11 0.546642 N/A 1.24s 18 10.90 0.481711 11 0.546642 N/A 0.62s 19 10.96 0.486717 13 0.555663 N/A 0.00s Creating new features... ------------------------------------------------- --> 10 new features were added to the dataset. # We can see the feature's fitness and description through the genetic_features attribute atom.genetic_features .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name description fitness 0 Feature 23 mul(Humidity3pm, mul(add(WindGustSpeed, WindDi... 0.542663 1 Feature 24 mul(sub(add(WindGustSpeed, WindDir9am), Sunshi... 0.539049 2 Feature 25 mul(Humidity3pm, mul(sub(Humidity3pm, Sunshine... 0.533669 3 Feature 26 mul(Humidity3pm, mul(Humidity3pm, mul(Humidity... 0.530592 4 Feature 27 mul(Humidity3pm, mul(Humidity3pm, add(WindGust... 0.532912 5 Feature 28 mul(sub(sub(add(WindGustSpeed, WindDir9am), Su... 0.522363 6 Feature 29 mul(Humidity3pm, mul(Humidity3pm, sub(sub(sub(... 0.518317 7 Feature 30 mul(mul(Humidity3pm, sub(sub(add(WindGustSpeed... 0.511776 8 Feature 31 mul(Humidity3pm, mul(Humidity3pm, mul(Humidity... 0.514151 9 Feature 32 mul(Humidity3pm, mul(Humidity3pm, sub(sub(sub(... 0.516328 # And fit the model again atom.run('LGB', metric='auc') Running pipeline ============================= >> Models in pipeline: LGB Metric: roc_auc Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> roc_auc: 0.9879 Score on the test set --> roc_auc: 0.9459 Time elapsed: 0.545s ------------------------------------------------- Total time: 0.557s Final results ========================= >> Duration: 0.559s ------------------------------------------ LightGBM --> roc_auc: 0.946 # And show the feature importance atom.plot_feature_importance(show=10)","title":"Feature engineering"},{"location":"examples/feature_engineering/feature_engineering/#feature-engineering","text":"This example shows how to use automated feature generation to improve your model's performance. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow .","title":"Feature engineering"},{"location":"examples/feature_engineering/feature_engineering/#load-the-data","text":"# Import packages import pandas as pd from atom import ATOMClassifier # Load data X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 118244 Perth 20.9 35.5 0.0 11.0 13.1 SW 31.0 17985 NorahHead 19.1 21.0 0.0 NaN NaN S 44.0 96569 Adelaide 7.6 16.1 1.8 NaN NaN SW 35.0 63982 MelbourneAirport 5.8 11.9 3.6 1.8 6.6 N 52.0 97093 MountGambier 6.2 11.7 10.8 1.8 0.8 WNW 31.0","title":"Load the data"},{"location":"examples/feature_engineering/feature_engineering/#run-the-pipeline","text":"# Initiate ATOM and apply data cleaning atom = ATOMClassifier(X, n_rows=1e4, test_size=0.2, verbose=0, random_state=1) atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) atom.encode(max_onehot=10, frac_to_other=0.04) # Let's see how a LightGBM model performs without adding additional features atom.run('LGB', metric='auc') atom.scoring() Results ===================== >> LightGBM --> roc_auc: 0.945 # What are the most important fetaures? atom.plot_feature_importance(show=10) Now let's create some new fetaures using Deep Feature Synthesis atom.verbose = 2 # Increase verbosity to see the output # Create 100 new features using DFS atom.feature_generation(strategy='dfs', n_features=100) # Select the best 50 features using RFECV atom.feature_selection(strategy='RFECV', solver='lgb', n_features=30, scoring='auc') Fitting FeatureGenerator... Creating new features... --> 100 new features were added to the dataset. Fitting FeatureSelector... C:\\Users\\Mavs\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) Performing feature selection ... --> Feature Location was removed due to low variance. Value 0.21025 repeated in 100.0% of rows. --> Feature WindDir3pm + WindSpeed9am was removed due to collinearity with another feature. --> Feature Pressure9am + RainToday_Yes was removed due to collinearity with another feature. --> Feature Cloud9am + Humidity9am was removed due to collinearity with another feature. --> Feature MaxTemp + WindGustDir was removed due to collinearity with another feature. --> Feature Location + Pressure3pm was removed due to collinearity with another feature. --> Feature Location + Rainfall was removed due to collinearity with another feature. --> Feature RainToday_Yes + Temp9am was removed due to collinearity with another feature. --> Feature Evaporation + Location was removed due to collinearity with another feature. --> Feature Location + MaxTemp was removed due to collinearity with another feature. --> Feature Location - RainToday_Yes was removed due to collinearity with another feature. --> Feature Cloud3pm - RainToday_Yes was removed due to collinearity with another feature. --> Feature Humidity3pm - WindDir9am was removed due to collinearity with another feature. --> Feature Sunshine - WindDir3pm was removed due to collinearity with another feature. --> Feature WindDir9am - WindSpeed3pm was removed due to collinearity with another feature. --> Feature Humidity9am * Pressure9am was removed due to collinearity with another feature. --> Feature MinTemp * Pressure9am was removed due to collinearity with another feature. --> Feature Location * WindGustDir was removed due to collinearity with another feature. --> Feature MaxTemp * Pressure3pm was removed due to collinearity with another feature. --> Feature Pressure9am * WindSpeed9am was removed due to collinearity with another feature. --> Feature Cloud3pm * Pressure3pm was removed due to collinearity with another feature. --> Feature Temp3pm * WindSpeed3pm was removed due to collinearity with another feature. --> Feature Location * Pressure3pm was removed due to collinearity with another feature. --> Feature Pressure9am * Temp9am was removed due to collinearity with another feature. --> Feature Humidity9am / Pressure3pm was removed due to collinearity with another feature. --> Feature RainToday_No / Location was removed due to collinearity with another feature. --> Feature WindSpeed9am / RainToday_No was removed due to collinearity with another feature. --> Feature Pressure3pm / Location was removed due to collinearity with another feature. --> Feature WindDir9am / Pressure9am was removed due to collinearity with another feature. --> Feature Evaporation / RainToday_Yes was removed due to collinearity with another feature. --> Feature SQRT(WindGustDir) was removed due to collinearity with another feature. --> Feature SQRT(Pressure9am) was removed due to collinearity with another feature. --> Feature WindSpeed9am was removed by the RFECV. --> Feature WindSpeed3pm was removed by the RFECV. --> Feature Cloud3pm was removed by the RFECV. --> Feature RainToday_Yes was removed by the RFECV. --> Feature RainToday_No was removed by the RFECV. --> Feature Rainfall * RainToday_No was removed by the RFECV. --> Feature Location / RainToday_Yes was removed by the RFECV. --> Feature LOG(Rainfall) was removed by the RFECV. # The collineart attribute shows what features # were removed due to collinearity atom.collinear .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drop_feature correlated_feature correlation_value 0 WindDir3pm + WindSpeed9am WindSpeed9am 0.99999 1 Pressure9am + RainToday_Yes Pressure9am 0.99832 2 Cloud9am + Humidity9am Humidity9am 0.99422 3 MaxTemp + WindGustDir MaxTemp 0.99998 4 Location + Pressure3pm Pressure3pm 1.0 5 Location + Rainfall Rainfall 1.0 6 RainToday_Yes + Temp9am Temp9am 0.99794 7 Evaporation + Location Evaporation 1.0 8 Location + MaxTemp MaxTemp, MaxTemp + WindGustDir 1.0, 0.99998 9 Location - RainToday_Yes RainToday_Yes, RainToday_No -1.0, 0.98441 10 Cloud3pm - RainToday_Yes Cloud3pm 0.98149 11 Humidity3pm - WindDir9am Humidity3pm 1.0 12 Sunshine - WindDir3pm Sunshine 0.99987 13 WindDir9am - WindSpeed3pm WindSpeed3pm, RainToday_No - WindSpeed3pm -0.99998, 0.99879 14 Humidity9am * Pressure9am Humidity9am, Cloud9am + Humidity9am 0.99965, 0.99341 15 MinTemp * Pressure9am MinTemp 0.99992 16 Location * WindGustDir WindGustDir 1.0 17 MaxTemp * Pressure3pm MaxTemp, MaxTemp + WindGustDir, Location + Max... 0.9998, 0.99974, 0.9998 18 Pressure9am * WindSpeed9am WindSpeed9am, WindDir3pm + WindSpeed9am, WindD... 0.99991, 0.99989, -0.99991 19 Cloud3pm * Pressure3pm Cloud3pm, Cloud3pm - RainToday_Yes 0.99986, 0.98165 20 Temp3pm * WindSpeed3pm MaxTemp * WindSpeed3pm 0.99029 21 Location * Pressure3pm Pressure3pm, Location + Pressure3pm 1.0, 1.0 22 Pressure9am * Temp9am Temp9am, RainToday_Yes + Temp9am 0.99987, 0.9976 23 Humidity9am / Pressure3pm Humidity9am, Cloud9am + Humidity9am, Humidity9... 0.99965, 0.99422, 0.99863 24 RainToday_No / Location RainToday_Yes, RainToday_No, Location - RainTo... -0.98441, 1.0, 0.98441 25 WindSpeed9am / RainToday_No WindSpeed9am, WindDir3pm + WindSpeed9am, WindD... 1.0, 0.99999, -0.99998, 0.99993 26 Pressure3pm / Location Pressure3pm, Location + Pressure3pm, Location ... 1.0, 1.0, 1.0 27 WindDir9am / Pressure9am WindDir9am, Location - WindDir9am 0.99958, -0.99958 28 Evaporation / RainToday_Yes Evaporation, Evaporation + Location 1.0, 1.0 29 SQRT(WindGustDir) WindGustDir, Location * WindGustDir 0.9979, 0.9979 30 SQRT(Pressure9am) Pressure9am, Pressure9am + RainToday_Yes 1.0, 0.99831 # After applying RFECV, we can plot the score per number of features atom.plot_rfecv() # Since the new features apply divisions, we can have inf values in the dataset atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) Fitting Imputer... Imputing missing values... --> Imputing 4 missing values using the KNN imputer in feature Temp9am / MinTemp. --> Dropping feature WindSpeed3pm / Rainfall for containing 5766 (64%) missing values. --> Imputing 33 missing values using the KNN imputer in feature Evaporation / WindSpeed3pm. --> Imputing 17 missing values using the KNN imputer in feature RainToday_No / Evaporation. --> Imputing 4 missing values using the KNN imputer in feature Cloud9am / MinTemp. --> Imputing 290 missing values using the KNN imputer in feature Pressure9am / Cloud3pm. --> Imputing 33 missing values using the KNN imputer in feature Pressure9am / WindSpeed3pm. --> Imputing 17 missing values using the KNN imputer in feature MinTemp / Evaporation. --> Imputing 4 missing values using the KNN imputer in feature Cloud3pm / MinTemp. --> Imputing 4 missing values using the KNN imputer in feature Rainfall / MinTemp. --> Imputing 17 missing values using the KNN imputer in feature WindSpeed9am / Evaporation. --> Imputing 33 missing values using the KNN imputer in feature WindSpeed9am / WindSpeed3pm. --> Imputing 33 missing values using the KNN imputer in feature Rainfall / WindSpeed3pm. # Let's see how the model performs now atom.run('LGB') Running pipeline ============================= >> Models in pipeline: LGB Metric: roc_auc Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> roc_auc: 0.9955 Score on the test set --> roc_auc: 0.9502 Time elapsed: 1.409s ------------------------------------------------- Total time: 1.428s Final results ========================= >> Duration: 1.430s ------------------------------------------ LightGBM --> roc_auc: 0.950 # Did the feature importance change? atom.plot_feature_importance(show=10) Lets try the same using Genetic Feature Generation atom = ATOMClassifier(X, n_rows=1e4, test_size=0.2, verbose=0, random_state=1) atom.impute(strat_num='knn', strat_cat='remove', min_frac_rows=0.8) atom.encode(max_onehot=10, frac_to_other=0.04) atom.verbose = 2 # Increase verbosity to see the steps # Create new features using Genetic Programming atom.feature_generation(strategy='genetic', n_features=10, generations=20, population=1000) Fitting FeatureGenerator... | Population Average | Best Individual | ---- ------------------------- ------------------------------------------ ---------- Gen Length Fitness Length Fitness OOB Fitness Time Left 0 3.13 0.135435 3 0.503097 N/A 11.11s 1 3.18 0.348074 7 0.506413 N/A 10.95s 2 3.23 0.437331 5 0.513643 N/A 9.99s 3 3.44 0.457825 5 0.513643 N/A 9.33s 4 4.50 0.449698 9 0.522278 N/A 8.81s 5 5.65 0.45053 13 0.527538 N/A 9.66s 6 7.76 0.472921 17 0.528579 N/A 7.96s 7 9.36 0.49098 15 0.529782 N/A 7.43s 8 9.83 0.488757 15 0.529782 N/A 6.76s 9 10.63 0.488877 15 0.529782 N/A 6.18s 10 11.03 0.485024 13 0.528901 N/A 5.61s 11 11.31 0.48378 11 0.527328 N/A 5.04s 12 10.61 0.482128 11 0.527328 N/A 4.35s 13 10.99 0.486747 11 0.527328 N/A 3.78s 14 10.81 0.487697 11 0.527328 N/A 3.11s 15 11.03 0.487411 11 0.527328 N/A 2.49s 16 10.89 0.490537 11 0.527328 N/A 1.86s 17 10.98 0.484706 11 0.546642 N/A 1.24s 18 10.90 0.481711 11 0.546642 N/A 0.62s 19 10.96 0.486717 13 0.555663 N/A 0.00s Creating new features... ------------------------------------------------- --> 10 new features were added to the dataset. # We can see the feature's fitness and description through the genetic_features attribute atom.genetic_features .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name description fitness 0 Feature 23 mul(Humidity3pm, mul(add(WindGustSpeed, WindDi... 0.542663 1 Feature 24 mul(sub(add(WindGustSpeed, WindDir9am), Sunshi... 0.539049 2 Feature 25 mul(Humidity3pm, mul(sub(Humidity3pm, Sunshine... 0.533669 3 Feature 26 mul(Humidity3pm, mul(Humidity3pm, mul(Humidity... 0.530592 4 Feature 27 mul(Humidity3pm, mul(Humidity3pm, add(WindGust... 0.532912 5 Feature 28 mul(sub(sub(add(WindGustSpeed, WindDir9am), Su... 0.522363 6 Feature 29 mul(Humidity3pm, mul(Humidity3pm, sub(sub(sub(... 0.518317 7 Feature 30 mul(mul(Humidity3pm, sub(sub(add(WindGustSpeed... 0.511776 8 Feature 31 mul(Humidity3pm, mul(Humidity3pm, mul(Humidity... 0.514151 9 Feature 32 mul(Humidity3pm, mul(Humidity3pm, sub(sub(sub(... 0.516328 # And fit the model again atom.run('LGB', metric='auc') Running pipeline ============================= >> Models in pipeline: LGB Metric: roc_auc Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> roc_auc: 0.9879 Score on the test set --> roc_auc: 0.9459 Time elapsed: 0.545s ------------------------------------------------- Total time: 0.557s Final results ========================= >> Duration: 0.559s ------------------------------------------ LightGBM --> roc_auc: 0.946 # And show the feature importance atom.plot_feature_importance(show=10)","title":"Run the pipeline"},{"location":"examples/multi_metric/multi_metric/","text":"Multi-metric runs This example shows how we can evaluate an ATOM pipeline on multiple metrics. Import the breast cancer dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict whether a patient has breast cancer or not. Load the data # Import packages from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier # Get the dataset's features and targets X, y = load_breast_cancer(return_X_y=True) Run the pipeline # Call ATOM and run the pipeline using multipe metrics # Note that for every step of the BO, both metrics are calculated, but only the first is used for optimization! atom = ATOMClassifier(X, y, n_jobs=2, verbose=2, warnings=False, random_state=1) atom.run(['MNB', 'QDA'], metric=('f1', 'recall'), n_calls=3, n_random_starts=1, bagging=4) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Applying data cleaning... Dataset stats ================= >> Shape: (569, 31) Scaled: False ---------------------------------- Size of training set: 456 Size of test set: 113 ---------------------------------- Class balance: 0:1 <==> 0.6:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 212 | 167 | 45 | | 1 | 357 | 289 | 68 | Running pipeline ============================= >> Models in pipeline: MNB, QDA Metric: f1, recall Running BO for Multinomial Naive Bayes... Random start 1 ---------------------------------- Parameters --> {'alpha': 1, 'fit_prior': True} Evaluation --> f1: 0.9260 Best f1: 0.9260 recall: 0.9722 Best recall: 0.9722 Time iteration: 2.823s Total time: 2.826s Iteration 2 ------------------------------------- Parameters --> {'alpha': 9.744, 'fit_prior': False} Evaluation --> f1: 0.9208 Best f1: 0.9260 recall: 0.9654 Best recall: 0.9722 Time iteration: 0.043s Total time: 2.872s Iteration 3 ------------------------------------- Parameters --> {'alpha': 0.99, 'fit_prior': True} Evaluation --> f1: 0.9244 Best f1: 0.9260 recall: 0.9724 Best recall: 0.9724 Time iteration: 0.032s Total time: 3.030s Results for Multinomial Naive Bayes: Bayesian Optimization --------------------------- Best parameters --> {'alpha': 1, 'fit_prior': True} Best evaluation --> f1: 0.9260 recall: 0.9722 Time elapsed: 3.201s Fitting ----------------------------------------- Score on the train set --> f1: 0.9243 recall: 0.9723 Score on the test set --> f1: 0.9103 recall: 0.9706 Time elapsed: 0.014s Bagging ----------------------------------------- Score --> f1: 0.9100 \u00b1 0.0005 recall: 0.9669 \u00b1 0.0064 Time elapsed: 0.034s ------------------------------------------------- Total time: 3.253s Running BO for Quadratic Discriminant Analysis... Random start 1 ---------------------------------- Parameters --> {'reg_param': 0} Evaluation --> f1: 0.9654 Best f1: 0.9654 recall: 0.9619 Best recall: 0.9619 Time iteration: 0.054s Total time: 0.056s Iteration 2 ------------------------------------- Parameters --> {'reg_param': 1.0} Evaluation --> f1: 0.9245 Best f1: 0.9654 recall: 0.9897 Best recall: 0.9897 Time iteration: 0.054s Total time: 0.114s Iteration 3 ------------------------------------- Parameters --> {'reg_param': 0.1} Evaluation --> f1: 0.9626 Best f1: 0.9654 recall: 0.9793 Best recall: 0.9897 Time iteration: 0.057s Total time: 0.252s Results for Quadratic Discriminant Analysis: Bayesian Optimization --------------------------- Best parameters --> {'reg_param': 0} Best evaluation --> f1: 0.9654 recall: 0.9619 Time elapsed: 0.435s Fitting ----------------------------------------- Score on the train set --> f1: 0.9828 recall: 0.9896 Score on the test set --> f1: 0.9710 recall: 0.9853 Time elapsed: 0.018s Bagging ----------------------------------------- Score --> f1: 0.9606 \u00b1 0.0081 recall: 0.9853 \u00b1 0.0104 Time elapsed: 0.036s ------------------------------------------------- Total time: 0.494s Final results ========================= >> Duration: 3.750s ------------------------------------------ Multinomial Naive Bayes --> f1: 0.910 \u00b1 0.001 recall: 0.967 \u00b1 0.006 Quadratic Discriminant Analysis --> f1: 0.961 \u00b1 0.008 recall: 0.985 \u00b1 0.010 ! Analyze the results # Note that some columns in the results dataframe now contain a list of scores, # one for each metric, in the same order as you called them atom.results[['score_bo', 'score_train', 'score_test']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } score_bo score_train score_test model MNB [0.9259597646215939, 0.9722323049001815] [0.924342105263158, 0.972318339100346] [0.9103448275862068, 0.9705882352941176] QDA [0.965402611638704, 0.9618874773139746] [0.9828178694158075, 0.9896193771626297] [0.9710144927536232, 0.9852941176470589] # Some plots allow us to choose the metric we want to show atom.plot_bagging(metric='recall')","title":"Multi-metric"},{"location":"examples/multi_metric/multi_metric/#multi-metric-runs","text":"This example shows how we can evaluate an ATOM pipeline on multiple metrics. Import the breast cancer dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict whether a patient has breast cancer or not.","title":"Multi-metric runs"},{"location":"examples/multi_metric/multi_metric/#load-the-data","text":"# Import packages from sklearn.datasets import load_breast_cancer from atom import ATOMClassifier # Get the dataset's features and targets X, y = load_breast_cancer(return_X_y=True)","title":"Load the data"},{"location":"examples/multi_metric/multi_metric/#run-the-pipeline","text":"# Call ATOM and run the pipeline using multipe metrics # Note that for every step of the BO, both metrics are calculated, but only the first is used for optimization! atom = ATOMClassifier(X, y, n_jobs=2, verbose=2, warnings=False, random_state=1) atom.run(['MNB', 'QDA'], metric=('f1', 'recall'), n_calls=3, n_random_starts=1, bagging=4) << ================== ATOM ================== >> Algorithm task: binary classification. Parallel processing with 2 cores. Applying data cleaning... Dataset stats ================= >> Shape: (569, 31) Scaled: False ---------------------------------- Size of training set: 456 Size of test set: 113 ---------------------------------- Class balance: 0:1 <==> 0.6:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 212 | 167 | 45 | | 1 | 357 | 289 | 68 | Running pipeline ============================= >> Models in pipeline: MNB, QDA Metric: f1, recall Running BO for Multinomial Naive Bayes... Random start 1 ---------------------------------- Parameters --> {'alpha': 1, 'fit_prior': True} Evaluation --> f1: 0.9260 Best f1: 0.9260 recall: 0.9722 Best recall: 0.9722 Time iteration: 2.823s Total time: 2.826s Iteration 2 ------------------------------------- Parameters --> {'alpha': 9.744, 'fit_prior': False} Evaluation --> f1: 0.9208 Best f1: 0.9260 recall: 0.9654 Best recall: 0.9722 Time iteration: 0.043s Total time: 2.872s Iteration 3 ------------------------------------- Parameters --> {'alpha': 0.99, 'fit_prior': True} Evaluation --> f1: 0.9244 Best f1: 0.9260 recall: 0.9724 Best recall: 0.9724 Time iteration: 0.032s Total time: 3.030s Results for Multinomial Naive Bayes: Bayesian Optimization --------------------------- Best parameters --> {'alpha': 1, 'fit_prior': True} Best evaluation --> f1: 0.9260 recall: 0.9722 Time elapsed: 3.201s Fitting ----------------------------------------- Score on the train set --> f1: 0.9243 recall: 0.9723 Score on the test set --> f1: 0.9103 recall: 0.9706 Time elapsed: 0.014s Bagging ----------------------------------------- Score --> f1: 0.9100 \u00b1 0.0005 recall: 0.9669 \u00b1 0.0064 Time elapsed: 0.034s ------------------------------------------------- Total time: 3.253s Running BO for Quadratic Discriminant Analysis... Random start 1 ---------------------------------- Parameters --> {'reg_param': 0} Evaluation --> f1: 0.9654 Best f1: 0.9654 recall: 0.9619 Best recall: 0.9619 Time iteration: 0.054s Total time: 0.056s Iteration 2 ------------------------------------- Parameters --> {'reg_param': 1.0} Evaluation --> f1: 0.9245 Best f1: 0.9654 recall: 0.9897 Best recall: 0.9897 Time iteration: 0.054s Total time: 0.114s Iteration 3 ------------------------------------- Parameters --> {'reg_param': 0.1} Evaluation --> f1: 0.9626 Best f1: 0.9654 recall: 0.9793 Best recall: 0.9897 Time iteration: 0.057s Total time: 0.252s Results for Quadratic Discriminant Analysis: Bayesian Optimization --------------------------- Best parameters --> {'reg_param': 0} Best evaluation --> f1: 0.9654 recall: 0.9619 Time elapsed: 0.435s Fitting ----------------------------------------- Score on the train set --> f1: 0.9828 recall: 0.9896 Score on the test set --> f1: 0.9710 recall: 0.9853 Time elapsed: 0.018s Bagging ----------------------------------------- Score --> f1: 0.9606 \u00b1 0.0081 recall: 0.9853 \u00b1 0.0104 Time elapsed: 0.036s ------------------------------------------------- Total time: 0.494s Final results ========================= >> Duration: 3.750s ------------------------------------------ Multinomial Naive Bayes --> f1: 0.910 \u00b1 0.001 recall: 0.967 \u00b1 0.006 Quadratic Discriminant Analysis --> f1: 0.961 \u00b1 0.008 recall: 0.985 \u00b1 0.010 !","title":"Run the pipeline"},{"location":"examples/multi_metric/multi_metric/#analyze-the-results","text":"# Note that some columns in the results dataframe now contain a list of scores, # one for each metric, in the same order as you called them atom.results[['score_bo', 'score_train', 'score_test']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } score_bo score_train score_test model MNB [0.9259597646215939, 0.9722323049001815] [0.924342105263158, 0.972318339100346] [0.9103448275862068, 0.9705882352941176] QDA [0.965402611638704, 0.9618874773139746] [0.9828178694158075, 0.9896193771626297] [0.9710144927536232, 0.9852941176470589] # Some plots allow us to choose the metric we want to show atom.plot_bagging(metric='recall')","title":"Analyze the results"},{"location":"examples/multiclass_classification/multiclass_classification/","text":"Multiclass classification This example shows how to compare the performance of three models on a multiclass classification task. Import the wine dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict wines into three groups (which cultivator it's from) using features based on the results of chemical analysis. Load the data # Import packages from sklearn.datasets import load_wine from atom import ATOMClassifier # Load the dataset's features and targets X, y = load_wine(return_X_y=True, as_frame=True) # Let's have a look at a subsample of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol malic_acid ash alcalinity_of_ash magnesium total_phenols flavanoids nonflavanoid_phenols 62 13.67 1.25 1.92 18.0 94.0 2.10 1.79 0.32 46 14.38 3.59 2.28 16.0 102.0 3.25 3.17 0.27 30 13.73 1.50 2.70 22.5 101.0 3.00 3.25 0.29 54 13.74 1.67 2.25 16.4 118.0 2.60 2.90 0.21 146 13.88 5.04 2.23 20.0 80.0 0.98 0.34 0.40 Run the pipeline atom = ATOMClassifier(X, y, n_jobs=-1, warnings='ignore', verbose=2, random_state=1) # Fit the pipeline with the selected models atom.run(models=['LR','LDA', 'RF'], metric='roc_auc_ovr', n_calls=4, n_random_starts=3, bo_params={'base_estimator': 'rf', 'max_time': 100}, bagging=5) << ================== ATOM ================== >> Algorithm task: multiclass classification. Parallel processing with 16 cores. Applying data cleaning... Dataset stats ================= >> Shape: (178, 14) Scaled: False ---------------------------------- Size of training set: 143 Size of test set: 35 ---------------------------------- Class balance: 0:1:2 <==> 1.2:1.5:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 59 | 50 | 9 | | 1 | 71 | 58 | 13 | | 2 | 48 | 35 | 13 | Running pipeline ============================= >> Models in pipeline: LR, LDA, RF Metric: roc_auc_ovr Running BO for Logistic Regression... Random start 1 ---------------------------------- Parameters --> {'max_iter': 335, 'solver': 'sag', 'class_weight': None, 'penalty': 'l2', 'C': 0.001} Evaluation --> roc_auc_ovr: 0.9970 Best roc_auc_ovr: 0.9970 Time iteration: 3.490s Total time: 3.494s Random start 2 ---------------------------------- Parameters --> {'max_iter': 683, 'solver': 'lbfgs', 'class_weight': 'balanced', 'penalty': 'l2', 'C': 0.096} Evaluation --> roc_auc_ovr: 0.9996 Best roc_auc_ovr: 0.9996 Time iteration: 3.023s Total time: 6.523s Random start 3 ---------------------------------- Parameters --> {'max_iter': 352, 'solver': 'saga', 'class_weight': 'balanced', 'penalty': 'l2', 'C': 0.001} Evaluation --> roc_auc_ovr: 0.9950 Best roc_auc_ovr: 0.9996 Time iteration: 3.125s Total time: 9.652s Iteration 4 ------------------------------------- Parameters --> {'max_iter': 603, 'solver': 'liblinear', 'class_weight': None, 'penalty': 'l2', 'C': 0.061} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 2.405s Total time: 12.267s Results for Logistic Regression: Bayesian Optimization --------------------------- Best parameters --> {'max_iter': 603, 'solver': 'liblinear', 'class_weight': None, 'penalty': 'l2', 'C': 0.061} Best evaluation --> roc_auc_ovr: 1.0000 Time elapsed: 12.464s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 1.0000 Score on the test set --> roc_auc_ovr: 0.9977 Time elapsed: 0.018s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9981 \u00b1 0.0009 Time elapsed: 0.046s ------------------------------------------------- Total time: 12.535s Running BO for Linear Discriminant Analysis... Random start 1 ---------------------------------- Parameters --> {'solver': 'eigen', 'shrinkage': 1.0} Evaluation --> roc_auc_ovr: 0.8975 Best roc_auc_ovr: 0.8975 Time iteration: 0.033s Total time: 0.034s Random start 2 ---------------------------------- Parameters --> {'solver': 'svd'} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 0.033s Total time: 0.070s Random start 3 ---------------------------------- Parameters --> {'solver': 'svd'} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 0.032s Total time: 0.106s Iteration 4 ------------------------------------- Parameters --> {'solver': 'lsqr', 'shrinkage': 0.7} Evaluation --> roc_auc_ovr: 0.8996 Best roc_auc_ovr: 1.0000 Time iteration: 0.033s Total time: 0.297s Results for Linear Discriminant Analysis: Bayesian Optimization --------------------------- Best parameters --> {'solver': 'svd'} Best evaluation --> roc_auc_ovr: 1.0000 Time elapsed: 0.531s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 1.0000 Score on the test set --> roc_auc_ovr: 1.0000 Time elapsed: 0.012s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9998 \u00b1 0.0005 Time elapsed: 0.034s ------------------------------------------------- Total time: 0.583s Running BO for Random Forest... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 245, 'max_depth': 7, 'max_features': 1.0, 'criterion': 'gini', 'min_samples_split': 7, 'min_samples_leaf': 16, 'ccp_alpha': 0.008, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> roc_auc_ovr: 0.9853 Best roc_auc_ovr: 0.9853 Time iteration: 0.559s Total time: 0.564s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 400, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'gini', 'min_samples_split': 20, 'min_samples_leaf': 12, 'ccp_alpha': 0.016, 'bootstrap': True, 'max_samples': 0.7} Evaluation --> roc_auc_ovr: 0.9937 Best roc_auc_ovr: 0.9937 Time iteration: 0.671s Total time: 1.239s Random start 3 ---------------------------------- Parameters --> {'n_estimators': 78, 'max_depth': 10, 'max_features': 0.7, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 14, 'ccp_alpha': 0.025, 'bootstrap': False} Evaluation --> roc_auc_ovr: 0.9865 Best roc_auc_ovr: 0.9937 Time iteration: 0.271s Total time: 1.515s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 323, 'max_depth': 7, 'max_features': 1.0, 'criterion': 'gini', 'min_samples_split': 16, 'min_samples_leaf': 1, 'ccp_alpha': 0.007, 'bootstrap': False} Evaluation --> roc_auc_ovr: 0.9315 Best roc_auc_ovr: 0.9937 Time iteration: 0.545s Total time: 2.257s Results for Random Forest: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 400, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'gini', 'min_samples_split': 20, 'min_samples_leaf': 12, 'ccp_alpha': 0.016, 'bootstrap': True, 'max_samples': 0.7} Best evaluation --> roc_auc_ovr: 0.9937 Time elapsed: 2.452s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 0.9997 Score on the test set --> roc_auc_ovr: 0.9825 Time elapsed: 0.753s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9737 \u00b1 0.0116 Time elapsed: 3.200s ------------------------------------------------- Total time: 6.411s Final results ========================= >> Duration: 19.530s ------------------------------------------ Logistic Regression --> roc_auc_ovr: 0.998 \u00b1 0.001 Linear Discriminant Analysis --> roc_auc_ovr: 1.000 \u00b1 0.000 ! Random Forest --> roc_auc_ovr: 0.974 \u00b1 0.012 Analyze the results # We can access the pipeline's results via the results attribute atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_bo time_bo score_train score_test time_fit mean_bagging std_bagging time_bagging time model LR Logistic Regression 1.000000 12.464s 1.000000 0.997669 0.018s 0.998135 0.000932 0.046s 12.535s LDA Linear Discriminant Analysis 1.000000 0.531s 1.000000 1.000000 0.012s 0.999767 0.000466 0.034s 0.583s RF Random Forest 0.993712 2.452s 0.999725 0.982517 0.753s 0.973686 0.011577 3.200s 6.411s # Show the scoring for a different metric than the one we trained on atom.scoring('precision_macro') Results ===================== >> Logistic Regression --> precision_macro: 0.976 Linear Discriminant Analysis --> precision_macro: 0.976 Random Forest --> precision_macro: 0.9 Let's have a closer look at the Random Forest # Get the results on some other metrics print('Jaccard score:', atom.rf.scoring('jaccard_weighted')) print('Recall score:', atom.rf.scoring('recall_macro')) Jaccard score: 0.7957142857142857 Recall score: 0.8974358974358975 # Plot the confusion matrix atom.RF.plot_confusion_matrix(figsize=(9, 9)) # Save the model as a pickle file atom.RF.save_model('Random_Forest_model') Random Forest model saved successfully!","title":"Multiclass_classification"},{"location":"examples/multiclass_classification/multiclass_classification/#multiclass-classification","text":"This example shows how to compare the performance of three models on a multiclass classification task. Import the wine dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict wines into three groups (which cultivator it's from) using features based on the results of chemical analysis.","title":"Multiclass classification"},{"location":"examples/multiclass_classification/multiclass_classification/#load-the-data","text":"# Import packages from sklearn.datasets import load_wine from atom import ATOMClassifier # Load the dataset's features and targets X, y = load_wine(return_X_y=True, as_frame=True) # Let's have a look at a subsample of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol malic_acid ash alcalinity_of_ash magnesium total_phenols flavanoids nonflavanoid_phenols 62 13.67 1.25 1.92 18.0 94.0 2.10 1.79 0.32 46 14.38 3.59 2.28 16.0 102.0 3.25 3.17 0.27 30 13.73 1.50 2.70 22.5 101.0 3.00 3.25 0.29 54 13.74 1.67 2.25 16.4 118.0 2.60 2.90 0.21 146 13.88 5.04 2.23 20.0 80.0 0.98 0.34 0.40","title":"Load the data"},{"location":"examples/multiclass_classification/multiclass_classification/#run-the-pipeline","text":"atom = ATOMClassifier(X, y, n_jobs=-1, warnings='ignore', verbose=2, random_state=1) # Fit the pipeline with the selected models atom.run(models=['LR','LDA', 'RF'], metric='roc_auc_ovr', n_calls=4, n_random_starts=3, bo_params={'base_estimator': 'rf', 'max_time': 100}, bagging=5) << ================== ATOM ================== >> Algorithm task: multiclass classification. Parallel processing with 16 cores. Applying data cleaning... Dataset stats ================= >> Shape: (178, 14) Scaled: False ---------------------------------- Size of training set: 143 Size of test set: 35 ---------------------------------- Class balance: 0:1:2 <==> 1.2:1.5:1.0 Instances in target per class: | | total | train_set | test_set | |---:|---------:|-------------:|------------:| | 0 | 59 | 50 | 9 | | 1 | 71 | 58 | 13 | | 2 | 48 | 35 | 13 | Running pipeline ============================= >> Models in pipeline: LR, LDA, RF Metric: roc_auc_ovr Running BO for Logistic Regression... Random start 1 ---------------------------------- Parameters --> {'max_iter': 335, 'solver': 'sag', 'class_weight': None, 'penalty': 'l2', 'C': 0.001} Evaluation --> roc_auc_ovr: 0.9970 Best roc_auc_ovr: 0.9970 Time iteration: 3.490s Total time: 3.494s Random start 2 ---------------------------------- Parameters --> {'max_iter': 683, 'solver': 'lbfgs', 'class_weight': 'balanced', 'penalty': 'l2', 'C': 0.096} Evaluation --> roc_auc_ovr: 0.9996 Best roc_auc_ovr: 0.9996 Time iteration: 3.023s Total time: 6.523s Random start 3 ---------------------------------- Parameters --> {'max_iter': 352, 'solver': 'saga', 'class_weight': 'balanced', 'penalty': 'l2', 'C': 0.001} Evaluation --> roc_auc_ovr: 0.9950 Best roc_auc_ovr: 0.9996 Time iteration: 3.125s Total time: 9.652s Iteration 4 ------------------------------------- Parameters --> {'max_iter': 603, 'solver': 'liblinear', 'class_weight': None, 'penalty': 'l2', 'C': 0.061} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 2.405s Total time: 12.267s Results for Logistic Regression: Bayesian Optimization --------------------------- Best parameters --> {'max_iter': 603, 'solver': 'liblinear', 'class_weight': None, 'penalty': 'l2', 'C': 0.061} Best evaluation --> roc_auc_ovr: 1.0000 Time elapsed: 12.464s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 1.0000 Score on the test set --> roc_auc_ovr: 0.9977 Time elapsed: 0.018s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9981 \u00b1 0.0009 Time elapsed: 0.046s ------------------------------------------------- Total time: 12.535s Running BO for Linear Discriminant Analysis... Random start 1 ---------------------------------- Parameters --> {'solver': 'eigen', 'shrinkage': 1.0} Evaluation --> roc_auc_ovr: 0.8975 Best roc_auc_ovr: 0.8975 Time iteration: 0.033s Total time: 0.034s Random start 2 ---------------------------------- Parameters --> {'solver': 'svd'} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 0.033s Total time: 0.070s Random start 3 ---------------------------------- Parameters --> {'solver': 'svd'} Evaluation --> roc_auc_ovr: 1.0000 Best roc_auc_ovr: 1.0000 Time iteration: 0.032s Total time: 0.106s Iteration 4 ------------------------------------- Parameters --> {'solver': 'lsqr', 'shrinkage': 0.7} Evaluation --> roc_auc_ovr: 0.8996 Best roc_auc_ovr: 1.0000 Time iteration: 0.033s Total time: 0.297s Results for Linear Discriminant Analysis: Bayesian Optimization --------------------------- Best parameters --> {'solver': 'svd'} Best evaluation --> roc_auc_ovr: 1.0000 Time elapsed: 0.531s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 1.0000 Score on the test set --> roc_auc_ovr: 1.0000 Time elapsed: 0.012s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9998 \u00b1 0.0005 Time elapsed: 0.034s ------------------------------------------------- Total time: 0.583s Running BO for Random Forest... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 245, 'max_depth': 7, 'max_features': 1.0, 'criterion': 'gini', 'min_samples_split': 7, 'min_samples_leaf': 16, 'ccp_alpha': 0.008, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> roc_auc_ovr: 0.9853 Best roc_auc_ovr: 0.9853 Time iteration: 0.559s Total time: 0.564s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 400, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'gini', 'min_samples_split': 20, 'min_samples_leaf': 12, 'ccp_alpha': 0.016, 'bootstrap': True, 'max_samples': 0.7} Evaluation --> roc_auc_ovr: 0.9937 Best roc_auc_ovr: 0.9937 Time iteration: 0.671s Total time: 1.239s Random start 3 ---------------------------------- Parameters --> {'n_estimators': 78, 'max_depth': 10, 'max_features': 0.7, 'criterion': 'gini', 'min_samples_split': 2, 'min_samples_leaf': 14, 'ccp_alpha': 0.025, 'bootstrap': False} Evaluation --> roc_auc_ovr: 0.9865 Best roc_auc_ovr: 0.9937 Time iteration: 0.271s Total time: 1.515s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 323, 'max_depth': 7, 'max_features': 1.0, 'criterion': 'gini', 'min_samples_split': 16, 'min_samples_leaf': 1, 'ccp_alpha': 0.007, 'bootstrap': False} Evaluation --> roc_auc_ovr: 0.9315 Best roc_auc_ovr: 0.9937 Time iteration: 0.545s Total time: 2.257s Results for Random Forest: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 400, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'gini', 'min_samples_split': 20, 'min_samples_leaf': 12, 'ccp_alpha': 0.016, 'bootstrap': True, 'max_samples': 0.7} Best evaluation --> roc_auc_ovr: 0.9937 Time elapsed: 2.452s Fitting ----------------------------------------- Score on the train set --> roc_auc_ovr: 0.9997 Score on the test set --> roc_auc_ovr: 0.9825 Time elapsed: 0.753s Bagging ----------------------------------------- Score --> roc_auc_ovr: 0.9737 \u00b1 0.0116 Time elapsed: 3.200s ------------------------------------------------- Total time: 6.411s Final results ========================= >> Duration: 19.530s ------------------------------------------ Logistic Regression --> roc_auc_ovr: 0.998 \u00b1 0.001 Linear Discriminant Analysis --> roc_auc_ovr: 1.000 \u00b1 0.000 ! Random Forest --> roc_auc_ovr: 0.974 \u00b1 0.012","title":"Run the pipeline"},{"location":"examples/multiclass_classification/multiclass_classification/#analyze-the-results","text":"# We can access the pipeline's results via the results attribute atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_bo time_bo score_train score_test time_fit mean_bagging std_bagging time_bagging time model LR Logistic Regression 1.000000 12.464s 1.000000 0.997669 0.018s 0.998135 0.000932 0.046s 12.535s LDA Linear Discriminant Analysis 1.000000 0.531s 1.000000 1.000000 0.012s 0.999767 0.000466 0.034s 0.583s RF Random Forest 0.993712 2.452s 0.999725 0.982517 0.753s 0.973686 0.011577 3.200s 6.411s # Show the scoring for a different metric than the one we trained on atom.scoring('precision_macro') Results ===================== >> Logistic Regression --> precision_macro: 0.976 Linear Discriminant Analysis --> precision_macro: 0.976 Random Forest --> precision_macro: 0.9 Let's have a closer look at the Random Forest # Get the results on some other metrics print('Jaccard score:', atom.rf.scoring('jaccard_weighted')) print('Recall score:', atom.rf.scoring('recall_macro')) Jaccard score: 0.7957142857142857 Recall score: 0.8974358974358975 # Plot the confusion matrix atom.RF.plot_confusion_matrix(figsize=(9, 9)) # Save the model as a pickle file atom.RF.save_model('Random_Forest_model') Random Forest model saved successfully!","title":"Analyze the results"},{"location":"examples/regression/regression/","text":"Regression This example shows how to use ATOM to apply PCA on the data and run a regression pipeline. Download the abalone dataset from https://archive.ics.uci.edu/ml/datasets/Abalone . The goal of this dataset is to predict the rings (age) of abalone shells from physical measurements. Load the data # Import packages import pandas as pd from atom import ATOMRegressor # Load the abalone dataset X = pd.read_csv('./datasets/abalone.csv') # Let's have a look at the data X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sex Length Diameter Height Whole weight Shucked weight Viscera weight Shell weight Rings 0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15 1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7 2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9 3 M 0.440 0.365 0.125 0.5160 0.2155 0.1140 0.155 10 4 I 0.330 0.255 0.080 0.2050 0.0895 0.0395 0.055 7 # Initialize ATOM for regression tasks and encode the categorical features atom = ATOMRegressor(X, y=\"Rings\", verbose=2, random_state=42) atom.encode() << ================== ATOM ================== >> Algorithm task: regression. Applying data cleaning... Dataset stats ================= >> Shape: (4177, 9) Categorical columns: 1 Scaled: False ---------------------------------- Size of training set: 3342 Size of test set: 835 Fitting Encoder... Encoding categorical columns... --> One-hot-encoding feature Sex. Contains 3 unique categories. # Plot the dataset's correlation matrix atom.plot_correlation() # Apply PCA for dimensionality reduction atom.feature_selection(strategy=\"pca\", n_features=6) Fitting FeatureSelector... Performing feature selection ... --> Feature Diameter was removed due to collinearity with another feature. --> Applying Principal Component Analysis... >>> Scaling features... >>> Total explained variance: 0.977 # Use the plotting methods to see the retained variance ratio atom.plot_pca() atom.plot_components(figsize=(8, 6), filename='atom_PCA_plot') Run the pipeline atom.run(['Tree', 'Bag', 'ET'], metric='MSE', n_calls=5, n_random_starts=2, bo_params={'base_estimator': 'GBRT', 'cv': 1}, bagging=5) Running pipeline ============================= >> Models in pipeline: Tree, Bag, ET Metric: neg_mean_squared_error Running BO for Decision Tree... Random start 1 ---------------------------------- Parameters --> {'criterion': 'mae', 'splitter': 'random', 'max_depth': 5, 'max_features': 0.9, 'min_samples_split': 8, 'min_samples_leaf': 19, 'ccp_alpha': 0.003} Evaluation --> neg_mean_squared_error: -8.2257 Best neg_mean_squared_error: -8.2257 Time iteration: 0.038s Total time: 0.042s Random start 2 ---------------------------------- Parameters --> {'criterion': 'mae', 'splitter': 'best', 'max_depth': 10, 'max_features': 0.9, 'min_samples_split': 3, 'min_samples_leaf': 12, 'ccp_alpha': 0.033} Evaluation --> neg_mean_squared_error: -9.0433 Best neg_mean_squared_error: -8.2257 Time iteration: 0.197s Total time: 0.243s Iteration 3 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 7, 'max_features': 0.6, 'min_samples_split': 17, 'min_samples_leaf': 19, 'ccp_alpha': 0.015} Evaluation --> neg_mean_squared_error: -6.3817 Best neg_mean_squared_error: -6.3817 Time iteration: 0.006s Total time: 0.342s Iteration 4 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'best', 'max_depth': 9, 'max_features': 0.6, 'min_samples_split': 19, 'min_samples_leaf': 13, 'ccp_alpha': 0.013} Evaluation --> neg_mean_squared_error: -6.5175 Best neg_mean_squared_error: -6.3817 Time iteration: 0.012s Total time: 0.448s Iteration 5 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 10, 'max_features': 0.6, 'min_samples_split': 16, 'min_samples_leaf': 4, 'ccp_alpha': 0.011} Evaluation --> neg_mean_squared_error: -10.0056 Best neg_mean_squared_error: -6.3817 Time iteration: 0.007s Total time: 0.551s Results for Decision Tree: Bayesian Optimization --------------------------- Best parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 7, 'max_features': 0.6, 'min_samples_split': 17, 'min_samples_leaf': 19, 'ccp_alpha': 0.015} Best evaluation --> neg_mean_squared_error: -6.3817 Time elapsed: 0.645s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -8.3743 Score on the test set --> neg_mean_squared_error: -6.9551 Time elapsed: 0.007s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -6.6744 \u00b1 0.9527 Time elapsed: 0.020s ------------------------------------------------- Total time: 0.678s Running BO for Bagging Regressor... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 112, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -5.8182 Best neg_mean_squared_error: -5.8182 Time iteration: 0.873s Total time: 0.876s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 131, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -6.7970 Best neg_mean_squared_error: -5.8182 Time iteration: 0.589s Total time: 1.467s Iteration 3 ------------------------------------- Parameters --> {'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': True} Evaluation --> neg_mean_squared_error: -5.4292 Best neg_mean_squared_error: -5.4292 Time iteration: 0.383s Total time: 2.015s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 74, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False, 'bootstrap_features': True} Evaluation --> neg_mean_squared_error: -5.9847 Best neg_mean_squared_error: -5.4292 Time iteration: 0.327s Total time: 2.434s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 18, 'max_samples': 0.8, 'max_features': 0.6, 'bootstrap': True, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -6.1556 Best neg_mean_squared_error: -5.4292 Time iteration: 0.092s Total time: 2.613s Results for Bagging Regressor: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': True} Best evaluation --> neg_mean_squared_error: -5.4292 Time elapsed: 2.708s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -0.0861 Score on the test set --> neg_mean_squared_error: -4.9042 Time elapsed: 0.514s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -4.9562 \u00b1 0.1064 Time elapsed: 2.145s ------------------------------------------------- Total time: 5.374s Running BO for Extra-Trees... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 112, 'max_depth': 6, 'max_features': 1.0, 'criterion': 'mae', 'min_samples_split': 8, 'min_samples_leaf': 19, 'ccp_alpha': 0.003, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -6.7733 Best neg_mean_squared_error: -6.7733 Time iteration: 0.995s Total time: 1.000s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 369, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 13, 'min_samples_leaf': 6, 'ccp_alpha': 0.0, 'bootstrap': False} Evaluation --> neg_mean_squared_error: -6.6959 Best neg_mean_squared_error: -6.6959 Time iteration: 0.475s Total time: 1.479s Iteration 3 ------------------------------------- Parameters --> {'n_estimators': 481, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 7, 'min_samples_leaf': 2, 'ccp_alpha': 0.001, 'bootstrap': False} Evaluation --> neg_mean_squared_error: -4.8752 Best neg_mean_squared_error: -4.8752 Time iteration: 0.726s Total time: 2.310s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 460, 'max_depth': 5, 'max_features': 1.0, 'criterion': 'mae', 'min_samples_split': 5, 'min_samples_leaf': 4, 'ccp_alpha': 0.034, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -7.0711 Best neg_mean_squared_error: -4.8752 Time iteration: 4.778s Total time: 7.196s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 474, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'mae', 'min_samples_split': 20, 'min_samples_leaf': 1, 'ccp_alpha': 0.018, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -7.2239 Best neg_mean_squared_error: -4.8752 Time iteration: 3.961s Total time: 11.260s Results for Extra-Trees: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 481, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 7, 'min_samples_leaf': 2, 'ccp_alpha': 0.001, 'bootstrap': False} Best evaluation --> neg_mean_squared_error: -4.8752 Time elapsed: 11.359s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -4.4007 Score on the test set --> neg_mean_squared_error: -4.1469 Time elapsed: 0.924s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -4.2367 \u00b1 0.0422 Time elapsed: 4.064s ------------------------------------------------- Total time: 16.352s Final results ========================= >> Duration: 22.406s ------------------------------------------ Decision Tree --> neg_mean_squared_error: -6.674 \u00b1 0.953 ~ Bagging Regressor --> neg_mean_squared_error: -4.956 \u00b1 0.106 ~ Extra-Trees --> neg_mean_squared_error: -4.237 \u00b1 0.042 ~ !","title":"Regression"},{"location":"examples/regression/regression/#regression","text":"This example shows how to use ATOM to apply PCA on the data and run a regression pipeline. Download the abalone dataset from https://archive.ics.uci.edu/ml/datasets/Abalone . The goal of this dataset is to predict the rings (age) of abalone shells from physical measurements.","title":"Regression"},{"location":"examples/regression/regression/#load-the-data","text":"# Import packages import pandas as pd from atom import ATOMRegressor # Load the abalone dataset X = pd.read_csv('./datasets/abalone.csv') # Let's have a look at the data X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sex Length Diameter Height Whole weight Shucked weight Viscera weight Shell weight Rings 0 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 0.150 15 1 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 0.070 7 2 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 0.210 9 3 M 0.440 0.365 0.125 0.5160 0.2155 0.1140 0.155 10 4 I 0.330 0.255 0.080 0.2050 0.0895 0.0395 0.055 7 # Initialize ATOM for regression tasks and encode the categorical features atom = ATOMRegressor(X, y=\"Rings\", verbose=2, random_state=42) atom.encode() << ================== ATOM ================== >> Algorithm task: regression. Applying data cleaning... Dataset stats ================= >> Shape: (4177, 9) Categorical columns: 1 Scaled: False ---------------------------------- Size of training set: 3342 Size of test set: 835 Fitting Encoder... Encoding categorical columns... --> One-hot-encoding feature Sex. Contains 3 unique categories. # Plot the dataset's correlation matrix atom.plot_correlation() # Apply PCA for dimensionality reduction atom.feature_selection(strategy=\"pca\", n_features=6) Fitting FeatureSelector... Performing feature selection ... --> Feature Diameter was removed due to collinearity with another feature. --> Applying Principal Component Analysis... >>> Scaling features... >>> Total explained variance: 0.977 # Use the plotting methods to see the retained variance ratio atom.plot_pca() atom.plot_components(figsize=(8, 6), filename='atom_PCA_plot')","title":"Load the data"},{"location":"examples/regression/regression/#run-the-pipeline","text":"atom.run(['Tree', 'Bag', 'ET'], metric='MSE', n_calls=5, n_random_starts=2, bo_params={'base_estimator': 'GBRT', 'cv': 1}, bagging=5) Running pipeline ============================= >> Models in pipeline: Tree, Bag, ET Metric: neg_mean_squared_error Running BO for Decision Tree... Random start 1 ---------------------------------- Parameters --> {'criterion': 'mae', 'splitter': 'random', 'max_depth': 5, 'max_features': 0.9, 'min_samples_split': 8, 'min_samples_leaf': 19, 'ccp_alpha': 0.003} Evaluation --> neg_mean_squared_error: -8.2257 Best neg_mean_squared_error: -8.2257 Time iteration: 0.038s Total time: 0.042s Random start 2 ---------------------------------- Parameters --> {'criterion': 'mae', 'splitter': 'best', 'max_depth': 10, 'max_features': 0.9, 'min_samples_split': 3, 'min_samples_leaf': 12, 'ccp_alpha': 0.033} Evaluation --> neg_mean_squared_error: -9.0433 Best neg_mean_squared_error: -8.2257 Time iteration: 0.197s Total time: 0.243s Iteration 3 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 7, 'max_features': 0.6, 'min_samples_split': 17, 'min_samples_leaf': 19, 'ccp_alpha': 0.015} Evaluation --> neg_mean_squared_error: -6.3817 Best neg_mean_squared_error: -6.3817 Time iteration: 0.006s Total time: 0.342s Iteration 4 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'best', 'max_depth': 9, 'max_features': 0.6, 'min_samples_split': 19, 'min_samples_leaf': 13, 'ccp_alpha': 0.013} Evaluation --> neg_mean_squared_error: -6.5175 Best neg_mean_squared_error: -6.3817 Time iteration: 0.012s Total time: 0.448s Iteration 5 ------------------------------------- Parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 10, 'max_features': 0.6, 'min_samples_split': 16, 'min_samples_leaf': 4, 'ccp_alpha': 0.011} Evaluation --> neg_mean_squared_error: -10.0056 Best neg_mean_squared_error: -6.3817 Time iteration: 0.007s Total time: 0.551s Results for Decision Tree: Bayesian Optimization --------------------------- Best parameters --> {'criterion': 'friedman_mse', 'splitter': 'random', 'max_depth': 7, 'max_features': 0.6, 'min_samples_split': 17, 'min_samples_leaf': 19, 'ccp_alpha': 0.015} Best evaluation --> neg_mean_squared_error: -6.3817 Time elapsed: 0.645s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -8.3743 Score on the test set --> neg_mean_squared_error: -6.9551 Time elapsed: 0.007s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -6.6744 \u00b1 0.9527 Time elapsed: 0.020s ------------------------------------------------- Total time: 0.678s Running BO for Bagging Regressor... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 112, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -5.8182 Best neg_mean_squared_error: -5.8182 Time iteration: 0.873s Total time: 0.876s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 131, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -6.7970 Best neg_mean_squared_error: -5.8182 Time iteration: 0.589s Total time: 1.467s Iteration 3 ------------------------------------- Parameters --> {'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': True} Evaluation --> neg_mean_squared_error: -5.4292 Best neg_mean_squared_error: -5.4292 Time iteration: 0.383s Total time: 2.015s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 74, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False, 'bootstrap_features': True} Evaluation --> neg_mean_squared_error: -5.9847 Best neg_mean_squared_error: -5.4292 Time iteration: 0.327s Total time: 2.434s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 18, 'max_samples': 0.8, 'max_features': 0.6, 'bootstrap': True, 'bootstrap_features': False} Evaluation --> neg_mean_squared_error: -6.1556 Best neg_mean_squared_error: -5.4292 Time iteration: 0.092s Total time: 2.613s Results for Bagging Regressor: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.6, 'bootstrap': False, 'bootstrap_features': True} Best evaluation --> neg_mean_squared_error: -5.4292 Time elapsed: 2.708s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -0.0861 Score on the test set --> neg_mean_squared_error: -4.9042 Time elapsed: 0.514s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -4.9562 \u00b1 0.1064 Time elapsed: 2.145s ------------------------------------------------- Total time: 5.374s Running BO for Extra-Trees... Random start 1 ---------------------------------- Parameters --> {'n_estimators': 112, 'max_depth': 6, 'max_features': 1.0, 'criterion': 'mae', 'min_samples_split': 8, 'min_samples_leaf': 19, 'ccp_alpha': 0.003, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -6.7733 Best neg_mean_squared_error: -6.7733 Time iteration: 0.995s Total time: 1.000s Random start 2 ---------------------------------- Parameters --> {'n_estimators': 369, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 13, 'min_samples_leaf': 6, 'ccp_alpha': 0.0, 'bootstrap': False} Evaluation --> neg_mean_squared_error: -6.6959 Best neg_mean_squared_error: -6.6959 Time iteration: 0.475s Total time: 1.479s Iteration 3 ------------------------------------- Parameters --> {'n_estimators': 481, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 7, 'min_samples_leaf': 2, 'ccp_alpha': 0.001, 'bootstrap': False} Evaluation --> neg_mean_squared_error: -4.8752 Best neg_mean_squared_error: -4.8752 Time iteration: 0.726s Total time: 2.310s Iteration 4 ------------------------------------- Parameters --> {'n_estimators': 460, 'max_depth': 5, 'max_features': 1.0, 'criterion': 'mae', 'min_samples_split': 5, 'min_samples_leaf': 4, 'ccp_alpha': 0.034, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -7.0711 Best neg_mean_squared_error: -4.8752 Time iteration: 4.778s Total time: 7.196s Iteration 5 ------------------------------------- Parameters --> {'n_estimators': 474, 'max_depth': 4, 'max_features': 0.8, 'criterion': 'mae', 'min_samples_split': 20, 'min_samples_leaf': 1, 'ccp_alpha': 0.018, 'bootstrap': True, 'max_samples': 0.6} Evaluation --> neg_mean_squared_error: -7.2239 Best neg_mean_squared_error: -4.8752 Time iteration: 3.961s Total time: 11.260s Results for Extra-Trees: Bayesian Optimization --------------------------- Best parameters --> {'n_estimators': 481, 'max_depth': 10, 'max_features': 0.8, 'criterion': 'mse', 'min_samples_split': 7, 'min_samples_leaf': 2, 'ccp_alpha': 0.001, 'bootstrap': False} Best evaluation --> neg_mean_squared_error: -4.8752 Time elapsed: 11.359s Fitting ----------------------------------------- Score on the train set --> neg_mean_squared_error: -4.4007 Score on the test set --> neg_mean_squared_error: -4.1469 Time elapsed: 0.924s Bagging ----------------------------------------- Score --> neg_mean_squared_error: -4.2367 \u00b1 0.0422 Time elapsed: 4.064s ------------------------------------------------- Total time: 16.352s Final results ========================= >> Duration: 22.406s ------------------------------------------ Decision Tree --> neg_mean_squared_error: -6.674 \u00b1 0.953 ~ Bagging Regressor --> neg_mean_squared_error: -4.956 \u00b1 0.106 ~ Extra-Trees --> neg_mean_squared_error: -4.237 \u00b1 0.042 ~ !","title":"Run the pipeline"},{"location":"examples/successive_halving/successive_halving/","text":"Successive halving This example shows how to compare multiple tree-based models using successive halving. Import the boston dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict house prices. Load the data # Import packages import numpy as np import pandas as pd from sklearn.datasets import load_boston from atom import ATOMRegressor # Load the dataset's features and targets X, y = load_boston(return_X_y=True) Run the pipeline atom = ATOMRegressor(X, y, verbose=1, random_state=1) << ================== ATOM ================== >> Algorithm task: regression. Applying data cleaning... Dataset stats ================= >> Shape: (506, 14) Scaled: False ---------------------------------- Size of training set: 405 Size of test set: 101 # We can compare tree-based models via successive halving atom.successive_halving(['tree', 'bag', 'et', 'rf', 'lgb', 'catb'], metric='mae', bagging=5) Running pipeline ============================= >> Metric: neg_mean_absolute_error Run 0 (17% of set) ============================>> Models in pipeline: Tree, Bag, ET, RF, LGB, CatB Size of training set: 67 Size of test set: 101 Results for Decision Tree: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -3.3257 Time elapsed: 0.007s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -4.3307 \u00b1 0.5250 Time elapsed: 0.020s ------------------------------------------------- Total time: 0.030s Results for Bagging Regressor: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -1.3054 Score on the test set --> neg_mean_absolute_error: -2.6950 Time elapsed: 0.020s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -3.0957 \u00b1 0.2677 Time elapsed: 0.079s ------------------------------------------------- Total time: 0.103s Results for Extra-Trees: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -2.1541 Time elapsed: 0.088s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.5554 \u00b1 0.1708 Time elapsed: 0.364s ------------------------------------------------- Total time: 0.456s Results for Random Forest: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -1.1509 Score on the test set --> neg_mean_absolute_error: -2.4143 Time elapsed: 0.111s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.9574 \u00b1 0.2253 Time elapsed: 0.509s ------------------------------------------------- Total time: 0.625s Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -3.4205 Score on the test set --> neg_mean_absolute_error: -4.5600 Time elapsed: 0.024s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -4.8393 \u00b1 0.2682 Time elapsed: 0.068s ------------------------------------------------- Total time: 0.097s Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0806 Score on the test set --> neg_mean_absolute_error: -2.3984 Time elapsed: 0.760s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.9165 \u00b1 0.2564 Time elapsed: 3.112s ------------------------------------------------- Total time: 3.875s Final results ========================= >> Duration: 5.188s ------------------------------------------ Decision Tree --> neg_mean_absolute_error: -4.331 \u00b1 0.525 ~ Bagging Regressor --> neg_mean_absolute_error: -3.096 \u00b1 0.268 ~ Extra-Trees --> neg_mean_absolute_error: -2.555 \u00b1 0.171 ~ ! Random Forest --> neg_mean_absolute_error: -2.957 \u00b1 0.225 ~ LightGBM --> neg_mean_absolute_error: -4.839 \u00b1 0.268 ~ CatBoost --> neg_mean_absolute_error: -2.916 \u00b1 0.256 ~ Run 1 (33% of set) ============================>> Models in pipeline: ET, CatB, RF Size of training set: 135 Size of test set: 101 Results for Extra-Trees: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -2.2361 Time elapsed: 0.099s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.6016 \u00b1 0.2890 Time elapsed: 0.418s ------------------------------------------------- Total time: 0.520s Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.2835 Score on the test set --> neg_mean_absolute_error: -2.4196 Time elapsed: 0.873s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.5681 \u00b1 0.2119 Time elapsed: 3.423s ------------------------------------------------- Total time: 4.300s Results for Random Forest: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.9820 Score on the test set --> neg_mean_absolute_error: -2.5055 Time elapsed: 0.132s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.6144 \u00b1 0.1188 Time elapsed: 0.603s ------------------------------------------------- Total time: 0.739s Final results ========================= >> Duration: 5.561s ------------------------------------------ Extra-Trees --> neg_mean_absolute_error: -2.602 \u00b1 0.289 ~ CatBoost --> neg_mean_absolute_error: -2.568 \u00b1 0.212 ~ ! Random Forest --> neg_mean_absolute_error: -2.614 \u00b1 0.119 ~ Run 2 (100% of set) ===========================>> Models in pipeline: CatB Size of training set: 405 Size of test set: 101 Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.3978 Score on the test set --> neg_mean_absolute_error: -1.8772 Time elapsed: 1.252s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.0501 \u00b1 0.0892 Time elapsed: 5.481s ------------------------------------------------- Total time: 6.737s Final results ========================= >> Duration: 6.740s ------------------------------------------ CatBoost --> neg_mean_absolute_error: -2.050 \u00b1 0.089 ~ Analyze results # Note that the results dataframe now is multi-index atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_train score_test time_fit mean_bagging std_bagging time_bagging time run model 0 Tree Decision Tree -0.000000e+00 -3.325743 0.007s -4.330693 0.525026 0.020s 0.030s Bag Bagging Regressor -1.305373e+00 -2.695050 0.020s -3.095663 0.267668 0.079s 0.103s ET Extra-Trees -2.256238e-14 -2.154089 0.088s -2.555434 0.170823 0.364s 0.456s RF Random Forest -1.150866e+00 -2.414297 0.111s -2.957400 0.225311 0.509s 0.625s LGB LightGBM -3.420518e+00 -4.559962 0.024s -4.839315 0.268167 0.068s 0.097s CatB CatBoost -8.055503e-02 -2.398431 0.760s -2.916470 0.256428 3.112s 3.875s 1 ET Extra-Trees -2.315185e-14 -2.236079 0.099s -2.601648 0.289034 0.418s 0.520s CatB CatBoost -2.835499e-01 -2.419625 0.873s -2.568085 0.211868 3.423s 4.300s RF Random Forest -9.819778e-01 -2.505465 0.132s -2.614416 0.118758 0.603s 0.739s 2 CatB CatBoost -3.977985e-01 -1.877205 1.252s -2.050118 0.089185 5.481s 6.737s # Plot the successive halving's results atom.plot_successive_halving()","title":"Successive halving"},{"location":"examples/successive_halving/successive_halving/#successive-halving","text":"This example shows how to compare multiple tree-based models using successive halving. Import the boston dataset from sklearn.datasets . This is a small and easy to train dataset whose goal is to predict house prices.","title":"Successive halving"},{"location":"examples/successive_halving/successive_halving/#load-the-data","text":"# Import packages import numpy as np import pandas as pd from sklearn.datasets import load_boston from atom import ATOMRegressor # Load the dataset's features and targets X, y = load_boston(return_X_y=True)","title":"Load the data"},{"location":"examples/successive_halving/successive_halving/#run-the-pipeline","text":"atom = ATOMRegressor(X, y, verbose=1, random_state=1) << ================== ATOM ================== >> Algorithm task: regression. Applying data cleaning... Dataset stats ================= >> Shape: (506, 14) Scaled: False ---------------------------------- Size of training set: 405 Size of test set: 101 # We can compare tree-based models via successive halving atom.successive_halving(['tree', 'bag', 'et', 'rf', 'lgb', 'catb'], metric='mae', bagging=5) Running pipeline ============================= >> Metric: neg_mean_absolute_error Run 0 (17% of set) ============================>> Models in pipeline: Tree, Bag, ET, RF, LGB, CatB Size of training set: 67 Size of test set: 101 Results for Decision Tree: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -3.3257 Time elapsed: 0.007s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -4.3307 \u00b1 0.5250 Time elapsed: 0.020s ------------------------------------------------- Total time: 0.030s Results for Bagging Regressor: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -1.3054 Score on the test set --> neg_mean_absolute_error: -2.6950 Time elapsed: 0.020s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -3.0957 \u00b1 0.2677 Time elapsed: 0.079s ------------------------------------------------- Total time: 0.103s Results for Extra-Trees: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -2.1541 Time elapsed: 0.088s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.5554 \u00b1 0.1708 Time elapsed: 0.364s ------------------------------------------------- Total time: 0.456s Results for Random Forest: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -1.1509 Score on the test set --> neg_mean_absolute_error: -2.4143 Time elapsed: 0.111s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.9574 \u00b1 0.2253 Time elapsed: 0.509s ------------------------------------------------- Total time: 0.625s Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -3.4205 Score on the test set --> neg_mean_absolute_error: -4.5600 Time elapsed: 0.024s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -4.8393 \u00b1 0.2682 Time elapsed: 0.068s ------------------------------------------------- Total time: 0.097s Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0806 Score on the test set --> neg_mean_absolute_error: -2.3984 Time elapsed: 0.760s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.9165 \u00b1 0.2564 Time elapsed: 3.112s ------------------------------------------------- Total time: 3.875s Final results ========================= >> Duration: 5.188s ------------------------------------------ Decision Tree --> neg_mean_absolute_error: -4.331 \u00b1 0.525 ~ Bagging Regressor --> neg_mean_absolute_error: -3.096 \u00b1 0.268 ~ Extra-Trees --> neg_mean_absolute_error: -2.555 \u00b1 0.171 ~ ! Random Forest --> neg_mean_absolute_error: -2.957 \u00b1 0.225 ~ LightGBM --> neg_mean_absolute_error: -4.839 \u00b1 0.268 ~ CatBoost --> neg_mean_absolute_error: -2.916 \u00b1 0.256 ~ Run 1 (33% of set) ============================>> Models in pipeline: ET, CatB, RF Size of training set: 135 Size of test set: 101 Results for Extra-Trees: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.0000 Score on the test set --> neg_mean_absolute_error: -2.2361 Time elapsed: 0.099s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.6016 \u00b1 0.2890 Time elapsed: 0.418s ------------------------------------------------- Total time: 0.520s Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.2835 Score on the test set --> neg_mean_absolute_error: -2.4196 Time elapsed: 0.873s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.5681 \u00b1 0.2119 Time elapsed: 3.423s ------------------------------------------------- Total time: 4.300s Results for Random Forest: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.9820 Score on the test set --> neg_mean_absolute_error: -2.5055 Time elapsed: 0.132s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.6144 \u00b1 0.1188 Time elapsed: 0.603s ------------------------------------------------- Total time: 0.739s Final results ========================= >> Duration: 5.561s ------------------------------------------ Extra-Trees --> neg_mean_absolute_error: -2.602 \u00b1 0.289 ~ CatBoost --> neg_mean_absolute_error: -2.568 \u00b1 0.212 ~ ! Random Forest --> neg_mean_absolute_error: -2.614 \u00b1 0.119 ~ Run 2 (100% of set) ===========================>> Models in pipeline: CatB Size of training set: 405 Size of test set: 101 Results for CatBoost: Fitting ----------------------------------------- Score on the train set --> neg_mean_absolute_error: -0.3978 Score on the test set --> neg_mean_absolute_error: -1.8772 Time elapsed: 1.252s Bagging ----------------------------------------- Score --> neg_mean_absolute_error: -2.0501 \u00b1 0.0892 Time elapsed: 5.481s ------------------------------------------------- Total time: 6.737s Final results ========================= >> Duration: 6.740s ------------------------------------------ CatBoost --> neg_mean_absolute_error: -2.050 \u00b1 0.089 ~","title":"Run the pipeline"},{"location":"examples/successive_halving/successive_halving/#analyze-results","text":"# Note that the results dataframe now is multi-index atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_train score_test time_fit mean_bagging std_bagging time_bagging time run model 0 Tree Decision Tree -0.000000e+00 -3.325743 0.007s -4.330693 0.525026 0.020s 0.030s Bag Bagging Regressor -1.305373e+00 -2.695050 0.020s -3.095663 0.267668 0.079s 0.103s ET Extra-Trees -2.256238e-14 -2.154089 0.088s -2.555434 0.170823 0.364s 0.456s RF Random Forest -1.150866e+00 -2.414297 0.111s -2.957400 0.225311 0.509s 0.625s LGB LightGBM -3.420518e+00 -4.559962 0.024s -4.839315 0.268167 0.068s 0.097s CatB CatBoost -8.055503e-02 -2.398431 0.760s -2.916470 0.256428 3.112s 3.875s 1 ET Extra-Trees -2.315185e-14 -2.236079 0.099s -2.601648 0.289034 0.418s 0.520s CatB CatBoost -2.835499e-01 -2.419625 0.873s -2.568085 0.211868 3.423s 4.300s RF Random Forest -9.819778e-01 -2.505465 0.132s -2.614416 0.118758 0.603s 0.739s 2 CatB CatBoost -3.977985e-01 -1.877205 1.252s -2.050118 0.089185 5.481s 6.737s # Plot the successive halving's results atom.plot_successive_halving()","title":"Analyze results"},{"location":"examples/train_sizing/train_sizing/","text":"Train sizing This example shows how to asses a model's performance based on the size of the training set. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package. The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow. Load the data # Import packages import numpy as np import pandas as pd from atom import ATOMClassifier # Load the Australian weather dataset X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 3118 BadgerysCreek 11.7 23.2 0.0 NaN NaN SW 28.0 18965 NorahHead 10.2 19.4 0.0 NaN NaN SSE 30.0 11196 CoffsHarbour 9.7 21.2 0.0 NaN NaN NW 26.0 62283 Sale 8.4 21.7 0.0 NaN NaN WSW 41.0 92461 Townsville 11.1 27.1 0.0 7.6 10.7 ENE 37.0 Run the pipeline # Initialize ATOM and prepare the data atom = ATOMClassifier(X, verbose=2, random_state=1) atom.impute(strat_num='median', strat_cat='most_frequent', min_frac_rows=0.8) atom.encode() << ================== ATOM ================== >> Algorithm task: binary classification. Applying data cleaning... Dataset stats ================= >> Shape: (142193, 22) Missing values: 292032 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 113755 Size of test set: 28438 ---------------------------------- Class balance: No:Yes <==> 3.5:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 110316 | 88263 | 22053 | | 1: Yes | 31877 | 25492 | 6385 | Fitting Imputer... Imputing missing values... --> Dropping 15182 rows for containing less than 80% non-missing values. --> Imputing 100 missing values with median in feature MinTemp. --> Imputing 57 missing values with median in feature MaxTemp. --> Imputing 640 missing values with median in feature Rainfall. --> Imputing 46535 missing values with median in feature Evaporation. --> Imputing 53034 missing values with median in feature Sunshine. --> Imputing 4381 missing values with most_frequent in feature WindGustDir. --> Imputing 4359 missing values with median in feature WindGustSpeed. --> Imputing 6624 missing values with most_frequent in feature WindDir9am. --> Imputing 612 missing values with most_frequent in feature WindDir3pm. --> Imputing 80 missing values with median in feature WindSpeed9am. --> Imputing 49 missing values with median in feature WindSpeed3pm. --> Imputing 532 missing values with median in feature Humidity9am. --> Imputing 1168 missing values with median in feature Humidity3pm. --> Imputing 1028 missing values with median in feature Pressure9am. --> Imputing 972 missing values with median in feature Pressure3pm. --> Imputing 42172 missing values with median in feature Cloud9am. --> Imputing 44251 missing values with median in feature Cloud3pm. --> Imputing 98 missing values with median in feature Temp9am. --> Imputing 702 missing values with median in feature Temp3pm. --> Imputing 640 missing values with most_frequent in feature RainToday. Fitting Encoder... Encoding categorical columns... --> Target-encoding feature Location. Contains 45 unique categories. --> Target-encoding feature WindGustDir. Contains 16 unique categories. --> Target-encoding feature WindDir9am. Contains 16 unique categories. --> Target-encoding feature WindDir3pm. Contains 16 unique categories. --> Label-encoding feature RainToday. Contains 2 unique categories. # We can analyze the impact of the training set's size on a LightGBM model atom.train_sizing('lgb', train_sizes=np.linspace(0.1, 1, 9), bagging=4) Running pipeline ============================= >> Models in pipeline: LGB Metric: f1 Run 0 (10% of set) ============================>> Size of training set: 11375 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.8029 Score on the test set --> f1: 0.6086 Time elapsed: 0.998s Bagging ----------------------------------------- Score --> f1: 0.5945 \u00b1 0.0073 Time elapsed: 2.229s ------------------------------------------------- Total time: 3.242s Final results ========================= >> Duration: 3.244s ------------------------------------------ LightGBM --> f1: 0.594 \u00b1 0.007 ~ Run 1 (21% of set) ============================>> Size of training set: 24172 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.7292 Score on the test set --> f1: 0.6273 Time elapsed: 1.244s Bagging ----------------------------------------- Score --> f1: 0.6166 \u00b1 0.0053 Time elapsed: 2.879s ------------------------------------------------- Total time: 4.129s Final results ========================= >> Duration: 4.131s ------------------------------------------ LightGBM --> f1: 0.617 \u00b1 0.005 Run 2 (32% of set) ============================>> Size of training set: 36970 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6955 Score on the test set --> f1: 0.6325 Time elapsed: 1.533s Bagging ----------------------------------------- Score --> f1: 0.6199 \u00b1 0.0038 Time elapsed: 3.502s ------------------------------------------------- Total time: 5.039s Final results ========================= >> Duration: 5.042s ------------------------------------------ LightGBM --> f1: 0.620 \u00b1 0.004 Run 3 (44% of set) ============================>> Size of training set: 49767 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6832 Score on the test set --> f1: 0.6386 Time elapsed: 1.825s Bagging ----------------------------------------- Score --> f1: 0.6256 \u00b1 0.0036 Time elapsed: 4.148s ------------------------------------------------- Total time: 5.979s Final results ========================= >> Duration: 5.981s ------------------------------------------ LightGBM --> f1: 0.626 \u00b1 0.004 Run 4 (55% of set) ============================>> Size of training set: 62565 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6818 Score on the test set --> f1: 0.6391 Time elapsed: 2.152s Bagging ----------------------------------------- Score --> f1: 0.6271 \u00b1 0.0025 Time elapsed: 4.838s ------------------------------------------------- Total time: 6.996s Final results ========================= >> Duration: 6.998s ------------------------------------------ LightGBM --> f1: 0.627 \u00b1 0.002 Run 5 (66% of set) ============================>> Size of training set: 75362 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6767 Score on the test set --> f1: 0.6399 Time elapsed: 2.418s Bagging ----------------------------------------- Score --> f1: 0.6346 \u00b1 0.0021 Time elapsed: 5.622s ------------------------------------------------- Total time: 8.045s Final results ========================= >> Duration: 8.047s ------------------------------------------ LightGBM --> f1: 0.635 \u00b1 0.002 Run 6 (77% of set) ============================>> Size of training set: 88160 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6665 Score on the test set --> f1: 0.6384 Time elapsed: 2.810s Bagging ----------------------------------------- Score --> f1: 0.6342 \u00b1 0.0021 Time elapsed: 6.240s ------------------------------------------------- Total time: 9.058s Final results ========================= >> Duration: 9.060s ------------------------------------------ LightGBM --> f1: 0.634 \u00b1 0.002 Run 7 (89% of set) ============================>> Size of training set: 100957 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6651 Score on the test set --> f1: 0.6432 Time elapsed: 3.063s Bagging ----------------------------------------- Score --> f1: 0.6372 \u00b1 0.0025 Time elapsed: 6.888s ------------------------------------------------- Total time: 9.958s Final results ========================= >> Duration: 9.960s ------------------------------------------ LightGBM --> f1: 0.637 \u00b1 0.003 Run 8 (100% of set) ===========================>> Size of training set: 113755 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6650 Score on the test set --> f1: 0.6549 Time elapsed: 3.379s Bagging ----------------------------------------- Score --> f1: 0.6508 \u00b1 0.0026 Time elapsed: 7.621s ------------------------------------------------- Total time: 11.009s Final results ========================= >> Duration: 11.012s ------------------------------------------ LightGBM --> f1: 0.651 \u00b1 0.003 Analyze the results # Note that the results dataframe now is multi-index atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_train score_test time_fit mean_bagging std_bagging time_bagging time run model 0 LGB LightGBM 0.802859 0.608590 0.998s 0.594472 0.007341 2.229s 3.242s 1 LGB LightGBM 0.729212 0.627277 1.244s 0.616583 0.005321 2.879s 4.129s 2 LGB LightGBM 0.695463 0.632544 1.533s 0.619899 0.003822 3.502s 5.039s 3 LGB LightGBM 0.683228 0.638575 1.825s 0.625589 0.003608 4.148s 5.979s 4 LGB LightGBM 0.681811 0.639062 2.152s 0.627105 0.002460 4.838s 6.996s 5 LGB LightGBM 0.676747 0.639897 2.418s 0.634642 0.002138 5.622s 8.045s 6 LGB LightGBM 0.666471 0.638376 2.810s 0.634245 0.002098 6.240s 9.058s 7 LGB LightGBM 0.665065 0.643197 3.063s 0.637232 0.002537 6.888s 9.958s 8 LGB LightGBM 0.665018 0.654904 3.379s 0.650772 0.002577 7.621s 11.009s # Plot the train sizing's results atom.plot_learning_curve()","title":"Train sizing"},{"location":"examples/train_sizing/train_sizing/#train-sizing","text":"This example shows how to asses a model's performance based on the size of the training set. The data used is a variation on the Australian weather dataset from https://www.kaggle.com/jsphyg/weather-dataset-rattle-package. The goal of this dataset is to predict whether or not it will rain tomorrow training a binay classifier on target RainTomorrow.","title":"Train sizing"},{"location":"examples/train_sizing/train_sizing/#load-the-data","text":"# Import packages import numpy as np import pandas as pd from atom import ATOMClassifier # Load the Australian weather dataset X = pd.read_csv('./datasets/weatherAUS.csv') # Let's have a look at a subset of the data X.sample(frac=1).iloc[:5, :8] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed 3118 BadgerysCreek 11.7 23.2 0.0 NaN NaN SW 28.0 18965 NorahHead 10.2 19.4 0.0 NaN NaN SSE 30.0 11196 CoffsHarbour 9.7 21.2 0.0 NaN NaN NW 26.0 62283 Sale 8.4 21.7 0.0 NaN NaN WSW 41.0 92461 Townsville 11.1 27.1 0.0 7.6 10.7 ENE 37.0","title":"Load the data"},{"location":"examples/train_sizing/train_sizing/#run-the-pipeline","text":"# Initialize ATOM and prepare the data atom = ATOMClassifier(X, verbose=2, random_state=1) atom.impute(strat_num='median', strat_cat='most_frequent', min_frac_rows=0.8) atom.encode() << ================== ATOM ================== >> Algorithm task: binary classification. Applying data cleaning... Dataset stats ================= >> Shape: (142193, 22) Missing values: 292032 Categorical columns: 5 Scaled: False ---------------------------------- Size of training set: 113755 Size of test set: 28438 ---------------------------------- Class balance: No:Yes <==> 3.5:1.0 Instances in RainTomorrow per class: | | total | train_set | test_set | |:-------|---------:|-------------:|------------:| | 0: No | 110316 | 88263 | 22053 | | 1: Yes | 31877 | 25492 | 6385 | Fitting Imputer... Imputing missing values... --> Dropping 15182 rows for containing less than 80% non-missing values. --> Imputing 100 missing values with median in feature MinTemp. --> Imputing 57 missing values with median in feature MaxTemp. --> Imputing 640 missing values with median in feature Rainfall. --> Imputing 46535 missing values with median in feature Evaporation. --> Imputing 53034 missing values with median in feature Sunshine. --> Imputing 4381 missing values with most_frequent in feature WindGustDir. --> Imputing 4359 missing values with median in feature WindGustSpeed. --> Imputing 6624 missing values with most_frequent in feature WindDir9am. --> Imputing 612 missing values with most_frequent in feature WindDir3pm. --> Imputing 80 missing values with median in feature WindSpeed9am. --> Imputing 49 missing values with median in feature WindSpeed3pm. --> Imputing 532 missing values with median in feature Humidity9am. --> Imputing 1168 missing values with median in feature Humidity3pm. --> Imputing 1028 missing values with median in feature Pressure9am. --> Imputing 972 missing values with median in feature Pressure3pm. --> Imputing 42172 missing values with median in feature Cloud9am. --> Imputing 44251 missing values with median in feature Cloud3pm. --> Imputing 98 missing values with median in feature Temp9am. --> Imputing 702 missing values with median in feature Temp3pm. --> Imputing 640 missing values with most_frequent in feature RainToday. Fitting Encoder... Encoding categorical columns... --> Target-encoding feature Location. Contains 45 unique categories. --> Target-encoding feature WindGustDir. Contains 16 unique categories. --> Target-encoding feature WindDir9am. Contains 16 unique categories. --> Target-encoding feature WindDir3pm. Contains 16 unique categories. --> Label-encoding feature RainToday. Contains 2 unique categories. # We can analyze the impact of the training set's size on a LightGBM model atom.train_sizing('lgb', train_sizes=np.linspace(0.1, 1, 9), bagging=4) Running pipeline ============================= >> Models in pipeline: LGB Metric: f1 Run 0 (10% of set) ============================>> Size of training set: 11375 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.8029 Score on the test set --> f1: 0.6086 Time elapsed: 0.998s Bagging ----------------------------------------- Score --> f1: 0.5945 \u00b1 0.0073 Time elapsed: 2.229s ------------------------------------------------- Total time: 3.242s Final results ========================= >> Duration: 3.244s ------------------------------------------ LightGBM --> f1: 0.594 \u00b1 0.007 ~ Run 1 (21% of set) ============================>> Size of training set: 24172 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.7292 Score on the test set --> f1: 0.6273 Time elapsed: 1.244s Bagging ----------------------------------------- Score --> f1: 0.6166 \u00b1 0.0053 Time elapsed: 2.879s ------------------------------------------------- Total time: 4.129s Final results ========================= >> Duration: 4.131s ------------------------------------------ LightGBM --> f1: 0.617 \u00b1 0.005 Run 2 (32% of set) ============================>> Size of training set: 36970 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6955 Score on the test set --> f1: 0.6325 Time elapsed: 1.533s Bagging ----------------------------------------- Score --> f1: 0.6199 \u00b1 0.0038 Time elapsed: 3.502s ------------------------------------------------- Total time: 5.039s Final results ========================= >> Duration: 5.042s ------------------------------------------ LightGBM --> f1: 0.620 \u00b1 0.004 Run 3 (44% of set) ============================>> Size of training set: 49767 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6832 Score on the test set --> f1: 0.6386 Time elapsed: 1.825s Bagging ----------------------------------------- Score --> f1: 0.6256 \u00b1 0.0036 Time elapsed: 4.148s ------------------------------------------------- Total time: 5.979s Final results ========================= >> Duration: 5.981s ------------------------------------------ LightGBM --> f1: 0.626 \u00b1 0.004 Run 4 (55% of set) ============================>> Size of training set: 62565 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6818 Score on the test set --> f1: 0.6391 Time elapsed: 2.152s Bagging ----------------------------------------- Score --> f1: 0.6271 \u00b1 0.0025 Time elapsed: 4.838s ------------------------------------------------- Total time: 6.996s Final results ========================= >> Duration: 6.998s ------------------------------------------ LightGBM --> f1: 0.627 \u00b1 0.002 Run 5 (66% of set) ============================>> Size of training set: 75362 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6767 Score on the test set --> f1: 0.6399 Time elapsed: 2.418s Bagging ----------------------------------------- Score --> f1: 0.6346 \u00b1 0.0021 Time elapsed: 5.622s ------------------------------------------------- Total time: 8.045s Final results ========================= >> Duration: 8.047s ------------------------------------------ LightGBM --> f1: 0.635 \u00b1 0.002 Run 6 (77% of set) ============================>> Size of training set: 88160 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6665 Score on the test set --> f1: 0.6384 Time elapsed: 2.810s Bagging ----------------------------------------- Score --> f1: 0.6342 \u00b1 0.0021 Time elapsed: 6.240s ------------------------------------------------- Total time: 9.058s Final results ========================= >> Duration: 9.060s ------------------------------------------ LightGBM --> f1: 0.634 \u00b1 0.002 Run 7 (89% of set) ============================>> Size of training set: 100957 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6651 Score on the test set --> f1: 0.6432 Time elapsed: 3.063s Bagging ----------------------------------------- Score --> f1: 0.6372 \u00b1 0.0025 Time elapsed: 6.888s ------------------------------------------------- Total time: 9.958s Final results ========================= >> Duration: 9.960s ------------------------------------------ LightGBM --> f1: 0.637 \u00b1 0.003 Run 8 (100% of set) ===========================>> Size of training set: 113755 Size of test set: 28438 Results for LightGBM: Fitting ----------------------------------------- Score on the train set --> f1: 0.6650 Score on the test set --> f1: 0.6549 Time elapsed: 3.379s Bagging ----------------------------------------- Score --> f1: 0.6508 \u00b1 0.0026 Time elapsed: 7.621s ------------------------------------------------- Total time: 11.009s Final results ========================= >> Duration: 11.012s ------------------------------------------ LightGBM --> f1: 0.651 \u00b1 0.003","title":"Run the pipeline"},{"location":"examples/train_sizing/train_sizing/#analyze-the-results","text":"# Note that the results dataframe now is multi-index atom.results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name score_train score_test time_fit mean_bagging std_bagging time_bagging time run model 0 LGB LightGBM 0.802859 0.608590 0.998s 0.594472 0.007341 2.229s 3.242s 1 LGB LightGBM 0.729212 0.627277 1.244s 0.616583 0.005321 2.879s 4.129s 2 LGB LightGBM 0.695463 0.632544 1.533s 0.619899 0.003822 3.502s 5.039s 3 LGB LightGBM 0.683228 0.638575 1.825s 0.625589 0.003608 4.148s 5.979s 4 LGB LightGBM 0.681811 0.639062 2.152s 0.627105 0.002460 4.838s 6.996s 5 LGB LightGBM 0.676747 0.639897 2.418s 0.634642 0.002138 5.622s 8.045s 6 LGB LightGBM 0.666471 0.638376 2.810s 0.634245 0.002098 6.240s 9.058s 7 LGB LightGBM 0.665065 0.643197 3.063s 0.637232 0.002537 6.888s 9.958s 8 LGB LightGBM 0.665018 0.654904 3.379s 0.650772 0.002577 7.621s 11.009s # Plot the train sizing's results atom.plot_learning_curve()","title":"Analyze the results"}]}